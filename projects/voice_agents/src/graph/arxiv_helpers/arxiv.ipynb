{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747c592a",
   "metadata": {},
   "source": [
    "# ArXiv API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8adba",
   "metadata": {},
   "source": [
    "Testing the arxiv api functioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c92c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from typing import Literal\n",
    "\n",
    "def search_arxiv(\n",
    "    query: str, \n",
    "    max_results: int = 10, \n",
    "    sort_criterion : Literal['relevance', 'last_submitted'] = 'relevance'\n",
    ")-> str | list[dict]:\n",
    "    \"\"\"\n",
    "    Searches arXiv for the top N articles based on a query.\n",
    "    Returns a list of dictionaries containing article ID, title, summary, and authors.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query (e.g., \"AI agents\", \"quantum computing\").\n",
    "        max_results (int): The maximum number of results to return. Default is 10.\n",
    "        sort_criterion (Literal['relevance', 'last_submitted']): The criterion to sort the results by. \n",
    "            Default is 'relevance': this sorts by relevance to the query.\n",
    "            'last_submitted' sorts by the date the article was submitted to the arXiv.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    if sort_criterion == 'relevance':\n",
    "        sort_by = arxiv.SortCriterion.Relevance\n",
    "    elif sort_criterion == 'last_submitted':\n",
    "        sort_by = arxiv.SortCriterion.SubmittedDate\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=sort_by\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # client.results() is a generator\n",
    "    for result in client.results(search):\n",
    "        results.append({\n",
    "            # entry_id is a URL like 'http://arxiv.org/abs/2310.12345'\n",
    "            # We split to get just the ID '2310.12345'\n",
    "            \"id\": result.entry_id.split('/')[-1],\n",
    "            \"title\": result.title,\n",
    "            \"published\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"summary\": result.summary.replace(\"\\n\", \" \") # Clean up newlines in abstract\n",
    "        })\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7887a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'arxiv' has no attribute 'Client'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test it out:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43msearch_arxiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAI agents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(entry)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36msearch_arxiv\u001b[39m\u001b[34m(query, max_results, sort_criterion)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch_arxiv\u001b[39m(\n\u001b[32m      5\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m, \n\u001b[32m      6\u001b[39m     max_results: \u001b[38;5;28mint\u001b[39m = \u001b[32m10\u001b[39m, \n\u001b[32m      7\u001b[39m     sort_criterion : Literal[\u001b[33m'\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlast_submitted\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m )-> \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    Searches arXiv for the top N articles based on a query.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    Returns a list of dictionaries containing article ID, title, summary, and authors.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;03m            'last_submitted' sorts by the date the article was submitted to the arXiv.\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     client = \u001b[43marxiv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m()\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sort_criterion == \u001b[33m'\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     23\u001b[39m         sort_by = arxiv.SortCriterion.Relevance\n",
      "\u001b[31mAttributeError\u001b[39m: module 'arxiv' has no attribute 'Client'"
     ]
    }
   ],
   "source": [
    "# test it out:\n",
    "\n",
    "result = search_arxiv(query=\"AI agents\")\n",
    "\n",
    "for entry in result:\n",
    "    print(entry)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce6b522",
   "metadata": {},
   "source": [
    "Can we read the article without downloading it to the local file system? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import requests\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def read_arxiv_in_memory(paper_id: str):\n",
    "    \"\"\"\n",
    "    Downloads an arXiv paper into memory (without saving to disk) and extracts its text.\n",
    "    \n",
    "    Args:\n",
    "        paper_id (str): The arXiv ID (e.g., \"2103.00020\").\n",
    "    \"\"\"\n",
    "    # 1. Get the PDF URL\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(id_list=[paper_id])\n",
    "    \n",
    "    try:\n",
    "        paper = next(client.results(search))\n",
    "        pdf_url = paper.pdf_url\n",
    "    except StopIteration:\n",
    "        return f\"Error: Paper {paper_id} not found.\"\n",
    "\n",
    "    # 2. Download bytes into memory\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # 3. Open PDF from bytes\n",
    "        # fitz.open(stream=..., filetype=\"pdf\") lets us read from RAM\n",
    "        with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "                \n",
    "        return text[:20000]  # Truncate to avoid context overflow\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error reading paper in memory: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7937975",
   "metadata": {},
   "source": [
    "Now let's download the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f2a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import os\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "def download_arxiv_pdf(paper_id: str, save_dir: str = \"./downloads\"):\n",
    "    \"\"\"\n",
    "    Downloads the PDF of an arXiv paper given its ID.\n",
    "    \n",
    "    Args:\n",
    "        paper_id (str): The arXiv ID (e.g., \"2103.00020\").\n",
    "        save_dir (str): The directory to save the PDF in. Defaults to \"./downloads\".\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path of the downloaded PDF.\n",
    "    \"\"\"\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    # We must \"search\" by ID to get the paper object\n",
    "    search = arxiv.Search(id_list=[paper_id])\n",
    "    \n",
    "    try:\n",
    "        paper = next(client.results(search))\n",
    "        \n",
    "        # Create a safe filename using the ID and a sanitized title\n",
    "        # e.g., \"2103.00020_Attention_Is_All_You_Need.pdf\"\n",
    "        safe_title = \"\".join(c for c in paper.title if c.isalnum() or c in (' ', '_', '-')).rstrip()\n",
    "        safe_title = safe_title.replace(\" \", \"_\")\n",
    "        filename = f\"{paper_id}_{safe_title}.pdf\"\n",
    "        \n",
    "        # Download\n",
    "        path = paper.download_pdf(dirpath=save_dir, filename=filename)\n",
    "        return f\"Successfully downloaded file to: {path}\"\n",
    "        \n",
    "    except StopIteration:\n",
    "        return f\"Error: Paper with ID {paper_id} not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb8841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2201.00978v1\n",
      "2106.02277v1\n",
      "2104.11502v1\n",
      "2208.03987v4\n",
      "2404.05657v1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Successfully downloaded file to: ./downloads/2106.02277v1_Glance-and-Gaze_Vision_Transformer.pdf'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = search_arxiv(query=\"Transformer models\")\n",
    "\n",
    "for entry in result:\n",
    "    print(entry['id'])\n",
    "\n",
    "# Download the PDF\n",
    "download_arxiv_pdf(paper_id=result[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b06abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "\n",
    "def read_pdf_text(filepath: str):\n",
    "    \"\"\"\n",
    "    Extracts text from a locally saved PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = pymupdf.open(filepath)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text[:10000] # Truncate if too long for context window\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ebdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2106.02277v1_Glance-and-Gaze_Vision_Transformer.pdf']\n"
     ]
    }
   ],
   "source": [
    "# list files in the downloads folder\n",
    "import os\n",
    "print(os.listdir(\"./test_downloads\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94994a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glance-and-Gaze Vision Transformer\n",
      "Qihang Yu1, Yingda Xia1, Yutong Bai1, Yongyi Lu1, Alan Yuille1, Wei Shen2\n",
      "1 The Johns Hopkins University\n",
      "2 Shanghai Jiaotong University\n",
      "Abstract\n",
      "Recently, there emerges a series of vision Transformers, which show superior\n",
      "performance with a more compact model size than conventional convolutional\n",
      "neural networks, thanks to the strong ability of Transformers to model long-range\n",
      "dependencies. However, the advantages of vision Transformers also come with a\n",
      "price: Self-attention, the core part of Transformer, has a quadratic complexity to\n",
      "the input sequence length. This leads to a dramatic increase of computation and\n",
      "memory cost with the increase of sequence length, thus introducing difﬁculties\n",
      "when applying Transformers to the vision tasks that require dense predictions based\n",
      "on high-resolution feature maps.\n",
      "In this paper, we propose a new vision Transformer, named Glance-and-Gaze Trans-\n",
      "former (GG-Transformer), to address the aforementioned issues. It is motivated\n",
      "by the Glance and Gaze behavior of human beings when recognizing objects in\n",
      "natural scenes, with the ability to efﬁciently model both long-range dependencies\n",
      "and local context. In GG-Transformer, the Glance and Gaze behavior is realized by\n",
      "two parallel branches: The Glance branch is achieved by performing self-attention\n",
      "on the adaptively-dilated partitions of the input, which leads to a linear complexity\n",
      "while still enjoying a global receptive ﬁeld; The Gaze branch is implemented by a\n",
      "simple depth-wise convolutional layer, which compensates local image context to\n",
      "the features obtained by the Glance mechanism. We empirically demonstrate our\n",
      "method achieves consistently superior performance over previous state-of-the-art\n",
      "Transformers on various vision tasks and benchmarks. The codes and models will\n",
      "be made available at https://github.com/yucornetto/GG-Transformer.\n",
      "1\n",
      "Introduction\n",
      "Convolution Neural Networks (CNNs) have been dominating the ﬁeld of computer vision, which\n",
      "have been a de-facto standard and achieved tremendous success in various tasks, e.g., image clas-\n",
      "siﬁcation [16], object detection [15], semantic segmentation [5], etc. CNNs model images from a\n",
      "local-to-global perspective, starting with extracting local features such as edges and textures, and\n",
      "forming high-level semantic concepts gradually. Although CNNs prove to be successful for various\n",
      "vision tasks, they lack the ability to globally represent long-range dependencies. To compensate\n",
      "a global view to CNN, researchers explored different methods such as non-local operation [36],\n",
      "self-attention [33], Atrous Spatial Pyramid Pooling (ASPP) [5].\n",
      "Recently, another type of networks with stacked Transformer blocks emerged. Unlike CNNs,\n",
      "Transformers naturally learn global features in a parameter-free manner, which makes them stronger\n",
      "alternatives and raises questions about the necessity of CNNs in vision systems. Since the advent\n",
      "of Vision Transformer (ViT) [12], which applied Transformers to vision tasks by projecting and\n",
      "tokenizing natural images into sequences, various improvements have been introduced rapidly, e.g.,\n",
      "better training and distillation strategies [32], tokenization [41], position encoding [7], local feature\n",
      "learning [14]. Moreover, besides Transformers’ success on image classiﬁcation, many efforts have\n",
      "been made to explore Transformers for various down-stream vision tasks [35, 24, 13, 3, 46].\n",
      "Preprint. Under review.\n",
      "arXiv:2106.02277v1  [cs.CV]  4 Jun 2021\n",
      "Nevertheless, the advantages of Transformers come at a price. Since self-attention operates on the\n",
      "whole sequences, it incurs much more memory and computation costs than convolution, especially\n",
      "when it comes to natural images, whose lengths are usually much longer than word sequences, if\n",
      "treating each pixel as a token . Therefore, most existing works have to adopt a compromised strategy\n",
      "to embed a large image patch for each token, although treating smaller patches for tokens leads\n",
      "to a better performance (e.g., ViT-32 compared to ViT-16 [12]). To address this dilemma, various\n",
      "strategies have been proposed. For instance, Pyramid Vision Transformer (PVT) [35] introduced a\n",
      "progressive shrinking pyramid to reduce the sequence length of the Transformer with the increase of\n",
      "network depth, and adopted spatial-reduction attention, where key and value in the attention module\n",
      "are down-sampled to a lower resolution. Swin-Transformer [24] also adopted the pyramid structure,\n",
      "and further proposed to divide input feature maps into different ﬁx-sized local windows, so that\n",
      "self-attention is computed within each window, which reduces the computation cost and makes it\n",
      "scalable to large image scales with linear complexity.\n",
      "Nonetheless, we notice that these strategies have some limitations: Spatial-reduction attention can\n",
      "reduce memory and computation costs to learn high-resolution feature maps, yet with a price of losing\n",
      "details which are expected from the high-resolution feature maps. Adopting self-attention within\n",
      "local windows is efﬁcient with linear complexity, but it sacriﬁces the most signiﬁcant advantage of\n",
      "Transformers in modeling long-range dependencies.\n",
      "To address these limitations, we propose Glance-and-Gaze Transformer (GG-Transformer), in-\n",
      "spired by the Glance-and-Gaze human behavior when recognizing objects in natural scenes [11],\n",
      "which takes advantage of both the long-range dependency modeling ability of Transformers and\n",
      "locality of convolutions in a complementary manner. A GG-Transformer block consists of two\n",
      "parallel branches: A Glance branch performs self-attention within adaptively-dilated partitions of\n",
      "input images or feature maps, which preserves the global receptive ﬁeld of the self-attention operation,\n",
      "meanwhile reduces its computation cost to a linear complexity as local window attention [24] does;\n",
      "A Gaze branch compensates locality to the features obtained by the Glance branch, which is imple-\n",
      "mented by a light-weight depth-wise convolutional layer. A merging operation ﬁnally re-arranges the\n",
      "points in each partition to their original locations, ensuring that the output of the GG-Transformer\n",
      "block has the same size as the input. We evaluate GG-Transformer on several vision tasks and\n",
      "benchmarks including image classiﬁcation on ImageNet [10], object detection on COCO [23], and\n",
      "semantic segmentation on ADE20K [48], and show its efﬁciency and superior performance, compared\n",
      "to previous state-of-the-art Transformers.\n",
      "2\n",
      "Related Work\n",
      "CNN and self-attention. Convolution has been the basic unit in deep neural networks for computer\n",
      "vision problems. Since standard CNN blocks were proposed in [22], researchers have been working\n",
      "on designing stronger and more efﬁcient network architectures, e.g., VGG [30], ResNet [16], Mo-\n",
      "bileNet [29], and EfﬁcientNet [31]. In addition to studying how to organize convolutional blocks into\n",
      "a network, several variants of the convolution layer have also been proposed, e.g., group convolu-\n",
      "tion [21], depth-wise convolution [6], and dilated convolution [40]. With the development of CNN\n",
      "architectures, researchers also seeked to improve contextual representation of CNNs. Representative\n",
      "works, such as ASPP [5] and PPM [45] enhance CNNs with multi-scale context, and NLNet [36]\n",
      "and CCNet [20] provided a non-local mechanism to CNNs. Moreover, instead of just using them\n",
      "as an add-on to CNNs, some works explored to use attention modules to replace convolutional\n",
      "blocks [18, 28, 34, 44].\n",
      "Vision Transformer. Recently, ViT [12] was proposed to adapt the Transformer [33] for image\n",
      "recognition by tokenizing and ﬂattening 2D images into sequence of tokens. Since then, many works\n",
      "have been done to improve Transformers, making them more suitable for vision tasks. These works\n",
      "can be roughly categorized into three types: (1) Type I made efforts to improve the ViT design\n",
      "itself. For example, DeiT [32] introduced a training scheme to get rid of large-scale pre-training and\n",
      "distillation method to further improve the performance. T2T-ViT [41] presented a token-to-token\n",
      "operation as alternatives to patch embedding, which keeps better local details. (2) Type II tried to\n",
      "introduce convolution back into the ViT design. E.g., Chu et al. [7] proposed to use convolution\n",
      "for position encoding. Wu et al. [37] used convolution to replace the linear projection layers in\n",
      "Transformers. (3) Type III tried to replace CNNs by building hierarchical Transformers as a plug-in\n",
      "backbone in many downstream tasks. Wang et al. [35] proposed a pyramid vision Transformer,\n",
      "2\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "Figure 1: Toy examples illustrating different methods to reduce computation and memory cost\n",
      "of self-attention. (a) Spatial reduction [35, 13] spatially downsamples the feature map; (b) Local\n",
      "window [24] restricts self-attention inside local windows; (c) Glance attention (ours) applies self-\n",
      "attention to adaptively-dilated partitions.\n",
      "which gradually downsamples the feature map and extract multi-scale features as common CNN\n",
      "backbones do. However, applying self-attention on high-resolution features is not affordable in\n",
      "terms of both memory and computation cost, thus they used spatial-reduction attention, which\n",
      "downsamples key and value in self-attention as a trade-off between efﬁciency and accuracy. Later,\n",
      "Liu et al. [24] proposed a new hierarchical Transformer architecture, named Swin-Transformer. To\n",
      "handle the expensive computation burden incurred with self-attention, they divided feature maps into\n",
      "several non-overlapped windows, and limited the self-attention operation to be performed within\n",
      "each window. By doing so, Swin-Transformer is more efﬁcient and also scalable to large resolution\n",
      "input. Besides, to compensate the missing global information, a shifted window strategy is proposed\n",
      "to exchange information between different windows.\n",
      "Our method differs from aforementioned works in the following aspects: Type I, II methods usually\n",
      "utilize a large patch size and thus incompatible to work with high-resolution feature map. Type III\n",
      "methods propo\n"
     ]
    }
   ],
   "source": [
    "# read it out\n",
    "text = read_pdf_text(filepath=\"./test_downloads/2106.02277v1_Glance-and-Gaze_Vision_Transformer.pdf\")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
