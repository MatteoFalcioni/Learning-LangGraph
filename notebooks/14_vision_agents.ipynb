{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd10c2b",
   "metadata": {},
   "source": [
    "# Vision Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630424c",
   "metadata": {},
   "source": [
    "In this repo we will implement a multimodal agentic system. \n",
    "\n",
    "\n",
    "Specifically, we continue with the arvix agent we implemented in [voice agents/](../voice_agents/) to research for interesting papers on arxiv. \n",
    "\n",
    "Then, once a paper (a single one for simplicity) is selected, the system will produce a summary: both written and visual. For the latter, we use Gemini's NanoBanana model, which has been known to produce great \"whiteboard summaries\" (check example below).\n",
    "\n",
    "After the image generation node, we have an image reviewer that checks the quality of the generated image. Since text summary and visual summary node run in parallel, we need a 'fan out' node (`create_report`) and a 'fan-in' node (`reduce`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3206d",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../projects/vision_agents/graph_plot/arxiv_20260107_163830.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079243e",
   "metadata": {},
   "source": [
    "## Multimodal Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034e6c6",
   "metadata": {},
   "source": [
    "Multimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers.\n",
    "\n",
    "Chat models can accept multimodal data as input and generate it as output. \n",
    "\n",
    "For LangChain, we need to structure additional input as content blocks, like this: \n",
    "\n",
    "```python\n",
    "\n",
    "# From base64 data\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe the content of this image.\"},\n",
    "        {\n",
    "            \"type\": \"image\",\n",
    "            \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\",\n",
    "            \"mime_type\": \"image/jpeg\",\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Content blocks are just a list of typed dictionaries.\n",
    "\n",
    "Find all examples here: [link](https://docs.langchain.com/oss/python/langchain/messages#multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8c913",
   "metadata": {},
   "source": [
    "In our implementation we attach images in input like this (functions defined in [`utils.py`](../projects/vision_agents/src/utils.py)):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e478585",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "def add_imgs(state: MyState, mime_type: Literal[\"image/jpeg\", \"image/png\"]) -> HumanMessage:\n",
    "    \"\"\"\n",
    "    Helper to create multimodal message from state\n",
    "\n",
    "    Args:\n",
    "        state (MyState): The state of the graph\n",
    "        mime_type (Literal[\"image/jpeg\", \"image/png\"]): The mime type of the images\n",
    "    Returns:\n",
    "        message (HumanMessage): The multimodal message\n",
    "    \"\"\"    \n",
    "    msg = \"Here are the images to review\"\n",
    "    \n",
    "    content_blocks = [{\"type\": \"text\", \"text\": msg}]   # it is a list of typed dicts, see https://docs.langchain.com/oss/python/langchain/messages#multimodal\n",
    "    \n",
    "    # Add images\n",
    "    for img_b64 in state.get(\"generated_images\", []):\n",
    "        content_blocks.append({\n",
    "            \"type\": \"image\",\n",
    "            \"base64\": img_b64,\n",
    "            \"mime_type\": mime_type\n",
    "        })\n",
    "\n",
    "    # construct the messages as HumanMessage(content_blocks=...)\n",
    "    message = HumanMessage(content_blocks=content_blocks)  # v1 format, see https://docs.langchain.com/oss/python/langchain/messages#multimodal\n",
    "    \n",
    "    return message   # NOTE: returns msg as is, then you need to wrap it in a list!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf9675",
   "metadata": {},
   "source": [
    "and pdf files like this: \n",
    "\n",
    "```python\n",
    "def add_pdfs(state: MyState) -> HumanMessage:\n",
    "    \"\"\"\n",
    "    Helper to add the pdf to the input message\n",
    "\n",
    "    Args:\n",
    "        state (MyState): The state of the graph\n",
    "\n",
    "    Returns:\n",
    "        message (HumanMessage): The message with the pdf\n",
    "    \"\"\"    \n",
    "    msg = \"Summarize the content of this document\"\n",
    "    content_blocks = [{\"type\": \"text\", \"text\": msg}]   # it is a list of typed dicts, see https://docs.langchain.com/oss/python/langchain/messages#multimodal\n",
    "    \n",
    "    for pdf_path in state.get(\"downloaded_papers_paths\", []):\n",
    "        with open(pdf_path, \"rb\") as f:\n",
    "            pdf_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        content_blocks.append({\n",
    "            \"type\": \"file\",\n",
    "            \"base_64\": pdf_b64,\n",
    "            \"mime_type\": \"application/pdf\"\n",
    "        })\n",
    "    message = HumanMessage(content_blocks=content_blocks)  # v1 format, see https://docs.langchain.com/oss/python/langchain/messages#multimodal\n",
    "    return message   # NOTE: returns msg as is, then you need to wrap it in a list!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75367489",
   "metadata": {},
   "source": [
    "## Generative Models Without LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f0dfc",
   "metadata": {},
   "source": [
    "Now, another interesting point is that sometimes we may find ourselves limited in what LangChain can offer in terms of compatibility with llm providers.\n",
    "\n",
    "Of course LangChain/LangGraph have their strengths (otherwise this course would not make a lot of sense) and one of the main pros of using LangChain is that we can manage different llm providers in a unified interface. \n",
    "\n",
    "But this can leave some 'blind spots' for the most reccent models or for soome specific applications: for example, using LangChain's ChatOpenAI wrapper to call Gemini's NanoBanana makes it hard to access the images generated by the model. \n",
    "\n",
    "It's simpler to use the Gemini API through the OpenAI sdk, or maybe through Gemini/Openrouter's APIs. You can see [here](https://openrouter.ai/google/gemini-3-pro-image-preview/api) that we have several choices.\n",
    "\n",
    "In our application we call nanobanana like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9defe42f",
   "metadata": {},
   "source": [
    "```python\n",
    "def nanobanana_generate(state: MyState, nanobanana_prompt: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Generates an image from a PDF using the Nanobanana model.\n",
    "\n",
    "    Args:\n",
    "        state (MyState): The state of the graph\n",
    "        nanobanana_prompt (str): The prompt for the Nanobanana model\n",
    "    Returns:\n",
    "        image_urls (list[str]): The list of image URLs\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the image generation fails\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    )\n",
    "    # Find example.jpg using glob - search from the src directory\n",
    "    utils_dir = Path(__file__).parent\n",
    "    example_files = list(utils_dir.glob(\"**/example.jpg\"))\n",
    "    if not example_files:\n",
    "        raise FileNotFoundError(\"Could not find example.jpg in the repository\")\n",
    "    example_file_path = example_files[0]\n",
    "    with open(example_file_path, \"rb\") as f:\n",
    "        example_img = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemini-3-pro-image-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\", \n",
    "                        \"text\": nanobanana_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:application/pdf;base64,{state.get('pdf_base64')}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{example_img}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        extra_body={\"modalities\": [\"image\", \"text\"]}\n",
    "    )\n",
    "\n",
    "    image_urls = []\n",
    "    response = response.choices[0].message\n",
    "    if response.images:\n",
    "        for image in response.images:\n",
    "            image_url = image['image_url']['url']  # Base64 data URL\n",
    "            image_urls.append(image_url)\n",
    "    else:\n",
    "        raise RuntimeError(\"Failed to generate image\")\n",
    "\n",
    "    return image_urls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2dfc5",
   "metadata": {},
   "source": [
    "You can see that we are actually putting together several things in this function: \n",
    "\n",
    "- we are using multimodal inputs, as we want the model to both: \n",
    "    a) see the example image before generating;\n",
    "    b) read the pdf of which we need a summary image;\n",
    "\n",
    "- we are getting the pdf in base64 encoding from state (we had already encoded it when downloading the pdf in the tools, and save the base64 form to state);\n",
    "- we are using openai's sdk\n",
    "- we are passing the prompt as the input text to the model (see next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023983e5",
   "metadata": {},
   "source": [
    "## Adding Images (or Audios) to System Prompts\n",
    "\n",
    "We already saw how important prompts are in agentic workflows. So it's natural that when using a model with multimodal input we'd want to have a **multimodal prompt**, by adding images or audios to our system prompt. \n",
    "\n",
    "How can we do that? \n",
    "\n",
    "Well, if we understand that a system prompt is just a message that is always preprended to the other input messages, we automaticlly know the answer already: we just need to construct our messages in a way that incorporates both the textual part of the prompt **and** the image/audio parts, with content blocks - exactly as we do for normal messages.\n",
    "\n",
    "That is exactly what we do in the `nanobanana_generate()` function:\n",
    "\n",
    "```python\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\", \n",
    "                        \"text\": nanobanana_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:application/pdf;base64,{state.get('pdf_base64')}\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{example_img}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db108b",
   "metadata": {},
   "source": [
    "where the nanobanana prompt is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cbec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nanobanana_prompt = \"\"\"\n",
    "You are an expert technical educator. You are provided with a PDF scientific paper. \n",
    "Your task is to synthesize the core concepts and key insights of this document into a SINGLE, high-resolution blackboard visual summary.\n",
    "\n",
    "Visual Style Requirements:\n",
    "\n",
    "- **Surface**: Professional magnetic whiteboard (bright white, slight glossy reflection).\n",
    "- **Ink**: Bold, wet-erase markers in Deep Blue, Emerald Green, and Safety Orange.\n",
    "- **Layout**: \n",
    "    - Use 'Notes App Chic' hierarchy. \n",
    "    - Center: A central 'Main Concept' box with a brain or robot icon.\n",
    "    - Left Column: 'Core Principles' in Green. Use code-like syntax (e.g., <input> tags).\n",
    "    - Right Column: 'Multimodal Context' in Blue with hand-drawn icons for files and eyes.\n",
    "    - Bottom: An 'Example Template' section using a purple frame.\n",
    "- **Annotations**: Add 'The Engineering Mindset' box in Orange at the bottom right with a warning icon.\n",
    "- **Text**: Neat, professional handwriting. No generic fonts.\n",
    "\n",
    "You will get an example image of the visual style you should follow. \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
