{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd10c2b",
   "metadata": {},
   "source": [
    "# Vision Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630424c",
   "metadata": {},
   "source": [
    "In this repo we will implement a multimodal agentic system. \n",
    "\n",
    "\n",
    "Specifically, we continue with the arvix agent we implemented in [voice agents/](../voice_agents/) to research for interesting papers on arxiv. \n",
    "\n",
    "Then, once a paper (a single one for simplicity) is selected, the system will produce a summary: both written and visual. For the latter, we use Gemini's NanoBanana model, which has been known to produce great \"whiteboard summaries\" (check example below).\n",
    "\n",
    "After the image generation node, we have an image reviewer that checks the quality of the generated image. Since text summary and visual summary node run in parallel, we need a 'fan out' node (`create_report`) and a 'fan-in' node (`reduce`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3206d",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../projects/vision_agents/graph_plot/arxiv_20260107_163830.png\" width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079243e",
   "metadata": {},
   "source": [
    "## Multimodal Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034e6c6",
   "metadata": {},
   "source": [
    "Multimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers.\n",
    "\n",
    "Chat models can accept multimodal data as input and generate it as output. \n",
    "\n",
    "Find all examples here: [link](https://docs.langchain.com/oss/python/langchain/messages#multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8c913",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75367489",
   "metadata": {},
   "source": [
    "## Using models through their api's (nanobanana)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f0dfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "023983e5",
   "metadata": {},
   "source": [
    "## Adding Images (or Audios) to Prompts\n",
    "\n",
    "Use content blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8db108b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
