{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b5e7bcb",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MatteoFalcioni/Learning-LangGraph/blob/main/notebooks/11_supervisor.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ef8fc",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d00a75",
   "metadata": {},
   "source": [
    "#### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18318ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U -r https://raw.githubusercontent.com/MatteoFalcioni/Learning-LangGraph/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51961352",
   "metadata": {},
   "source": [
    "#### local (notebooks or files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a7d779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load api keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c70268",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e597e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m REQUIRED_KEYS = [\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLANGSMITH_TRACING\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLANGSMITH_PROJECT\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     10\u001b[39m ]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "REQUIRED_KEYS = [\n",
    "    'OPENAI_API_KEY',\n",
    "    'LANGSMITH_TRACING',\n",
    "    'LANGSMITH_ENDPOINT',\n",
    "    'LANGSMITH_API_KEY',\n",
    "    'LANGSMITH_PROJECT'\n",
    "]\n",
    "\n",
    "def _set_colab_keys(key : str):\n",
    "    # Retrieve the secret value using its key/name\n",
    "    secret_value = userdata.get(key)\n",
    "    # set it as a standard OS environment variable\n",
    "    os.environ[key] = secret_value\n",
    "\n",
    "for key in REQUIRED_KEYS:\n",
    "    _set_colab_keys(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d745f5af",
   "metadata": {},
   "source": [
    "# Supervised Workflows & MCP\n",
    "\n",
    "In this notebook we will be creating a supervised agent workflow: a supervisor agent managing 3 subagents.\n",
    "\n",
    "In our example our subagents will peform three distinct functions: \n",
    "- the `email_agent` will be checking incoming mails on our Gmail, writing drafts, requesting for approval or edits (HITL) and sanding emails if approved;\n",
    "- the `calendar_agent` will check our Google Calendar, remind us of incoming events and writing new events if requested;\n",
    "- the `github_agent` will check our `GitHub`notifications and, if we approve, dismiss them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8defae41",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"../projects/supervisor/src/graph/graph_plot/supervised_20251229_163528.png\" width=\"600\">\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f22d9",
   "metadata": {},
   "source": [
    "We will be writing python files now instead of notebooks, and use this notebook only for notes and remarks on our implementation. \n",
    "\n",
    "Check out the full implementation here: [`projects/supervisor`](../projects/supervisor/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe9d71",
   "metadata": {},
   "source": [
    "# 1. Handoff Tools\n",
    "\n",
    "We start with the core of the implementation of a supervisor agent: handing off tasks to the subagents. \n",
    "\n",
    "In order to do so we usually follow a simple approach: give our supervisor agent \"handoff tools\", i.e. tools that it can call to route the flow to the other agents. \n",
    "\n",
    "> **Note 1:** You'll see that i usually use a simple agent for the supervisor, while i construct nodes for the subagents. \n",
    ">\n",
    ">This is not mandatory. I just like to do it because usually the subagents need to perform more complex tasks, and therefore I often need to do additional operations in the subagent nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a068455",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e08b64",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Note 2 (Advanced):** Following this logic, if we need to perform additional operations when dealing with the supervisor, we can create a supervisor node as well. \n",
    ">\n",
    "> Catch for this second approach: if the supervisor **routes** with a tool (using Command) but the supervisor is invoked in a node, the goto must be propagated. In this we could extract the goto from the Command returned from the supervisor agent, or maybe it's easier to just use a state var \"next\" that the supervisor fills with its tools. Or even a structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd1b03",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94880488",
   "metadata": {},
   "source": [
    "Let's see some examples of handoff tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852be96",
   "metadata": {},
   "source": [
    "Note that all these tools are structured with `graph.PARENT` because we do not have a supervisor node right now. Our supervisor will be running as a compiled agent inside a parent graph.\n",
    "\n",
    "Therefore we need `Command.PARENT` to tell the `Command` that it should escape the supervisor and navigate to that given node in the parent graph.\n",
    "\n",
    "```\n",
    "Parent Graph\n",
    "â”œâ”€â”€ supervisor_node  # (our supervisor agent)\n",
    "â”‚       â””â”€â”€ handoff_to_gmail_agent tool (returns Command)\n",
    "â””â”€â”€ gmail_agent_node\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9cfffe",
   "metadata": {},
   "source": [
    "### 1.1. Simple Handoff Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95654661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain_core.messages import ToolMessage, HumanMessage\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "agent_name = \"gmail_agent\"\n",
    "@tool\n",
    "def handoff_to_gmail_agent(\n",
    "    runtime: ToolRuntime,\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Transfer the task to the gmail agent.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=f\"Successfully transferred to subagent\",\n",
    "        tool_call_id=runtime.tool_call_id,\n",
    "    )\n",
    "\n",
    "    return Command(\n",
    "        goto=agent_name,\n",
    "        update={\"messages\": [tool_msg]},\n",
    "        graph=Command.PARENT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3842ebc",
   "metadata": {},
   "source": [
    "### 1.2. Handoff Tool with Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9f141",
   "metadata": {},
   "source": [
    "We could add a task message from the supervisor to the full chat history, or we could replace chat history with just the task. \n",
    "\n",
    "These are two different approaches: both make the agent focus more on the task, but the latter makes the agent focus specifically on that task the supervisor asks and erases the subagent memory of everything else. This is the approach of `deepagents` for subagents. Let's see how: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ba1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"gmail_agent\"\n",
    "@tool\n",
    "def handoff_to_gmail_agent(\n",
    "    task: Annotated[str, \"The task to be performed by the subagent\"],\n",
    "    runtime: ToolRuntime,\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Transfer the task to the gmail agent.\n",
    "    \"\"\"\n",
    "\n",
    "    tool_msg = ToolMessage(\n",
    "        content=f\"Successfully transferred to subagent\",\n",
    "        tool_call_id=runtime.tool_call_id,\n",
    "    )\n",
    "\n",
    "    state = runtime.state\n",
    "    task_msg = HumanMessage(content=task)\n",
    "\n",
    "    return Command(\n",
    "        goto=agent_name,\n",
    "        update={**state, \"messages\": [tool_msg] + [task_msg]},  # this will replace the previous chat history\n",
    "        graph=Command.PARENT,  # transfer to nearest parent graph \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e5aae",
   "metadata": {},
   "source": [
    "### 1.3. Handoff Tool with HITL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e4eb3",
   "metadata": {},
   "source": [
    "We can add human approval before routing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418081c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = \"gmail_agent\"\n",
    "@tool\n",
    "def handoff_to_gmail_agent_HITL(\n",
    "    task: Annotated[str, \"The task to be performed by the subagent\"],\n",
    "    runtime: ToolRuntime,\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Transfer the task to the gmail agent.\n",
    "    \"\"\"\n",
    "    # interrupt the main flow and ask the user for approval\n",
    "    usr_response = interrupt(\n",
    "            value=f\"The agent supervisor wants to call the {agent_name} to perform the following task: *{task}*\\nDo you approve?\"\n",
    "        )\n",
    "\n",
    "    if usr_response['decision'] == \"accept\":\n",
    "            goto = agent_name\n",
    "            tool_msg = [ToolMessage(content=f\"Successfully transferred to {agent_name}\", tool_call_id=runtime.tool_call_id)]\n",
    "            task_msg = [HumanMessage(content=f\"The agent supervisor advices you to perform the following task : \\n{task}\")]\n",
    "            msgs = tool_msg + task_msg\n",
    "\n",
    "    elif usr_response['decision'] == 'reject':\n",
    "        goto = \"supervisor\"\n",
    "        msgs = [ToolMessage(content=f\"Routing to {agent_name} was rejected by the user.\", tool_call_id=runtime.tool_call_id)]\n",
    "    \n",
    "    else: \n",
    "        raise ValueError(f\"Invalid user response: {usr_response['decision']}\")\n",
    "\n",
    "    return Command(\n",
    "        goto=goto,\n",
    "        update={\"messages\": msgs}, \n",
    "        graph=Command.PARENT,  \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7bf2d",
   "metadata": {},
   "source": [
    "## 2. Model Context Protocol (*MCP*)\n",
    "\n",
    "For the GitHub tool we will use ***Model Context Protocol***, a new technology that is gaining a lot of traction in agentic AI applications. \n",
    "\n",
    "It's soon referred to as the \"USB C\" of AI agents: a comparison that's kind of inaccurate, since the MCP technology is often propietary, can give rise to technical overhead (it can introudce additional latency) and can have security risks if not handled carefully. \n",
    "\n",
    "Still, MCP is revolutionary for AI agents: Before MCP, if you had 5 AI models and 5 data sources (Google Drive, Slack, GitHub, etc.), you had to write 25 different integrations ($5 \\times 5$).MCP changes this to a \"plug-and-play\" model ($M + N$ complexity).\n",
    "\n",
    "Now, MCP are widely used in no code / low code AI agents builds: they are basically standardized tools that are plugable and ready to use. Since we use LangGraph, we are already building \"provider-agnostic\" tools, so what's the point?\n",
    "\n",
    "Well, the point is that we can get lots of useful tools, prompts, resources, already built from others. Again, no need to reinvent the wheel if we want to dotate our agent with a tool to work on Github.  \n",
    "\n",
    "Here there is a huge list of MCP servers divided into categories: [MCP Servers](https://github.com/modelcontextprotocol/servers). \n",
    "\n",
    "Resources: \n",
    "- [LangChain YT: Understanding MCP From Scratch](https://youtu.be/CDjjaTALI68?si=nyFegYEL_cTVUX5W)\n",
    "- [LangChain YT: Using MCP with LangGraph agents](https://youtu.be/OX89LkTvNKQ?si=rQfTh6A3mklaXzbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d77c34f",
   "metadata": {},
   "source": [
    "### Example 1: GitHub MCP server "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7c830",
   "metadata": {},
   "source": [
    "Let's see how to use an MCP server to get tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "137423f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Initialize the MCP Client via http (https://github.com/github/github-mcp-server?tab=readme-ov-file)\n",
    "client = MultiServerMCPClient({\n",
    "    \"github\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://api.githubcopilot.com/mcp/x/notifications\",  # https://github.com/github/github-mcp-server/blob/main/docs/remote-server.md\n",
    "        \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {os.getenv('GITHUB_TOKEN')}\"\n",
    "            },\n",
    "        \n",
    "    }\n",
    "})\n",
    "\n",
    "# 2. Get the tools\n",
    "tools = await client.get_tools()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dec96a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== dismiss_notification ====================\n",
      "\n",
      "Dismiss a notification by marking it as read or done\n",
      "\n",
      "==================== get_notification_details ====================\n",
      "\n",
      "Get detailed information for a specific GitHub notification, always call this tool when the user asks for details about a specific notification, if you don't know the ID list notifications first.\n",
      "\n",
      "==================== list_notifications ====================\n",
      "\n",
      "Lists all GitHub notifications for the authenticated user, including unread notifications, mentions, review requests, assignments, and updates on issues or pull requests. Use this tool whenever the user asks what to work on next, requests a summary of their GitHub activity, wants to see pending reviews, or needs to check for new updates or tasks. This tool is the primary way to discover actionable items, reminders, and outstanding work on GitHub. Always call this tool when asked what to work on next, what is pending, or what needs attention in GitHub.\n",
      "\n",
      "==================== manage_notification_subscription ====================\n",
      "\n",
      "Manage a notification subscription: ignore, watch, or delete a notification thread subscription.\n",
      "\n",
      "==================== manage_repository_notification_subscription ====================\n",
      "\n",
      "Manage a repository notification subscription: ignore, watch, or delete repository notifications subscription for the provided repository.\n",
      "\n",
      "==================== mark_all_notifications_read ====================\n",
      "\n",
      "Mark all notifications as read\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    print(f\"{'='*20} {tool.name} {'='*20}\\n\\n{tool.description}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6b56e",
   "metadata": {},
   "source": [
    "Crazy simple right? Look at all the tools we just got out of the box! We will use these in our GitHub agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81887815",
   "metadata": {},
   "source": [
    "### Example 2: LangChain MCP server "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91055a9e",
   "metadata": {},
   "source": [
    "Another example: LangChain mcp server to let our agents search directly in the LangChain documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39692c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
    "\n",
    "LG_client = MultiServerMCPClient({\n",
    "    \"github\": {\n",
    "        \"transport\": \"http\",\n",
    "        \"url\": \"https://docs.langchain.com/mcp\",  \n",
    "    }\n",
    "})\n",
    "\n",
    "# Get the tools\n",
    "LG_tools = await client.get_tools()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1da1feaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== SearchDocsByLangChain ====================\n",
      "\n",
      "Search across the Docs by LangChain knowledge base to find relevant information, code examples, API references, and guides. Use this tool when you need to answer questions about Docs by LangChain, find specific documentation, understand how features work, or locate implementation details. The search returns contextual content with titles and direct links to the documentation pages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tool in LG_tools:\n",
    "    print(f\"{'='*20} {tool.name} {'='*20}\\n\\n{tool.description}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1479952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How should i make my documents (let's say they are pdfs) into a vector store?\n",
      "\n",
      "[{'type': 'text', 'text': 'Title: Vector stores\\nLink: https://docs.langchain.com/oss/javascript/integrations/vectorstores/index\\nContent: Overview\\nA vector store stores embedded data and performs similarity search. flowchart LR\\n\\n    subgraph \"ðŸ“¥ Indexing phase (store)\"\\n        A[ðŸ“„ Documents] --> B[ðŸ”¢ Embedding model]\\n        B --> C[ðŸ”˜ Embedding vectors]\\n        C --> D[(Vector store)]\\n    end\\n\\n    subgraph \"ðŸ“¤ Query phase (retrieval)\"\\n        E[â“ Query text] --> F[ðŸ”¢ Embedding model]\\n        F --> G[ðŸ”˜ Query vector]\\n        G --> H[ðŸ” Similarity search]\\n        H --> D\\n        D --> I[ðŸ“„ Top-k results]\\n    end\\n\\nAdding documents\\nYou can add documents to the vector store by using the addDocuments function. import { Document } from \"@langchain/core/documents\";\\nconst document = new Document({\\n  pageContent: \"Hello world\",\\n});\\nawait vectorStore.addDocuments([document]);\\n\\nDeleting documents\\nYou can delete documents from the vector store by using the delete function. await vectorStore.delete({\\n  filter: {\\n    pageContent: \"Hello world\",\\n  },\\n});\\n\\n', 'id': 'lc_dd55dbb0-9c5d-43fa-855e-6f0520bddfae'}, {'type': 'text', 'text': 'Title: Vector stores\\nLink: https://docs.langchain.com/oss/python/integrations/vectorstores/index\\nContent: Overview\\nA vector store stores embedded data and performs similarity search. flowchart LR\\n\\n    subgraph \"ðŸ“¥ Indexing phase (store)\"\\n        A[ðŸ“„ Documents] --> B[ðŸ”¢ Embedding model]\\n        B --> C[ðŸ”˜ Embedding vectors]\\n        C --> D[(Vector store)]\\n    end\\n\\n    subgraph \"ðŸ“¤ Query phase (retrieval)\"\\n        E[â“ Query text] --> F[ðŸ”¢ Embedding model]\\n        F --> G[ðŸ”˜ Query vector]\\n        G --> H[ðŸ” Similarity search]\\n        H --> D\\n        D --> I[ðŸ“„ Top-k results]\\n    end\\n\\nDeleting documents\\nDelete by specifying IDs: vector_store.delete(ids=[\"id1\"])\\n\\nAdding documents\\nAdd Document objects (holding page_content and optional metadata) like so: vector_store.add_documents(documents=[doc1, doc2], ids=[\"id1\", \"id2\"])\\n\\n', 'id': 'lc_5322c841-d50e-49b0-9637-959819d14b83'}, {'type': 'text', 'text': 'Title: 3. Vector stores\\nLink: https://docs.langchain.com/oss/javascript/langchain/knowledge-base\\nContent: LangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres ) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store: Having instantiated our vector store, we can now index the documents. Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail. Once we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying: Synchronously and asynchronously; By string query and by vector; With and without returning similarity scores; By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results). The methods will generally include a list of Document objects in their outputs. Usage Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document. Return documents based on similarity to a string query: Return scores: Score: 0.23699893057346344\\n\\nDocument {\\n    pageContent: \\'Table of Contents...\\',\\n    metadata: {\\'page\\': 35, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 0}\\n}\\n\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres ) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store: Having instantiated our vector store, we can now index the documents. Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail. Once we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying: Synchronously and asynchronously; By string query and by vector; With and without returning similarity scores; By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results). The methods will generally include a list of Document objects in their outputs. Usage Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document. Return documents based on similarity to a string query: Return scores: Return documents based on similarity to an embedded query: Document {\\n    pageContent: \\'FISCAL 2023 COMPARED TO FISCAL 2022...\\',\\n    metadata: {\\n        \\'page\\': 36,\\n        \\'source\\': \\'../example_data/nke-10k-2023.pdf\\',\\n        \\'start_index\\': 0\\n    }\\n}\\n\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres ) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store: Having instantiated our vector store, we can now index the documents. Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail. Once we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying: Synchronously and asynchronously; By string query and by vector; With and without returning similarity scores; By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results). The methods will generally include a list of Document objects in their outputs. Usage Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document. Return documents based on similarity to a string query: Document {\\n    pageContent: \\'direct to consumer operations sell products...\\',\\n    metadata: {\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}\\n}\\n\\n', 'id': 'lc_4eccd464-44fd-4fbf-987f-c8edb7076427'}, {'type': 'text', 'text': 'Title: 3. Vector stores\\nLink: https://docs.langchain.com/oss/python/langchain/knowledge-base\\nContent: LangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres ) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store: Having instantiated our vector store, we can now index the documents. Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail. Once we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying: Synchronously and asynchronously; By string query and by vector; With and without returning similarity scores; By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results). The methods will generally include a list of Document objects in their outputs. Usage Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document. Return documents based on similarity to a string query: page_content=\\'direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213\\nNIKE Brand in-line stores (including employee-only stores) 74\\nConverse stores (including factory stores) 82\\nTOTAL 369\\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2\\' metadata={\\'page\\': 4, \\'source\\': \\'../example_data/nke-10k-2023.pdf\\', \\'start_index\\': 3125}\\n\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres ) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store: Having instantiated our vector store, we can now index the documents. ids = vector_store.add_documents(documents=all_splits)\\n\\nLangChain VectorStore objects contain methods for adding text and Document objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres ) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\\'s select a vector store: Having instantiated our vector store, we can now index the documents. Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific integration for more detail. Once we\\'ve instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying: Synchronously and asynchronously; By string query and by vector; With and without returning similarity scores; By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results). The methods will generally include a list of Document objects in their outputs. Usage Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document. Return documents based on similarity to a string query: Async query: Return scores: Return documents based on similarity to an embedded query: Learn more: API Reference Integration-specific docs\\n\\n', 'id': 'lc_77d60cf2-ea62-4228-bf77-cd82f87e0dce'}, {'type': 'text', 'text': \"Title: Storing documents\\nLink: https://docs.langchain.com/oss/python/langchain/rag\\nContent: Now we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial , our approach is to embed the contents of each document split and insert these embeddings into a vector store . Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial. document_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial , our approach is to embed the contents of each document split and insert these embeddings into a vector store . Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial. ['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6']\\n\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial , our approach is to embed the contents of each document split and insert these embeddings into a vector store . Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial. Go deeper Embeddings : Wrapper around a text embedding model, used for converting text to embeddings. Integrations : 30+ integrations to choose from. Interface : API reference for the base interface. VectorStore : Wrapper around a vector database, used for storing and querying embeddings. Integrations : 40+ integrations to choose from. Interface : API reference for the base interface. This completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\n\", 'id': 'lc_46444a71-eb95-4862-aae6-734eebcb7a7c'}, {'type': 'text', 'text': 'Title: Vector Stores\\nLink: https://docs.langchain.com/oss/python/contributing/implement-langchain\\nContent: All vector stores must inherit from the  VectorStore  base class. This interface consists of methods for writing, deleting and searching for documents in the vector store. See the  vector store integration guide  for details on implementing a vector store integration. The vector store integration guide is currently WIP. In the meantime, read the  vector store conceptual guide  for details on how LangChain vector stores function.\\n\\nImplement a LangChain integration\\nIntegration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards. LangChain components are subclasses of base classes in langchain-core . Examples include chat models , tools , retrievers , and more. Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each. Chat models are subclasses of the BaseChatModel class. They implement methods for generating chat completions, handling message formatting, and managing model parameters. The chat model integration guide is currently WIP. In the meantime, read the chat model conceptual guide for details on how LangChain chat models function. Tools are used in 2 main ways: To define an \"input schema\" or \"args schema\" to pass to a chat model\\'s tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage. The Tools class must inherit from the BaseTool base class. This interface has 3 properties and 2 methods that should be implemented in a subclass. The tools integration guide is currently WIP. In the meantime, read the tools conceptual guide for details on how LangChain tools function. Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class. The retriever integration guide is currently WIP. In the meantime, read the retriever conceptual guide for details on how LangChain retrievers function. All vector stores must inherit from the VectorStore base class. This interface consists of methods for writing, deleting and searching for documents in the vector store. See the vector store integration guide for details on implementing a vector store integration. The vector store integration guide is currently WIP. In the meantime, read the vector store conceptual guide for details on how LangChain vector stores function. Embedding models are subclasses of the Embeddings class. The embedding model integration guide is currently WIP. In the meantime, read the embedding model conceptual guide for details on how LangChain embedding models function. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\\n\\nRetrievers\\nRetrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class. The retriever integration guide is currently WIP. In the meantime, read the  retriever conceptual guide  for details on how LangChain retrievers function.\\n\\n', 'id': 'lc_e6dea54b-65e6-4e60-9a3a-6984d388ba35'}, {'type': 'text', 'text': 'Title: Vector Stores\\nLink: https://docs.langchain.com/oss/javascript/contributing/implement-langchain\\nContent: All vector stores must inherit from the  VectorStore  base class. This interface consists of methods for writing, deleting and searching for documents in the vector store. See the  vector store integration guide  for details on implementing a vector store integration. The vector store integration guide is currently WIP. In the meantime, read the  vector store conceptual guide  for details on how LangChain vector stores function.\\n\\nImplement a LangChain integration\\nIntegration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards. LangChain components are subclasses of base classes in langchain-core . Examples include chat models , tools , retrievers , and more. Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each. Chat models are subclasses of the BaseChatModel class. They implement methods for generating chat completions, handling message formatting, and managing model parameters. The chat model integration guide is currently WIP. In the meantime, read the chat model conceptual guide for details on how LangChain chat models function. Tools are used in 2 main ways: To define an \"input schema\" or \"args schema\" to pass to a chat model\\'s tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage. The Tools class must inherit from the BaseTool base class. This interface has 3 properties and 2 methods that should be implemented in a subclass. The tools integration guide is currently WIP. In the meantime, read the tools conceptual guide for details on how LangChain tools function. Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class. The retriever integration guide is currently WIP. In the meantime, read the retriever conceptual guide for details on how LangChain retrievers function. All vector stores must inherit from the VectorStore base class. This interface consists of methods for writing, deleting and searching for documents in the vector store. See the vector store integration guide for details on implementing a vector store integration. The vector store integration guide is currently WIP. In the meantime, read the vector store conceptual guide for details on how LangChain vector stores function. Embedding models are subclasses of the Embeddings class. The embedding model integration guide is currently WIP. In the meantime, read the embedding model conceptual guide for details on how LangChain embedding models function. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.\\n\\nRetrievers\\nRetrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class. The retriever integration guide is currently WIP. In the meantime, read the  retriever conceptual guide  for details on how LangChain retrievers function.\\n\\n', 'id': 'lc_2ed7621e-5728-4271-8fa0-3e6edb25d5a5'}, {'type': 'text', 'text': 'Title: undefined\\nLink: https://docs.langchain.com/\\nContent: ', 'id': 'lc_87f9c637-e16a-47d4-88e9-92ab303334fa'}, {'type': 'text', 'text': 'Title: undefined\\nLink: https://docs.langchain.com/\\nContent: ', 'id': 'lc_a66727f8-e891-4c80-ae23-d2659f75d8cd'}, {'type': 'text', 'text': \"Title: Storing documents\\nLink: https://docs.langchain.com/oss/javascript/langchain/rag\\nContent: Now we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial , our approach is to embed the contents of each document split and insert these embeddings into a vector store . Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial. await vectorStore.addDocuments(allSplits);\\n\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial , our approach is to embed the contents of each document split and insert these embeddings into a vector store . Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial. Go deeper Embeddings : Wrapper around a text embedding model, used for converting text to embeddings. Integrations : 30+ integrations to choose from. Interface : API reference for the base interface. VectorStore : Wrapper around a vector database, used for storing and querying embeddings. Integrations : 40+ integrations to choose from. Interface : API reference for the base interface. This completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\nSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs. To handle this we'll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time. As in the semantic search tutorial , we use a RecursiveCharacterTextSplitter , which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\n\\n\", 'id': 'lc_df33d25c-7f63-4898-8986-e28a222b768c'}]\n",
      "To convert your PDF documents into a vector store using LangChain, you can follow these general steps:\n",
      "\n",
      "1. **Extract Text from PDFs**: First, you need to extract the text content from your PDF files. You can use libraries like PyMuPDF, pdfminer, or PyPDF2 in Python to read the PDF files and extract the text.\n",
      "\n",
      "2. **Split the Text**: Since documents can be lengthy, it's often beneficial to split the extracted text into smaller chunks. This can be done using a text splitter, such as `RecursiveCharacterTextSplitter`, which splits the text based on common separators (like new lines) until each chunk is of an appropriate size.\n",
      "\n",
      "3. **Embed the Text**: Use an embedding model to convert the text chunks into numerical vectors. LangChain provides various embedding models that can be used for this purpose.\n",
      "\n",
      "4. **Store in a Vector Store**: After obtaining the embedding vectors, you can store them in a vector store. You can use the `addDocuments` method to add your document chunks to the vector store.\n",
      "\n",
      "5. **Query the Vector Store**: Once your documents are indexed in the vector store, you can perform similarity searches by querying the vector store with new text inputs.\n",
      "\n",
      "Hereâ€™s a simplified code example to illustrate these steps:\n",
      "\n",
      "```python\n",
      "from langchain.embeddings import YourEmbeddingModel\n",
      "from langchain.vectorstores import YourVectorStore\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from your_pdf_extraction_library import extract_text_from_pdf\n",
      "\n",
      "# Step 1: Extract text from PDF\n",
      "pdf_text = extract_text_from_pdf(\"your_document.pdf\")\n",
      "\n",
      "# Step 2: Split the text into chunks\n",
      "text_splitter = RecursiveCharacterTextSplitter()\n",
      "chunks = text_splitter.split_text(pdf_text)\n",
      "\n",
      "# Step 3: Embed the text chunks\n",
      "embedding_model = YourEmbeddingModel()\n",
      "embedded_chunks = [embedding_model.embed(chunk) for chunk in chunks]\n",
      "\n",
      "# Step 4: Store in a vector store\n",
      "vector_store = YourVectorStore()\n",
      "vector_store.add_documents(embedded_chunks)\n",
      "\n",
      "# Step 5: Query the vector store\n",
      "query_vector = embedding_model.embed(\"Your query text\")\n",
      "results = vector_store.similarity_search(query_vector)\n",
      "```\n",
      "\n",
      "### Additional Resources\n",
      "- For more detailed information on vector stores, you can refer to the [LangChain Vector Stores documentation](https://docs.langchain.com/oss/python/integrations/vectorstores/index).\n",
      "- If you need to handle specific integrations or configurations, check the [LangChain integrations guide](https://docs.langchain.com/oss/python/langchain/knowledge-base).\n",
      "\n",
      "This process will allow you to effectively convert your PDF documents into a searchable vector store using LangChain.\n"
     ]
    }
   ],
   "source": [
    "# let's use it\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "prompt=\"\"\"\n",
    "You are an AI assistant specialized in LangChain Documentation. \n",
    "You possess a tool to search into the documentation. \n",
    "You can use this several times to improve your knowledge base before answering the user's question. \n",
    "\"\"\"\n",
    "\n",
    "docs_agent = create_agent(\n",
    "    model=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    "    tools=LG_tools,\n",
    "    system_prompt=prompt\n",
    ")\n",
    "\n",
    "res = await docs_agent.ainvoke({\"messages\" : [HumanMessage(content=\"How should i make my documents (let's say they are pdfs) into a vector store?\")]})\n",
    "\n",
    "for message in res['messages']:\n",
    "    print(message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
