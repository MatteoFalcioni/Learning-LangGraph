{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c991322",
   "metadata": {},
   "source": [
    "# Voice Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1399d9",
   "metadata": {},
   "source": [
    "For this project we will be creating a voice agent \"the sandwich way\", meaning that we implement a pipeline like:\n",
    "```\n",
    "TTS -> AGENT -> STT\n",
    "```\n",
    "Another common implementation is to use a whole voice agent alltogether, but we lose control over the different parts and usually we cannot use the most recent llms.\n",
    "\n",
    "On the other hand, \"the sandwich\" leaves us with more liability and is less production ready. Needs more work. \n",
    "\n",
    "> **NOTE:** Also, one thing that's not always said when talking about this sandwich architcture is that there is actually an additional step: VAD (Voice Activity Detection) which detects when the user is speaking and when he/she is done. We can skip this if we record by pressing a button, but again, less natural-conversation-like feeling.\n",
    "\n",
    "Reference: https://docs.langchain.com/oss/python/langchain/voice-agent#overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6dd92e",
   "metadata": {},
   "source": [
    "You can find the full working implementation here: [projects/voice agents](../projects/voice_agents/README.md). We will go over the highlights of the project in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270a8b7",
   "metadata": {},
   "source": [
    "## Speech to Text (STT)\n",
    "\n",
    "We can implement STT in different ways, but mainly we either:\n",
    "- stream a transcription model\n",
    "- input an audio file into a transcription model (no streaming)\n",
    "\n",
    "In [STT](../projects/voice_agents/src/STT/README.md) you can find 3 examples of STT: two of these use [Deepgram](https://developers.deepgram.com/home), one uses Whisper from OpenAI. \n",
    "\n",
    "Using Deepgram we can directly stream and reduce latency: Whisper needs files to transcribe, so must first save to files and then transcribe -> high latency (but also, hig accuracy). \n",
    "\n",
    "Key differences: \n",
    "\n",
    "- Deepgram `Nova3`: streaming transcription model, efficient\n",
    "- Deepgram `Flux`: AI transcription model, automatically detects pauses (integrated VAD): perfect for voice agents\n",
    "- OpenAI's `Whisper`: high accuracy, does not stream -> high latency\n",
    "\n",
    "For the actual implementation we use the flux model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b6d60",
   "metadata": {},
   "source": [
    "## Text To Speech \n",
    "\n",
    "For the text to speech part there exist some lightweight models that you can run locally on your machine (you do not even need a GPU, they can run on CPU: mind tho that running on CPU they will be slower and you'll lose that feeling of natural, 0 latency models. But.. they are free.)\n",
    "\n",
    "So, we use [Kokoro](https://huggingface.co/hexgrad/Kokoro-82M): check [the tts readme](../projects/voice_agents/src/TTS/README.md) for how to download it from Hugging Face. We run it on cpu with onnx optimization, but you can modify the code to run it on gpu if you have one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82997f47",
   "metadata": {},
   "source": [
    "## Graph\n",
    "\n",
    "### Constructing Tools from Api's\n",
    "\n",
    "We implement some arxiv api [tools](../projects/voice_agents/src/graph/tools.py) by using some helpers functions that leverage the arxiv api, defined in [`arxiv_helpers`](../projects/voice_agents/src/graph/arxiv_helpers/arxiv_functions). \n",
    "\n",
    "This is a nice example of how to construct tools from an api: you'll notice that we do all the work inside the helper functions, but the process is shadowed from the agent: the actual tools are something like\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def download_pdf(paper_id: Annotated[str, \"The ID of the paper to download from the arXiv\"]):\n",
    "    \"\"\"\n",
    "    Download the PDF of a paper from arXiv, given its ID.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to download paper with id: {paper_id}...\")\n",
    "\n",
    "    return download_arxiv_pdf(paper_id)\n",
    "```\n",
    "\n",
    "but `download_arxiv_pdf()` is defined as:\n",
    "\n",
    "```python\n",
    "def download_arxiv_pdf(paper_id: str, save_dir: str = \"./downloads\"):\n",
    "    \"\"\"\n",
    "    Downloads the PDF of an arXiv paper given its ID.\n",
    "    \n",
    "    Args:\n",
    "        paper_id (str): The arXiv ID (e.g., \"2103.00020\").\n",
    "        save_dir (str): The directory to save the PDF in. Defaults to \"./downloads\".\n",
    "    \n",
    "    Returns:\n",
    "        str: The file path of the downloaded PDF.\n",
    "    \"\"\"\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    # We must \"search\" by ID to get the paper object\n",
    "    search = arxiv.Search(id_list=[paper_id])\n",
    "    \n",
    "    try:\n",
    "        paper = next(client.results(search))\n",
    "        \n",
    "        # Create a safe filename using the ID and a sanitized title\n",
    "        # e.g., \"2103.00020_Attention_Is_All_You_Need.pdf\"\n",
    "        safe_title = \"\".join(c for c in paper.title if c.isalnum() or c in (' ', '_', '-')).rstrip()\n",
    "        safe_title = safe_title.replace(\" \", \"_\")\n",
    "        filename = f\"{paper_id}_{safe_title}.pdf\"\n",
    "        \n",
    "        # Download\n",
    "        path = paper.download_pdf(dirpath=save_dir, filename=filename)\n",
    "        return f\"Successfully downloaded file to: {path}\"\n",
    "        \n",
    "    except StopIteration:\n",
    "        return f\"Error: Paper with ID {paper_id} not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error downloading paper: {str(e)}\"\n",
    "```\n",
    "\n",
    "It doesn't make sense for the agent to know all the work that goes on for downloading a pdf. It would just fill its context and let it be proner to errors, given the higher complexity. **Always keep tools as simple as possible**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7e3db",
   "metadata": {},
   "source": [
    "### Details\n",
    "\n",
    "- You'll notice that we use Human In the Loop throught the human in the loop middleware on `download_pdf`: we want to be sure before downloading. \n",
    "\n",
    "- The function that handles streams is [`stream_graph_task`](../projects/voice_agents/src/graph/executor.py): you'll notice that it also handles interrupts. we need a function like this because then we pass it inside the flux model's stream. \n",
    "\n",
    "- The nice cli look is made through the use of the `rich` library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
