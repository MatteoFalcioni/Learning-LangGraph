[
    {
        "index": 0,
        "title": "SimpleQA Accuracy under three rephrasing-epoch configurations",
        "description": "This table compares the SimpleQA Accuracy under three different configurations related to data rephrasing and the number of training epochs. The first row shows the accuracy for raw wiki-text repeated for 10 epochs (23.76). The second row shows the accuracy for data rephrased once and repeated for 10 epochs (27.39). The third row shows the accuracy for data rephrased 10 times with a single training pass (28.94). The table demonstrates that rephrasing data leads to improved accuracy, with more rephrasing and fewer epochs yielding better results."
    },
    {
        "index": 1,
        "title": "Architectural comparison between Kimi K2 and DeepSeek-V3",
        "description": "This table provides a detailed comparison of architectural parameters between Kimi K2 and DeepSeek-V3. It includes metrics such as #Layers, Total Parameters, Activated Parameters, Experts (total), Experts Active per Token, Shared Experts, Attention Heads, Number of Dense Layers, and Expert Grouping. Kimi K2 has 61 layers, 1.04T total parameters (\u219154% from DeepSeek-V3), 32.6B activated parameters (\u219313% from DeepSeek-V3), 384 total experts (\u219150% from DeepSeek-V3), 8 active experts per token, 1 shared expert, 64 attention heads (\u219350% from DeepSeek-V3), and 1 dense layer (\u219367% from DeepSeek-V3). Unlike DeepSeek-V3, Kimi K2 does not use expert grouping."
    },
    {
        "index": 2,
        "title": "Performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across diverse tasks.",
        "description": "This table presents a comprehensive performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across a diverse set of tasks categorized into Coding Tasks, Tool Use Tasks, Math & STEM Tasks, and General Tasks. For each benchmark, the table lists the scores for Kimi-K2-Instruct, DeepSeek-V3-0324, Qwen3-235B-A22B (open-source), Claude Sonnet 4, Claude Opus 4, GPT-4.1, and Gemini 2.5 Flash (proprietary). Key highlights in Coding Tasks include Kimi-K2-Instruct's strong performance on LiveCodeBench v6 (53.7) and OJBench (27.1), and notable scores on SWE-bench Verified. In Tool Use Tasks, Kimi-K2-Instruct leads on Tau2 retail (70.6), Tau2 airline (56.5), Tau2 telecom (65.8), and AceBench (76.5). For Math & STEM Tasks, Kimi-K2-Instruct shows excellent results on AIME 2024 (69.6), AIME 2025 (49.5), MATH-500 (97.4), HMMT 2025 (38.8), CNMO 2024 (74.3), PolyMath-en (65.1), ZebraLogic (89.0), AutoLogi (89.5), GPQA-Diamond (75.1), and SuperGPQA (57.2). In General Tasks, Kimi-K2-Instruct performs well on MMLU (89.5), MMLU-Redux (92.7), MMLU-Pro (81.1), IFEval (89.8), Multi-Challenge (54.1), SimpleQA (31.0), Livebench (76.4), Arena Hard v2.0 (54.5% win rate for hard prompts, 85.0% for creative writing), FACTS Grounding (88.5), HHEM v2.1 (98.9), and FaithJudge (92.6). Long context tasks like LongBench v2 (49.1), FRAMES (77.1), MRCR (55.0), and DROP (93.5) are also included. Bold values indicate global SOTA, and underlined bold indicates the best open-source result."
    },
    {
        "index": 3,
        "title": "Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks.",
        "description": "This table provides a comprehensive comparison of Kimi-K2-Base against leading open-source models across diverse tasks, grouped by English, Code, Math, and Chinese capabilities. For each benchmark, the table includes the number of shots, and scores for Kimi-K2-Base, DeepSeek-V3-Base, Llama4-Maverick-Base, and Qwen2.5-72B-Base. The architecture for Kimi-K2-Base, DeepSeek-V3-Base, and Llama4-Maverick-Base is MoE, while Qwen2.5-72B-Base is Dense. Key highlights include Kimi-K2-Base achieving state-of-the-art performance in English language understanding for MMLU (87.79), MMLU-pro (69.17), MMLU-redux (90.17), SuperGPQA (44.67), SimpleQA (35.25), TriviaQA (85.09), BBH (88.71) and ARC-Challenge (95.73). For coding capabilities, Kimi-K2-Base leads in CRUXEval-I-cot (74.00), CRUXEval-O-cot (83.50), LiveCodeBench (v6) (26.29), and EvalPlus (80.33). In mathematical reasoning, Kimi-K2-Base excels in MATH (70.22), GSM8k (92.12), and GSM8k-platinum (94.21). For Chinese language capabilities, Kimi-K2-Base achieves top scores in C-Eval (92.50), CMMLU (90.90), and CSimpleQA (77.57)."
    },
    {
        "index": 4,
        "title": "Enabled Plugins and Strategies",
        "description": "This table lists the plugins and strategies evaluated in the safety assessment. The plugins include Harmful (Graphic Content, Harassment and Bullying, Hate Speech, Insults, Profanity, Radicalization, Self Harm, Sexual Content, ToxicChat), Criminal (Chemical & Biological Weapons, Child Exploitation, Copyright Violations, Cybercrime, Illegal Activities, Illegal Drugs, Indiscriminate Weapons, Intellectual Property Violation, Non-Violent Crime, Violent Crime, Sex Crimes), Misinformation (Competitor Endorsement, Unsupervised Contracts, Excessive Agency, Hallucination, Misinformation and Disinformation, Specialized Advice, Unsafe Practices, Imitation, Overreliance, Political Opinions, Religious Sensitivity), Privacy (Privacy Violation, PII in API/Database, Direct PII Exposure, PII in Session Data, PII via Social Engineering), and Security (ASCII Smuggling, CyberSecEval, Harmbench, Debug Access, Divergent Repetition, DoNotAnswer, Malicious Code, Pliny, Prompt Extraction, Reasoning DoS, Tool Discovery). The strategies evaluated alongside these plugins are Basic, Prompt Injection, Iterative Jailbreak, and Crescendo. Each plugin is paired with all listed strategies for evaluation."
    },
    {
        "index": 5,
        "title": "Safety Evaluation Results",
        "description": "This table presents the safety evaluation results, showing the passing rates (%) of different models (Kimi-K2-Instruct, DeepSeek-V3-0324, DeepSeek-R1, Qwen3-235B-A22B) across various plugin-strategy combinations. For Harmful content, Kimi-K2-Instruct generally performs well on Basic and Base64 strategies but shows lower passing rates on Iterative Jailbreak and Crescendo (64.71). For Criminal content, Kimi-K2-Instruct is 100% on Basic but drops significantly on Iterative Jailbreak (57.57) and Crescendo (56.06). For Misinformation, Kimi-K2-Instruct maintains high passing rates for Basic (97.28), Base64 (98.48), and Prompt Injection (98.39), with lower rates on Iterative Jailbreak (63.97) and Crescendo (85.71). For Privacy, all models show high passing rates, with Kimi-K2-Instruct achieving 100% on Basic and Base64 and high scores for other strategies. For Security, Kimi-K2-Instruct has varying performance, with Base64 scoring 82.93 and Iterative Jailbreak being the lowest at 43.90. The table suggests that complex strategies like Iterative Jailbreak and Crescendo pose greater challenges for some models."
    },
    {
        "index": 6,
        "title": "Kimi-K2-Instruct Open-Ended Evaluation (aggregated)",
        "description": "This bar chart titled 'Kimi-K2-Instruct Open-Ended Evaluation (aggregated)' compares the win, tie, and loss percentages of Kimi-K2-Instruct against DeepSeek-V3-0324, Claude-Sonnet-4, and ChatGPT-4o-latest on an aggregated Chinese in-house benchmark. Against DeepSeek-V3-0324, Kimi-K2-Instruct has a 59.6% win rate, 23.5% tie rate, and 16.9% loss rate. Against Claude-Sonnet-4, Kimi-K2-Instruct has a 64.6% win rate, 18.8% tie rate, and 16.6% loss rate. Against ChatGPT-4o-latest, Kimi-K2-Instruct shows a 65.4% win rate, 17.6% tie rate, and 17.0% loss rate. The chart visually represents Kimi-K2-Instruct's strong performance in Chinese open-ended evaluations, consistently achieving higher win rates and lower loss rates compared to the other models tested."
    }
]