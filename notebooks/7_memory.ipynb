{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a09addec",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MatteoFalcioni/Learning-LangGraph/blob/main/notebooks/7_memory.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d0691",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5292894",
   "metadata": {},
   "source": [
    "#### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a1b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/langgraph/\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U -r https://raw.githubusercontent.com/MatteoFalcioni/Learning-LangGraph/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c7b11",
   "metadata": {},
   "source": [
    "#### local (notebooks or files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bab2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load api keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3446aa",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed788f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "REQUIRED_KEYS = [\n",
    "    'OPENAI_API_KEY',\n",
    "    'LANGSMITH_TRACING',\n",
    "    'LANGSMITH_ENDPOINT',\n",
    "    'LANGSMITH_API_KEY',\n",
    "    'LANGSMITH_PROJECT'\n",
    "]\n",
    "\n",
    "def _set_colab_keys(key : str):\n",
    "    # Retrieve the secret value using its key/name\n",
    "    secret_value = userdata.get(key)\n",
    "    # set it as a standard OS environment variable\n",
    "    os.environ[key] = secret_value\n",
    "\n",
    "for key in REQUIRED_KEYS:\n",
    "    _set_colab_keys(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85249b",
   "metadata": {},
   "source": [
    "# Persistence\n",
    "\n",
    "LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every step. \n",
    "\n",
    "Those checkpoints are saved to a `thread` (a unique identifier) which can be accessed after graph execution. \n",
    "\n",
    "Because threads allow access to graph’s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47395e53",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c82972",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997979d",
   "metadata": {},
   "source": [
    "**The main persistence feature we want to have a look at is agents' memory**: its implementations is really straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea63cf",
   "metadata": {},
   "source": [
    "LangGraph provides a memory checkpointer out of the bat, the `InMemorySaver` object:\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "```\n",
    "\n",
    "In order to use this object as our checkpointer, we need to pass it as an argument when we compile our graph: \n",
    "\n",
    "```python\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "```\n",
    "\n",
    "And then we need to specify a thread-id in the config dictionary at invocation, i.e. : \n",
    "\n",
    "```python\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke(init_state, config)\n",
    "```\n",
    "\n",
    "The coice of the thread id is completely arbitrary, I chose \"1\" just for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ca79c",
   "metadata": {},
   "source": [
    "Let's see an actual example with a simple graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236d3a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': ['a', 'b']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import Annotated, Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.types import Command\n",
    "from operator import add\n",
    "\n",
    "class MyState(TypedDict):\n",
    "    foo: Annotated[list[str], add]\n",
    "\n",
    "def node_a(state: MyState) -> Command[Literal[\"node_b\"]]:\n",
    "    return Command(\n",
    "        update={\n",
    "            \"foo\" : [\"a\"]\n",
    "        },\n",
    "        goto=\"node_b\"\n",
    "    )\n",
    "\n",
    "def node_b(state: MyState) -> Command[Literal[\"__end__\"]]:\n",
    "    return Command(\n",
    "        update={\n",
    "            \"foo\" : [\"b\"]\n",
    "        },\n",
    "        goto=\"__end__\"\n",
    "    )\n",
    "\n",
    "builder = StateGraph(MyState)\n",
    "builder.add_node(\"node_a\", node_a)\n",
    "builder.add_node(\"node_b\", node_b)\n",
    "builder.add_edge(START, \"node_a\")\n",
    "# no edges a->b->END (using Command!)\n",
    "\n",
    "# instantiate the checkpointer\n",
    "checkpointer = InMemorySaver()\n",
    "# compile the graph w/ the checkpointer\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# invoke the graph with a config dict\n",
    "init_state = {\"foo\": []}\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "graph.invoke(init_state, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f053ece",
   "metadata": {},
   "source": [
    "Now that we are using a checkpointer, LangGraph automatically saves state checkpoints of our graph during execution. \n",
    "\n",
    "In this graph run, we expect to see 4 checkpoints: \n",
    "\n",
    "- Empty checkpoint with `START` as the next node to be executed\n",
    "- Checkpoint with the user input `{'foo': []}` and `node_a` as the next node to be executed\n",
    "- Checkpoint with the outputs of `node_a` `{'foo': ['a']}` and `node_b` as the next node to be executed\n",
    "- Checkpoint with the outputs of `node_b` `{'foo': ['a', 'b']}` and no next nodes to be executed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f0297",
   "metadata": {},
   "source": [
    "We can view the last state of the graph by calling `graph.get_state(config)` : this will return a `StateSnapshot` object corresponding to the latest checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809c32e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'foo': ['a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04cc-6017-8002-de12a6228bac'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-12-15T09:57:17.068898+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04ca-60aa-8001-d6eff0508203'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the latest state snapshot\n",
    "graph.get_state(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22856abd",
   "metadata": {},
   "source": [
    "\n",
    "You can get the full history of the graph execution for a given thread by calling `graph.get_state_history(config)`. \n",
    "\n",
    "This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. \n",
    "\n",
    "Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae17616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'foo': ['a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04cc-6017-8002-de12a6228bac'}}, metadata={'source': 'loop', 'step': 2, 'parents': {}}, created_at='2025-12-15T09:57:17.068898+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04ca-60aa-8001-d6eff0508203'}}, tasks=(), interrupts=())\n",
      "StateSnapshot(values={'foo': ['a']}, next=('node_b',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04ca-60aa-8001-d6eff0508203'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-12-15T09:57:17.068094+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04c7-66be-8000-3ff5fb122b6f'}}, tasks=(PregelTask(id='1f65d63b-6d7d-7ed9-2d97-a9cbd57411a3', name='node_b', path=('__pregel_pull', 'node_b'), error=None, interrupts=(), state=None, result={'foo': ['b']}),), interrupts=())\n",
      "StateSnapshot(values={'foo': []}, next=('node_a',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04c7-66be-8000-3ff5fb122b6f'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-12-15T09:57:17.067016+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04c5-6143-bfff-7dd0405f1509'}}, tasks=(PregelTask(id='d7a19202-7524-4aab-2fe3-eedb972b3bfe', name='node_a', path=('__pregel_pull', 'node_a'), error=None, interrupts=(), state=None, result={'foo': ['a']}),), interrupts=())\n",
      "StateSnapshot(values={'foo': []}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0d99c7-04c5-6143-bfff-7dd0405f1509'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-12-15T09:57:17.066058+00:00', parent_config=None, tasks=(PregelTask(id='9449c741-531b-ee26-520a-cd4a77036321', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'foo': []}),), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "history = graph.get_state_history(config)\n",
    "\n",
    "for checkpoint in history:\n",
    "    print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68180e",
   "metadata": {},
   "source": [
    "It’s also possible to play-back a prior graph execution. If we invoke a graph with a `thread_id` and a `checkpoint_id`, then we will re-play the previously executed steps before a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps after the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355d7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'foo': ['a', 'b']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1f0d99c7-04ca-60aa-8001-d6eff0508203\"}}  # took this checkpoint id from the above output\n",
    "graph.invoke(None, config=config)  # execute from that checkpoint on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b22e0",
   "metadata": {},
   "source": [
    "This type of jumping back to a node, replaying execution or eventually modifying the graph state and continuing  from a given checkpoint, is reffered to as \"time travel\" in LangGraph. \n",
    "\n",
    "If you want to know more, see [notebook 9 - second part](9_human_in_the_loop.ipynb), or check this [LangChain time travel guide](https://docs.langchain.com/oss/python/langgraph/use-time-travel). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c748c",
   "metadata": {},
   "source": [
    "## Agents With Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df0f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.agents import AgentState, create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.types import Command\n",
    "\n",
    "def reduce_str(left : str | None, right : str | None) -> str:\n",
    "    \"\"\"Reduce two strings by replacing.\"\"\"\n",
    "    if left is None:\n",
    "        return \"\"\n",
    "    if right is None:\n",
    "        return \"\"\n",
    "    return right\n",
    "\n",
    "class MyState(AgentState):\n",
    "    user_id: Annotated[str, reduce_str]\n",
    "\n",
    "@tool\n",
    "def get_user_info(runtime: ToolRuntime) -> Command:\n",
    "    \"\"\"Look up user info.\"\"\"\n",
    "    user_id = runtime.state[\"user_id\"]  \n",
    "    result_string = \"User is Matteo\" if user_id == \"user_123\" else \"Unknown user\"\n",
    "    return Command(\n",
    "        update = {\n",
    "            \"messages\" : [ToolMessage(content=result_string, tool_call_id=runtime.tool_call_id)]\n",
    "        }\n",
    "    )\n",
    "\n",
    "agent = create_agent(\n",
    "    model=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    "    tools=[get_user_info],\n",
    "    system_prompt=\"You are a helpful assistant that provides user information based on user ID.\",\n",
    "    state_schema=MyState,\n",
    ")\n",
    "\n",
    "def agent_node(state : MyState) -> Command[Literal[\"__end__\"]]:\n",
    "    \n",
    "    result = agent.invoke(state)\n",
    "    messages = result[\"messages\"]\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": messages\n",
    "        },\n",
    "        goto=\"__end__\"\n",
    "    )\n",
    "\n",
    "builder = StateGraph(MyState)\n",
    "builder.add_node(\"agent_node\", agent_node)\n",
    "builder.add_edge(START, \"agent_node\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "init_state = {\"messages\" : [HumanMessage(content=\"Get user info\")], \"user_id\": \"user_123\"}\n",
    "config = {\"configurable\": {\"thread_id\": \"test_123\"}}\n",
    "\n",
    "result = graph.invoke(init_state, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44cedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Get user info\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_user_info (call_rhoOYHGnsOSj4TgNcP85p4PI)\n",
      " Call ID: call_rhoOYHGnsOSj4TgNcP85p4PI\n",
      "  Args:\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_user_info\n",
      "\n",
      "User is Matteo\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The user is Matteo.\n"
     ]
    }
   ],
   "source": [
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2108f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your first question was \"Get user info,\" and my answer was \"The user is Matteo.\"\n"
     ]
    }
   ],
   "source": [
    "second_message = HumanMessage(content=\"What was my first question? ANd what was your answer?\")\n",
    "new_state = {\"messages\" : [second_message], \"user_id\": \"\"}\n",
    "\n",
    "for chunk in graph.stream(new_state, config=config):  # same config as before (same thread)\n",
    "    \n",
    "    for node_name, values in chunk.items():\n",
    "        if 'messages' in values:\n",
    "            values['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce854d0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591b45f",
   "metadata": {},
   "source": [
    "Another very useful concept I recommend you to have a look at is memory stores. \n",
    "\n",
    "The idea behind this is: \"*what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across **all** chat conversations (e.g., threads) with that user...*\"\n",
    "\n",
    "You can imagine how useful this can be - all the Chatbots you find online have this feature in one way or another. \n",
    "\n",
    "We will not go through the details of this implementation, but you can find details here: [Memory Store](https://docs.langchain.com/oss/python/langgraph/persistence#memory-store)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
