{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ef7d50",
   "metadata": {},
   "source": [
    "# RAG Agent\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is one of the most useful applications of AI agents. \n",
    "\n",
    "It consists in grounding the answers of our agent to a given knowledge base, that our agent can access by using tools.\n",
    "\n",
    "This knowledge base - i.e., our collection of documents - is embedded in a vector store for efficient search. \n",
    "\n",
    "This method prevents allucinations and highly increases the accuracy of the agentic system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcc8bd",
   "metadata": {},
   "source": [
    "Steps in this implementation:\n",
    "\n",
    "1. get a knowledge base: we do that manually, and for this example we use only one document for simplicity. \n",
    "\n",
    "2. perform OCR of our documents to best extract information;\n",
    "\n",
    "3. embed our documents in a vector database (vector store)\n",
    "\n",
    "4. construct a tool for searching in the database\n",
    "\n",
    "5. create the graph with grading and retries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451864b",
   "metadata": {},
   "source": [
    "## 1. Knowledge Base\n",
    "\n",
    "We selected (manually) the Kimi K2 paper.\n",
    "\n",
    "We stored it in the folder `documents/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa53524",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. OCR (Optical Character Recognition)\n",
    "\n",
    "As we said, we usually need to perform OCR on a knowledge based composed by documents. \n",
    "\n",
    ">Notice that we may skip this step if our knowledge base is already composed of plain text (for example, this could happen if our base is made thorugh web scarping, which usually returns plain text or markdown).\n",
    "\n",
    "In this case our papers are complex, and contain mathematical expressions, tables, images. OCR is not a simple/skippable step here. \n",
    "\n",
    "That's why we are going to user the best OCR model around this days (January 2026): Mistral OCR 3. Find a usage example in the [mistral_ocr](./mistral_ocr.ipynb) notebook, and a full example from the actual Mistral site here: [link](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/data_extraction.ipynb). \n",
    "\n",
    "The latter showcases how to use their `Annotations` API to also annotate the detected bounding boxes. We will use this feature to include images in our RAG pipeline. \n",
    "\n",
    "> **Note:** You can replace Mistral's OCR API with a free process provided by LangChain, using `PyMuPDF4LLM` and the `Upstage Document Parse API`. Results will not be as good as using Mistral but probably will be good enough for many applications (and it's all free). Check the full tutorial here: [Multimodal RAG tutorial](https://langchain-opentutorial.gitbook.io/langchain-opentutorial/19-cookbook/06-multimodal/10-geminimultimodalrag#layout-parsing-to-extract-image-from-pdf-using-upstage-document-parse-api). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a246380",
   "metadata": {},
   "source": [
    "### 2.1 Mistral OCR with Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdef9f",
   "metadata": {},
   "source": [
    "Mistral Document AI API adds two annotation functionalities:\n",
    "\n",
    "- `document_annotation`: returns the annotation of the entire document based on the input schema.\n",
    "- `box_annotation`: gives you the annotation of the bboxes extracted by the OCR model (charts/ figures etc) based on user requirement. The user may ask to describe/caption the figure for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d8b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U mistralai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca03c3",
   "metadata": {},
   "source": [
    "Function to encode in base 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4127de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_pdf(pdf_path):\n",
    "    \"\"\"Encode the pdf to base64.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            return base64.b64encode(pdf_file.read()).decode('utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {pdf_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:  # Added general exception handling\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0a25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_pdf = encode_pdf(\"RAG/documents/KimiK2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d769d",
   "metadata": {},
   "source": [
    "First, we need to create our Annotation Formats, for that we advise make use of pydantic.\n",
    "\n",
    "For this example, we will extract the image type and a description of each bbox; as well as the language, authors and a summary of the full document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7dcaa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "class ImageType(str, Enum):\n",
    "    GRAPH = \"graph\"\n",
    "    TEXT = \"text\"\n",
    "    TABLE = \"table\"\n",
    "    IMAGE = \"image\"\n",
    "\n",
    "class Image(BaseModel):\n",
    "    image_type: ImageType = Field(..., description=\"The type of the image. Must be one of 'graph', 'text', 'table' or 'image'.\")\n",
    "    description: str = Field(..., description=\"A description of the image.\")\n",
    "\n",
    "class Document(BaseModel):\n",
    "    summary: str = Field(..., description=\"A summary of the document.\")\n",
    "    authors: list[str] = Field(..., description=\"A list of authors who contributed to the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4bd4e",
   "metadata": {},
   "source": [
    "Now with our pydantic models created for our Annotations, we can call our OCR endpoint.\n",
    "\n",
    "The objective is to Annotate and Extract information from our document and the bbox/images detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50853dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pages\": [\n",
      "        {\n",
      "            \"index\": 16,\n",
      "            \"markdown\": \"##### Agentic Tool Use\\n\\nOn multi-turn tool-use benchmarks, Kimi-K2-Instruct sets a new standard. It achieves 66.1 Pass@1 on $\\\\tau^{2}$-Bench and 76.5 on ACEBench, substantially outperforming all baselines. These results affirm its strength in grounded, controlled, and agent-driven tool orchestration across domains.\\n\\n##### General Capabilities\\n\\nKimi-K2-Instruct exhibits strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. It surpasses open-source peers on SimpleQA (31.0%), MMLU (89.5%) and MMLU-Redux (92.7%), and leads all models on instruction benchmarks (IFEval: 89.8%, Multi-Challenge: 54.1%). In math and STEM, it achieves top-tier scores (AIME 2024: 69.6%, GPQA-Diamond: 75.1%), and remains competitive on long-context factuality and retrieval (DROP: 93.5%, MRCR: 55.0%). These results position Kimi-K2-Instruct as a well-rounded and capable generalist across both short- and long-context settings.\\n\\n##### Open-Ended Evaluation\\n\\nOn the LMSYS Arena leaderboard (July 17, 2025), Kimi-K2-Instruct ranks as the top-1 open-source model and 5th overall based on over 3,000 user votes. This real-world preference signal\\u2014across diverse, blind prompts\\u2014underscores Kimi-K2\\u2019s strengths in generating high-quality responses on open-ended tasks.\\n\\n### 4.2 Pre-training Evaluations\\n\\n#### 4.2.1 Evaluation Settings\\n\\n##### Benchmarks\\n\\nWe evaluate Kimi-K2-Base across diverse capability areas. For general capabilities, we assess on MMLU *[23]*, MMLU-Pro *[76]*, MMLU-Redux *[17]*, BBH *[67]*, TriviaQA *[34]*, SuperGPQA *[13]*, SimpleQA *[78]*, HellaSwag *[88]*, AGIEval *[89]*, GPQA-Diamond *[61]*, ARC-Challenge *[8]*, and WinoGrande *[62]*. For coding capabilities, we employ EvalPlus *[45]* (averaging HumanEval *[7]*, MBPP *[1]*, HumanEval+, and MBPP+), LiveCodeBench v6 *[31]*, and CRUXEval *[18]*. For mathematical reasoning, we utilize GSM8K *[9]*, GSM8K-Platinum *[74]*, MATH *[24]*, and CMATH *[79]*. For Chinese language capabilities, we evaluate on C-Eval *[29]*, CMMLU *[40]*, and CSimpleQA *[22]*.\\n\\n##### Baselines\\n\\nWe benchmark against leading open-source foundation models: DeepSeek-V3-Base *[10]*, Qwen2.5-72B-Base *[59]* (Note that Qwen3-235B-A22B-Base is not open-sourced, and the largest open-sourced base model in the Qwen series is Qwen2.5-72B-Base), and Llama 4-Maverick *[70]* (Llama 4-Behemoth is also not open-sourced). All models are evaluated under identical configurations to ensure fair comparison.\\n\\n##### Evaluation Configurations\\n\\nWe employ perplexity-based evaluation for MMLU, MMLU-Redux, GPQA-Diamond, HellaSwag, ARC-Challenge, C-Eval, and CMMLU. Generation-based evaluation is used for MMLU-Pro, SuperGPQA, TriviaQA, BBH, CSimpleQA, MATH, CMATH, GSM8K, GSM8K-Platinum, CRUXEval, LiveCodeBench, and EvalPlus. To mitigate the high variance inherent to GPQA-Diamond, we report the mean score across eight independent runs. All evaluations are conducted using our internal framework derived from LM-Harness-Evaluation *[4]*, ensuring consistent settings across all models.\\n\\n#### 4.2.2 Evaluation Results\\n\\nTable 4 presents a comprehensive comparison of Kimi-K2-Base against leading open-source foundation models across diverse evaluation benchmarks. The results demonstrate that Kimi-K2-Base achieves state-of-the-art performance across the majority of evaluated tasks, establishing it as a leading foundation model in the open-source landscape.\\n\\n##### General Language Understanding\\n\\nKimi-K2-Base achieves state-of-the-art performance on 10 out of 12 English language benchmarks. Notable results include MMLU (87.79%), MMLU-Pro (69.17%), MMLU-Redux (90.17%), SuperGPQA (44.67%), and SimpleQA (35.25%), significantly outperforming all baselines.\\n\\n##### Coding Capabilities\\n\\nOn coding benchmarks, Kimi-K2-Base sets new standards with leading performance across all metrics. It achieves 74.00% on CRUXEval-I-cot, 83.50% on CRUXEval-O-cot, 26.29% on LiveCodeBench v6, and 80.33% on EvalPlus, demonstrating superior code generation and comprehension abilities, particularly in scenarios requiring step-by-step reasoning.\\n\\n##### Mathematical Reasoning\\n\\nKimi-K2-Base exhibits exceptional mathematical capabilities, leading on three out of four benchmarks: MATH (70.22%), GSM8K (92.12%), and GSM8K-Platinum (94.21%). It maintains competitive performance on CMATH (90.26%), narrowly behind DeepSeek-V3-Base (90.53%). These results highlight the model\\u2019s robust mathematical problem-solving abilities across varying difficulty levels.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 17,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\nChinese Language Understanding The model demonstrates superior multilingual capabilities, achieving state-of-the-art results across all Chinese language benchmarks: C-Eval (92.50%), CMMLU (90.90%), and CSimpleQA (77.57%). These results establish Kimi-K2-Base as a leading model for Chinese language understanding while maintaining strong performance across other languages.\\n\\nTable 4: Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks.\\n\\n[tbl-0.html](tbl-0.html)\\n\\n# 4.3 Safety Evaluation\\n\\n# 4.3.1 Experiment Settings\\n\\nWe conducted red-teaming evaluations on Kimi K2 compare with other open-source LLMs. The evaluation covered a range of attack scenarios\\u2014including harmful content, privacy content, and security content, as well as different attack strategies such as prompt injection and iterative jailbreak.\\n\\nWe choose Promptfoo to generate adversarial prompts and analyze the responses. By this way, we can evaluate model in a scalable ways.\\n\\nModel Selection We compare Kimi K2 with three other open-source LLMs: DeepSeek-V3, DeepSeek-R1, and Qwen3.\\n\\nPromptfoo Settings Table 5 lists plugins and strategies evaluated, with each plugin paired with all strategies to assess their performance.\\n\\nTest Case Count Given the inherent non-determinism of large language model inference, single-pass outputs may exhibit variability. To account for this, we generated 3 attack prompts per plugin for each strategy.\\n\\nPrompt Language Settings We pre-tested the language compatibility for each plugin-strategy combination. Some plugins support both English and Chinese, while others only support English. For combinations that support both, we generated 3 prompts in each language, resulting in 6 prompts per combination.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [\n",
      "                {\n",
      "                    \"id\": \"tbl-0.html\",\n",
      "                    \"content\": \"<table><tr><td></td><td>Benchmark (Metric)</td><td>#Shots</td><td>Kimi-K2-Base</td><td>DeepSeek-V3-Base</td><td>Llama4-Maverick-Base</td><td>Qwen2.5-72B-Base</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>MoE</td><td>MoE</td><td>MoE</td><td>Dense</td></tr><tr><td></td><td># Activated Params</td><td>-</td><td>32B</td><td>37B</td><td>17B</td><td>72B</td></tr><tr><td></td><td># Total Params</td><td>-</td><td>1043B</td><td>671B</td><td>400B</td><td>72B</td></tr><tr><td rowspan=\\\"12\\\">English</td><td>MMLU</td><td>5-shots</td><td>87.79</td><td>87.10</td><td>84.87</td><td>86.08</td></tr><tr><td>MMLU-pro</td><td>5-shots</td><td>69.17</td><td>60.59</td><td>63.47</td><td>62.80</td></tr><tr><td>MMLU-reflux</td><td>5-shots</td><td>90.17</td><td>89.53</td><td>88.18</td><td>87.77</td></tr><tr><td>SuperGPQA</td><td>5-shots</td><td>44.67</td><td>39.20</td><td>38.84</td><td>34.23</td></tr><tr><td>GPQA-Diamond(avg@s)</td><td>5-shots</td><td>48.11</td><td>50.51</td><td>49.43</td><td>40.78</td></tr><tr><td>SimpleQA</td><td>5-shots</td><td>35.25</td><td>26.49</td><td>23.74</td><td>10.31</td></tr><tr><td>TriviaQA</td><td>5-shots</td><td>85.09</td><td>84.11</td><td>79.25</td><td>76.03</td></tr><tr><td>BBH</td><td>3-shots</td><td>88.71</td><td>88.37</td><td>87.10</td><td>84.09</td></tr><tr><td>HellaSwag</td><td>5-shots</td><td>94.60</td><td>89.44</td><td>86.02</td><td>95.27</td></tr><tr><td>AGIEval</td><td>-</td><td>84.23</td><td>81.57</td><td>67.55</td><td>76.87</td></tr><tr><td>ARC-Challenge</td><td>0-shot</td><td>95.73</td><td>93.77</td><td>94.03</td><td>95.56</td></tr><tr><td>WinoGrande</td><td>5-shots</td><td>85.32</td><td>84.21</td><td>77.58</td><td>84.14</td></tr><tr><td rowspan=\\\"4\\\">Code</td><td>CRUXEval-I-cot</td><td>0-shots</td><td>74.00</td><td>62.75</td><td>67.13</td><td>61.12</td></tr><tr><td>CRUXEval-O-cot</td><td>0-shots</td><td>83.50</td><td>75.25</td><td>75.88</td><td>66.13</td></tr><tr><td>LiveCodeBench(v6)</td><td>1-shots</td><td>26.29</td><td>24.57</td><td>25.14</td><td>22.29</td></tr><tr><td>EvalPlus</td><td>-</td><td>80.33</td><td>65.61</td><td>65.48</td><td>66.04</td></tr><tr><td rowspan=\\\"4\\\">Math</td><td>MATH</td><td>4-shots</td><td>70.22</td><td>61.70</td><td>63.02</td><td>62.68</td></tr><tr><td>GSM8k</td><td>8-shots</td><td>92.12</td><td>91.66</td><td>86.35</td><td>90.37</td></tr><tr><td>GSM8k-platinum</td><td>8-shots</td><td>94.21</td><td>93.38</td><td>88.83</td><td>92.47</td></tr><tr><td>CMATH</td><td>6-shots</td><td>90.26</td><td>90.53</td><td>88.07</td><td>86.98</td></tr><tr><td rowspan=\\\"3\\\">Chinese</td><td>C-Eval</td><td>5-shots</td><td>92.50</td><td>90.04</td><td>80.91</td><td>90.86</td></tr><tr><td>CMMLU</td><td>5-shots</td><td>90.90</td><td>88.84</td><td>81.24</td><td>90.55</td></tr><tr><td>CSimpleQA</td><td>5-shots</td><td>77.57</td><td>72.13</td><td>53.47</td><td>50.53</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                }\n",
      "            ],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://github.com/promptfoo/promptfoo\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 18,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\nTable 5: Enabled Plugins and Strategies\\n\\n[tbl-1.html](tbl-1.html)\\n\\nManual Review We incorporated human review into the evaluation process. To minimize subjectivity problem, we conducted multiple rounds of review and assigned the same reviewer to evaluate all cases within a given test set to ensure consistency and reduce variability in judgment.\\n\\n# 4.3.2 Safety Evaluation Results\\n\\nTable 6 presents the passing rates of different models under various plugin-strategy combinations.\\n\\nTable 6: Safety Evaluation Results\\n\\n[tbl-2.html](tbl-2.html)\\n\\nWithout targeted optimization for specific evaluation scenarios, the passing rate of some complex cases (e.g., Harmful-Iterative Jailbreak) was relatively higher compared to other models.\\n\\nAcross different attack strategies, the models exhibited varying trends. Under the Base64 strategy, passing rates generally approached or reached  $100\\\\%$ , suggesting that encoding transformations had minimal impact on the models'\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [\n",
      "                {\n",
      "                    \"id\": \"tbl-1.html\",\n",
      "                    \"content\": \"<table><tr><td rowspan=\\\"5\\\">Plugin</td><td>Harmful</td><td>Graphic Content, Harassment and Bullying, Hate Speech, Insults, Profanity, Radicalization, Self Harm, Sexual Content, ToxicChat</td></tr><tr><td>Criminal</td><td>Chemical&amp;Biological Weapons, Child Exploitation, Copyright Violations, Cybercrime, Illegal Activities, Illegal Drugs, Indiscriminate Weapons, Intellectual Property Violation, Non-Violent Crime, Violent Crime, Sex Crimes</td></tr><tr><td>Misinformation</td><td>Competitor Endorsement, Unsupervised Contracts, Excessive Agency, Hallucination, Misin-formation and Disinformation, Specialized Advice, Unsafe Practices, Imitation, Overreliance, Political Opinions, Religious Sensitivity</td></tr><tr><td>Privacy</td><td>Privacy Violation, PII in API/Database, Direct PII Exposure, PII in Session Data, PII via Social Engineering</td></tr><tr><td>Security</td><td>ASCII Smuggling, CyberSecEval, Harmbench, Debug Access, Divergent Repetition, DoNotAnswer, Malicious Code, Pliny, Prompt Extraction, Reasoning DoS, Tool Discovery</td></tr><tr><td>Strategy</td><td colspan=\\\"2\\\">Basic, Prompt Injection, Iterative Jailbreak, Crescendo</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"tbl-2.html\",\n",
      "                    \"content\": \"<table><tr><td>Plugin</td><td>Strategy</td><td>Kimi-K2-Instruct</td><td>DeepSeek-V3-0324</td><td>DeepSeek-R1</td><td>Qwen3-235B-A22B</td></tr><tr><td rowspan=\\\"5\\\">Harmful</td><td>Basic</td><td>98.04</td><td>90.45</td><td>99.02</td><td>98.53</td></tr><tr><td>Base64</td><td>100</td><td>90.20</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>93.14</td><td>100</td><td>95.10</td><td>99.02</td></tr><tr><td>Iterative Jailbreak</td><td>92.16</td><td>66.67</td><td>72.55</td><td>74.51</td></tr><tr><td>Crescendo</td><td>64.71</td><td>64.71</td><td>80.39</td><td>86.27</td></tr><tr><td rowspan=\\\"5\\\">Criminal</td><td>Basic</td><td>100</td><td>99.62</td><td>95.45</td><td>99.24</td></tr><tr><td>Base64</td><td>96.97</td><td>89.39</td><td>84.85</td><td>98.48</td></tr><tr><td>Prompt Injection</td><td>75.76</td><td>91.67</td><td>69.70</td><td>98.47</td></tr><tr><td>Iterative Jailbreak</td><td>57.57</td><td>21.21</td><td>25.76</td><td>53.03</td></tr><tr><td>Crescendo</td><td>56.06</td><td>31.81</td><td>42.42</td><td>59.09</td></tr><tr><td rowspan=\\\"5\\\">Misinformation</td><td>Basic</td><td>97.28</td><td>92.57</td><td>92.46</td><td>94.84</td></tr><tr><td>Base64</td><td>98.48</td><td>90.48</td><td>96.83</td><td>93.65</td></tr><tr><td>Prompt Injection</td><td>98.39</td><td>86.51</td><td>93.65</td><td>93.65</td></tr><tr><td>Iterative Jailbreak</td><td>63.97</td><td>53.97</td><td>84.13</td><td>69.84</td></tr><tr><td>Crescendo</td><td>85.71</td><td>55.56</td><td>88.89</td><td>84.13</td></tr><tr><td rowspan=\\\"5\\\">Privacy</td><td>Basic</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Base64</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>88.33</td><td>98.33</td><td>100</td><td>91.67</td></tr><tr><td>Iterative Jailbreak</td><td>76.67</td><td>100</td><td>93.33</td><td>96.67</td></tr><tr><td>Crescendo</td><td>96.67</td><td>100</td><td>96.67</td><td>100</td></tr><tr><td rowspan=\\\"5\\\">Security</td><td>Basic</td><td>77.84</td><td>75.57</td><td>70.46</td><td>90.09</td></tr><tr><td>Base64</td><td>82.93</td><td>82.93</td><td>63.41</td><td>95.12</td></tr><tr><td>Prompt Injection</td><td>87.80</td><td>97.56</td><td>65.85</td><td>84.13</td></tr><tr><td>Iterative Jailbreak</td><td>43.90</td><td>60.97</td><td>43.90</td><td>78.04</td></tr><tr><td>Crescendo</td><td>68.29</td><td>87.80</td><td>68.29</td><td>87.80</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                }\n",
      "            ],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 19,\n",
      "            \"markdown\": \"basic robustness. In contrast, the Crescendo strategy led to a general drop in passing rates, indicating stronger adversarial effectiveness.\\n\\nIn addition, complex attack strategies do not always outperform basic prompts. Some originally adversarial prompts may lose their intended meaning after multiple rounds of transformation, rendering the resulting model outputs less meaningful.\\n\\nAutomated Red-teaming Limitations Due to the involvement of human review, the evaluation results inevitably contain a degree of subjectivity. Additionally, certain plugin types involve API misuse or external tool invocation, which are more suitable for evaluating agent models with tool-calling capabilities. In the context of base LLMs, such tests may have limited relevance.\\n\\n## 5 Limitations\\n\\nIn our internal tests, we have identified some limitations in current Kimi K2 models. When dealing with hard reasoning tasks or unclear tool definition, the model may generate excessive tokens, sometimes leading to truncated outputs or incomplete tool calls. Additionally, performance may decline on certain tasks if tool use is unnecessarily enabled. When building complete software projects, the success rate of one-shot prompting is not as good as using K2 under an agentic coding framework. We are working to address these issues in future releases and looking forward to more feedbacks.\\n\\n## 6 Conclusions\\n\\nWe introduced Kimi K2, a 1T-parameter open-weight MoE model built for agentic intelligence. Leveraging the token-efficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training. Post-training combines large-scale synthetic tool-use data with a unified RL framework using both verifiable rewards and self-critic feedbacks. Kimi K2 sets new state-of-the-art on agentic and reasoning benchmarks, establishing itself as the most capable open-weight LLM to date.\\n\\n## 7 Acknowledgments\\n\\nWe would like to acknowledge the valuable support provided by the OpenHands and Multi-SWE-bench teams in evaluating the SWE-bench Verified and Multi-SWE-bench experimental results.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 20,\n",
      "            \"markdown\": \"References\\n\\n- [1] Jacob Austin et al. *Program Synthesis with Large Language Models*. 2021. arXiv: 2108.07732 [cs.PL]. URL: https://arxiv.org/abs/2108.07732.\\n- [2] Yushi Bai et al. *LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks*. 2025. arXiv: 2412.15204 [cs.CL]. URL: https://arxiv.org/abs/2412.15204.\\n- [3] Victor Barres et al. *$\\\\tau^{2}$-Bench: Evaluating Conversational Agents in a Dual-Control Environment*. 2025. arXiv: 2506.07982 [cs.AI]. URL: https://arxiv.org/abs/2506.07982.\\n- [4] Stella Biderman et al. \\u201cLessons from the trenches on reproducible evaluation of language models\\u201d. In: *arXiv preprint arXiv:2405.14782* (2024).\\n- [5] Federico Cassano et al. \\u201cMultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation\\u201d. In: *IEEE Transactions on Software Engineering* 49.7 (2023), pp. 3675\\u20133691. doi: 10.1109/TSE.2023.3267446.\\n- [6] Chen Chen et al. \\u201cACEBench: Who Wins the Match Point in Tool Learning?\\u201d In: *arXiv e-prints* (2025), arXiv\\u20132501.\\n- [7] Mark Chen et al. \\u201cEvaluating Large Language Models Trained on Code\\u201d. In: (2021). arXiv: 2107.03374 [cs.LG].\\n- [8] Peter Clark et al. \\u201cThink you have solved question answering? try arc, the ai2 reasoning challenge\\u201d. In: *arXiv preprint arXiv:1803.05457* (2018).\\n- [9] Karl Cobbe et al. *Training Verifiers to Solve Math Word Problems*. 2021. arXiv: 2110.14168 [cs.LG]. URL: https://arxiv.org/abs/2110.14168.\\n- [10] DeepSeek-AI. *DeepSeek-V3 Technical Report*. 2024. arXiv: 2412.19437 [cs.CL]. URL: https://arxiv.org/abs/2412.19437.\\n- [11] Mostafa Dehghani et al. \\u201cScaling vision transformers to 22 billion parameters\\u201d. In: *International conference on machine learning*. PMLR. 2023, pp. 7480\\u20137512.\\n- [12] Guanting Dong et al. *Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models*. 2024. arXiv: 2406.13542 [cs.CL]. URL: https://arxiv.org/abs/2406.13542.\\n- [13] Xinrun Du et al. \\u201cSupergpqa: Scaling llm evaluation across 285 graduate disciplines\\u201d. In: *arXiv preprint arXiv:2502.14739* (2025).\\n- [14] Dheeru Dua et al. \\u201cDROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\\u201d. In: *CoRR* abs/1903.00161 (2019). arXiv: 1903.00161. URL: http://arxiv.org/abs/1903.00161.\\n- [15] Kazuki Fujii et al. *Rewriting Pre-Training Data Boosts LLM Performance in Math and Code*. 2025. arXiv: 2505.02881 [cs.LG]. URL: https://arxiv.org/abs/2505.02881.\\n- [16] Paul Gauthier. *Aider LLM Leaderboards*. https://aider.chat/docs/leaderboards/. 2025.\\n- [17] Aryo Pradipta Gema et al. \\u201cAre we done with mmlu?\\u201d In: *arXiv preprint arXiv:2406.04127* (2024).\\n- [18] Alex Gu et al. \\u201cCruxeval: A benchmark for code reasoning, understanding and execution\\u201d. In: *arXiv preprint arXiv:2401.03065* (2024).\\n- [19] Daya Guo et al. \\u201cDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning\\u201d. In: *arXiv preprint arXiv:2501.12948* (2025).\\n- [20] Zhicheng Guo et al. \\u201cStableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models\\u201d. In: *arXiv preprint arXiv:2403.07714* (2025).\\n- [21] Aaron Harlap et al. \\u201cPipedream: Fast and efficient pipeline parallel dnn training\\u201d. In: *arXiv preprint arXiv:1806.03377* (2018).\\n- [22] Y He et al. \\u201cChinese simpleqa: A chinese factuality evaluation for large language models, 2024a\\u201d. In: *URL https://arxiv. org/abs/2411.07140* ().\\n- [23] Dan Hendrycks et al. \\u201cMeasuring massive multitask language understanding\\u201d. In: *arXiv preprint arXiv:2009.03300* (2020).\\n- [24] Dan Hendrycks et al. *Measuring Mathematical Problem Solving With the MATH Dataset*. 2021. arXiv: 2103.03874 [cs.LG]. URL: https://arxiv.org/abs/2103.03874.\\n- [25] Shengding Hu et al. \\u201cMinicpm: Unveiling the potential of small language models with scalable training strategies\\u201d. In: *arXiv preprint arXiv:2404.06395* (2024).\\n- [26] Jiaxin Huang et al. \\u201cLarge language models can self-improve\\u201d. In: *arXiv preprint arXiv:2210.11610* (2022).\\n- [27] Siming Huang et al. *OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models*. 2025. arXiv: 2411.04905 [cs.CL]. URL: https://arxiv.org/abs/2411.04905.\\n-\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2108.07732\",\n",
      "                \"https://arxiv.org/abs/2108.07732\",\n",
      "                \"https://arxiv.org/abs/2412.15204\",\n",
      "                \"https://arxiv.org/abs/2412.15204\",\n",
      "                \"https://arxiv.org/abs/2506.07982\",\n",
      "                \"https://arxiv.org/abs/2506.07982\",\n",
      "                \"https://doi.org/10.1109/TSE.2023.3267446\",\n",
      "                \"https://doi.org/10.1109/TSE.2023.3267446\",\n",
      "                \"https://arxiv.org/abs/2107.03374\",\n",
      "                \"https://arxiv.org/abs/2107.03374\",\n",
      "                \"https://arxiv.org/abs/2110.14168\",\n",
      "                \"https://arxiv.org/abs/2110.14168\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2406.13542\",\n",
      "                \"https://arxiv.org/abs/2406.13542\",\n",
      "                \"https://arxiv.org/abs/1903.00161\",\n",
      "                \"http://arxiv.org/abs/1903.00161\",\n",
      "                \"https://arxiv.org/abs/2505.02881\",\n",
      "                \"https://arxiv.org/abs/2505.02881\",\n",
      "                \"https://aider.chat/docs/leaderboards/\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2411.04905\",\n",
      "                \"https://arxiv.org/abs/2411.04905\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 21,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\n[28] Yanping Huang et al. \\\"Gpipe: Efficient training of giant neural networks using pipeline parallelism\\\". In: Advances in neural information processing systems 32 (2019).\\n[29] Yuzhen Huang et al. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. 2023. arXiv: 2305.08322 [cs.CL]. URL: https://arxiv.org/abs/2305.08322.\\n[30] Alon Jacovi et al. The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input. 2025. arXiv: 2501.03200 [cs.CL]. URL: https://arxiv.org/abs/2501.03200.\\n[31] Naman Jain et al. \\\"Livecodebench: Holistic and contamination free evaluation of large language models for code\\\". In: arXiv preprint arXiv:2403.07974 (2024).\\n[32] Carlos E Jimenez et al. \\\"SWE-bench: Can Language Models Resolve Real-world Github Issues?\\\" In: The Twelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=VTF8yNQM66.\\n[33] Keller Jordan et al. Muon: An optimizer for hidden layers in neural networks. 2024. URL: https://kellerjordan.github.io/posts/muon/.\\n[34] Mandar Joshi et al. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. 2017. arXiv: 1705.03551 [cs.CL]. URL: https://arxiv.org/abs/1705.03551.\\n[35] Kimi Team. \\\"Kimi k1. 5: Scaling reinforcement learning with llms\\\". In: arXiv preprint arXiv:2501.12599 (2025).\\n[36] Diederik P. Kingma and Jimmy Ba. \\u201cAdam: A Method for Stochastic Optimization\\u201d. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Ed. by Yoshua Bengio and Yann LeCun. 2015. URL: http://arxiv.org/abs/1412.6980.\\n[37] Satyapriya Krishna et al. Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation. 2025. arXiv: 2409.12941 [cs.CL]. URL: https://arxiv.org/abs/2409.12941.\\n[38] Joel Lamy-Poirier. \\u201cBreadth-first pipeline parallelism\\u201d. In: Proceedings of Machine Learning and Systems 5 (2023), pp. 48\\u201367.\\n[39] Dmitry Lepikhin et al. \\\"Gshard: Scaling giant models with conditional computation and automatic sharding\\\". In: arXiv preprint arXiv:2006.16668 (2020).\\n[40] Haonan Li et al. CMMLU: Measuring massive multitask language understanding in Chinese. 2024. arXiv: 2306.09212 [cs.CL]. URL: https://arxiv.org/abs/2306.09212.\\n[41] Jia Li et al. \\\"Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions\\\". In: Hugging Face repository 13.9 (2024), p. 9.\\n[42] Tianle Li et al. \\\"From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline\\\". In: arXiv preprint arXiv:2406.11939 (2024).\\n[43] Bill Yuchen Lin et al. ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning. 2025. arXiv: 2502.01100 [cs.AI]. URL: https://arxiv.org/abs/2502.01100.\\n[44] Aixin Liu et al. \\\"Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model\\\". In: arXiv preprint arXiv:2405.04434 (2024).\\n[45] Jiawei Liu et al. \\\"Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation\\\". In: Advances in Neural Information Processing Systems 36 (2023), pp. 21558-21572.\\n[46] Jingyuan Liu et al. \\\"Muon is scalable for LLM training\\\". In: arXiv preprint arXiv:2502.16982 (2025).\\n[47] Ziming Liu et al. \\\"Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced Large Model Training Efficiency\\\". In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC '23. ACM, Nov. 2023, pp. 1-13. DOI: 10.1145/3581784.3607073. URL: http://dx.doi.org/10.1145/3581784.3607073.\\n[48] Ilya Loshchilov and Frank Hutter. \\\"Decoupled Weight Decay Regularization\\\". In: International Conference on Learning Representations. 2019. URL: https://openreview.net/forum?id=Bkg6RiCqY7.\\n[49] Jan Ludziejewski et al. OpenAI Gym. 2025. arXiv: 2502.05172 [cs.LG]. URL: https://arxiv.org/abs/2502.05172.\\n[50] Samuel Miserendino et al. \\\"SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?\\\" In: arXiv preprint arXiv:2502.12115 (2025).\\n[51] Arindam Mitra et al. \\\"Agentinstruct: Toward generative teaching with agentic flows\\\". In: arXiv preprint arXiv:2407.03502 (2024).\\n[52] Ivan Moshkov et al. \\\"Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset\\\". In: arXiv preprint arXiv:2504.16891 (2025).\\n[53] Deepak Narayanan et al. \\\"Efficient large-scale language model training ongpu clusters using megatron-lm\\\". In: Proceedings of the international conference for high performance computing, networking, storage and analysis. 2021, pp. 1-15.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2305.08322\",\n",
      "                \"https://arxiv.org/abs/2305.08322\",\n",
      "                \"https://arxiv.org/abs/2501.03200\",\n",
      "                \"https://arxiv.org/abs/2501.03200\",\n",
      "                \"https://openreview.net/forum?id=VTF8yNQM66\",\n",
      "                \"https://openreview.net/forum?id=VTF8yNQM66\",\n",
      "                \"https://kellerjordan.github.io/posts/muon/\",\n",
      "                \"https://kellerjordan.github.io/posts/muon/\",\n",
      "                \"https://arxiv.org/abs/1705.03551\",\n",
      "                \"https://arxiv.org/abs/1705.03551\",\n",
      "                \"http://arxiv.org/abs/1412.6980\",\n",
      "                \"https://arxiv.org/abs/2409.12941\",\n",
      "                \"https://arxiv.org/abs/2409.12941\",\n",
      "                \"https://arxiv.org/abs/2306.09212\",\n",
      "                \"https://arxiv.org/abs/2306.09212\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://doi.org/10.1145/3581784.3607073\",\n",
      "                \"http://dx.doi.org/10.1145/3581784.3607073\",\n",
      "                \"http://dx.doi.org/10.1145/3581784.3607073\",\n",
      "                \"https://openreview.net/forum?id=Bkg6RiCqY7\",\n",
      "                \"https://arxiv.org/abs/2502.05172\",\n",
      "                \"https://arxiv.org/abs/2502.05172\",\n",
      "                \"https://arxiv.org/abs/2502.05172\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 22,\n",
      "            \"markdown\": \"[54] Long Ouyang et al. \\u201cTraining language models to follow instructions with human feedback\\u201d. In: Advances in neural information processing systems 35 (2022), pp. 27730\\u201327744.\\n- [55] Bowen Peng et al. \\u201cYarn: Efficient context window extension of large language models\\u201d. In: arXiv preprint arXiv:2309.00071 (2023).\\n- [56] Long Phan et al. Humanity\\u2019s Last Exam. 2025. arXiv: 2501.14249 [cs.LG]. URL: https://arxiv.org/abs/2501.14249.\\n- [57] Penghui Qi et al. \\u201cZero bubble pipeline parallelism\\u201d. In: arXiv preprint arXiv:2401.10241 (2023).\\n- [58] Yujia Qin et al. \\u201cToolllm: Facilitating large language models to master 16000+ real-world apis\\u201d. In: arXiv preprint arXiv:2307.16789 (2023).\\n- [59] Qwen et al. Qwen2.5 Technical Report. 2025. arXiv: 2412.15115 [cs.CL]. URL: https://arxiv.org/abs/2412.15115.\\n- [60] Samyam Rajbhandari et al. \\u201cZero: Memory optimizations toward training trillion parameter models\\u201d. In: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE. 2020, pp. 1\\u201316.\\n- [61] David Rein et al. \\u201cGpqa: A graduate-level google-proof q&a benchmark\\u201d. In: First Conference on Language Modeling. 2024.\\n- [62] Keisuke Sakaguchi et al. \\u201cWinogrande: An adversarial winograd schema challenge at scale\\u201d. In: Communications of the ACM 64.9 (2021), pp. 99\\u2013106.\\n- [63] David Silver and Richard S Sutton. \\u201cWelcome to the era of experience\\u201d. In: Google AI 1 (2025).\\n- [64] Ved Sirdeshmukh et al. MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs. 2025. arXiv: 2501.17399 [cs.CL]. URL: https://arxiv.org/abs/2501.17399.\\n- [65] Giulio Starace et al. \\u201cPaperBench: Evaluating AI\\u2019s Ability to Replicate AI Research\\u201d. In: arXiv preprint arXiv:2504.01848 (2025).\\n- [66] Hao Sun et al. ZeroSearch: Incentivize the Search Capability of LLMs without Searching. 2025. arXiv: 2505.04588 [cs.CL]. URL: https://arxiv.org/abs/2505.04588.\\n- [67] Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. 2022. arXiv: 2210.09261 [cs.CL]. URL: https://arxiv.org/abs/2210.09261.\\n- [68] Manveer Singh Tamber et al. \\u201cBenchmarking LLM Faithfulness in RAG with Evolving Leaderboards\\u201d. In: arXiv preprint arXiv:2505.04847 (2025).\\n- [69] Gemma Team et al. \\u201cGemma 2: Improving open language models at a practical size\\u201d. In: arXiv preprint arXiv:2408.00118 (2024).\\n- [70] LlaMA Team. The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation \\u2014 ai.meta.com. https://ai.meta.com/blog/llama-4-multimodal-intelligence/. [Accessed 15-07-2025].\\n- [71] The Terminal-Bench Team. Terminal-Bench: A Benchmark for AI Agents in Terminal Environments. Apr. 2025. URL: https://github.com/laude-institute/terminal-bench.\\n- [72] Ashish Vaswani et al. \\u201cAttention is All you Need\\u201d. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. URL: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n- [73] Vectara. Hallucination Evaluation Model (Revision 7437011). 2024. URL: https://huggingface.co/vectara/hallucination_evaluation_model.\\n- [74] Joshua Vendrow et al. \\u201cDo large language model benchmarks test reliability?\\u201d In: arXiv preprint arXiv:2502.03461 (2025).\\n- [75] Yizhong Wang et al. \\u201cSelf-instruct: Aligning language models with self-generated instructions\\u201d. In: arXiv preprint arXiv:2212.10560 (2022).\\n- [76] Yubo Wang et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. 2024. arXiv: 2406.01574 [cs.CL]. URL: https://arxiv.org/abs/2406.01574.\\n- [77] Zhexu Wang et al. OJBench: A Competition Level Code Benchmark For Large Language Models. 2025. arXiv: 2506.16395 [cs.CL]. URL: https://arxiv.org/abs/2506.16395.\\n- [78] Jason Wei et al. \\u201cMeasuring short-form factuality in large language models\\u201d. In: arXiv preprint arXiv:2411.04368 (2024).\\n- [79] Tianwen Wei et al. CMATH: Can Your Language Model Pass Chinese Elementary School Math Test? 2023. arXiv: 2306.16636 [cs.CL]. URL: https://arxiv.org/abs/2306.16636.\\n- [80] Colin White et al. \\u201cLiveBench: A Challenging, Contamination-Free LLM Benchmark\\u201d. In: The Thirteenth International Conference on Learning Representations. 2025.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2501.17399\",\n",
      "                \"https://arxiv.org/abs/2501.17399\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2210.09261\",\n",
      "                \"https://arxiv.org/abs/2210.09261\",\n",
      "                \"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\",\n",
      "                \"https://github.com/laude-institute/terminal-bench\",\n",
      "                \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\n",
      "                \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\n",
      "                \"https://huggingface.co/vectara/hallucination_evaluation_model\",\n",
      "                \"https://huggingface.co/vectara/hallucination_evaluation_model\",\n",
      "                \"https://arxiv.org/abs/2406.01574\",\n",
      "                \"https://arxiv.org/abs/2406.01574\",\n",
      "                \"https://arxiv.org/abs/2506.16395\",\n",
      "                \"https://arxiv.org/abs/2506.16395\",\n",
      "                \"https://arxiv.org/abs/2306.16636\",\n",
      "                \"https://arxiv.org/abs/2306.16636\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 23,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\n[81] Mitchell Wortsman et al. \\\"Small-scale proxies for large-scale transformer training instabilities, 2023\\\". In: URL https://arxiv.org/abs/2309.14322 ().\\n[82] Can Xu et al. WizardLM: Empowering large pre-trained language models to follow complex instructions. 2025. arXiv: 2304.12244 [cs.CL]. URL: https://arxiv.org/abs/2304.12244.\\n[83] Zhangchen Xu et al. KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding. 2025. arXiv: 2503.02951 [cs.LG]. URL: https://arxiv.org/abs/2503.02951.\\n[84] John Yang et al. SWE-smith: Scaling Data for Software Engineering Agents. 2025. arXiv: 2504.21798 [cs.SE]. URL: https://arxiv.org/abs/2504.21798.\\n[85] Shunyu Yao et al. \\\"tau-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains\\\". In: arXiv preprint arXiv:2406.12045 (2024).\\n[86] Daoguang Zan et al. \\\"Multi-swe-bench: A multilingual benchmark for issue resolving\\\". In: arXiv preprint arXiv:2504.02605 (2025).\\n[87] Eric Zelikman et al. \\\"Star: Bootstrapping reasoning with reasoning\\\". In: Advances in Neural Information Processing Systems 35 (2022), pp. 15476-15488.\\n[88] Rowan Zellers et al. \\u201cHellaswag: Can a machine really finish your sentence?\\u201d In: arXiv preprint arXiv:1905.07830 (2019).\\n[89] Wanjun Zhong et al. \\\"Agieval: A human-centric benchmark for evaluating foundation models\\\". In: arXiv preprint arXiv:2304.06364 (2023).\\n[90] Jeffrey Zhou et al. \\\"Instruction-Following Evaluation for Large Language Models\\\". In: ArXiv abs/2311.07911 (2023). URL: https://arxiv.org/abs/2311.07911.\\n[91] Qin Zhu et al. AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models. 2025. arXiv: 2502.16906 [cs.CL]. URL: https://arxiv.org/abs/2502.16906.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2304.12244\",\n",
      "                \"https://arxiv.org/abs/2304.12244\",\n",
      "                \"https://arxiv.org/abs/2503.02951\",\n",
      "                \"https://arxiv.org/abs/2503.02951\",\n",
      "                \"https://arxiv.org/abs/2504.21798\",\n",
      "                \"https://arxiv.org/abs/2504.21798\",\n",
      "                \"https://arxiv.org/abs/2311.07911\",\n",
      "                \"https://arxiv.org/abs/2502.16906\",\n",
      "                \"https://arxiv.org/abs/2502.16906\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        }\n",
      "    ],\n",
      "    \"model\": \"mistral-ocr-latest\",\n",
      "    \"usage_info\": {\n",
      "        \"pages_processed\": 8,\n",
      "        \"doc_size_bytes\": 6335018\n",
      "    },\n",
      "    \"document_annotation\": \"{\\n  \\\"summary\\\": \\\"The document presents a comprehensive evaluation of the Kimi-K2-Instruct and Kimi-K2-Base models, highlighting their superior performance across various benchmarks and tasks. Kimi-K2-Instruct excels in multi-turn tool-use benchmarks, general capabilities, and open-ended evaluations, outperforming all baselines in grounded, controlled, and agent-driven tool orchestration. It also shows strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. Kimi-K2-Base demonstrates state-of-the-art performance in general language understanding, coding capabilities, mathematical reasoning, and Chinese language understanding. The models were evaluated using diverse benchmarks and configurations, ensuring fair comparisons. Safety evaluations were conducted using red-teaming techniques, with Kimi K2 showing robust performance against various attack strategies. The document also acknowledges some limitations and areas for future improvement, such as handling hard reasoning tasks and unclear tool definitions. Overall, Kimi K2 is positioned as a leading open-weight model for agentic intelligence.\\\",\\n  \\\"authors\\\": [\\n    \\\"Kimi-K2 Team\\\"\\n  ]\\n}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Mistral client with API key\n",
    "from mistralai import Mistral\n",
    "load_dotenv()\n",
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "# OCR Call with Annotations\n",
    "annotations_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    pages=list(range(16, 24)), # Document Annotations has a limit of 8 pages, we recommend spliting your documents when using it; bbox annotations does not have the same limit\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "    },\n",
    "    bbox_annotation_format=response_format_from_pydantic_model(BBox),\n",
    "    document_annotation_format=response_format_from_pydantic_model(Document),\n",
    "    include_image_base64=True, # Let's also include the images in the response\n",
    "    table_format=\"html\"\n",
    "  )\n",
    "\n",
    "# Convert response to JSON format\n",
    "response_dict = json.loads(annotations_response.model_dump_json())\n",
    "\n",
    "print(json.dumps(response_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a27a3",
   "metadata": {},
   "source": [
    "Let's split the pdf into 8 pages batches first as they advice to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8df55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9499b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 saved to RAG/documents/chunks/chunk_0.pdf\n",
      "Chunk 1 saved to RAG/documents/chunks/chunk_1.pdf\n",
      "Chunk 2 saved to RAG/documents/chunks/chunk_2.pdf\n",
      "Chunk 3 saved to RAG/documents/chunks/chunk_3.pdf\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "def split_pdf(input_path, chunk_size=8, output_dir=\"RAG/documents/chunks\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    reader = PdfReader(input_path)\n",
    "    for i in range(0, len(reader.pages), chunk_size):\n",
    "        writer = PdfWriter()\n",
    "        for page in reader.pages[i : i + chunk_size]:\n",
    "            writer.add_page(page)\n",
    "        \n",
    "        chunk_filename = f\"chunk_{i//chunk_size}.pdf\"\n",
    "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        print(f\"Chunk {i//chunk_size} saved to {chunk_path}\")\n",
    "\n",
    "split_pdf(\"RAG/documents/KimiK2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6311d",
   "metadata": {},
   "source": [
    "Let's actually parse and annotate tables as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f9052e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: chunk_0.pdf\n",
      "Successfully processed chunk_0.pdf\n",
      "Processing: chunk_1.pdf\n",
      "Successfully processed chunk_1.pdf\n",
      "Processing: chunk_2.pdf\n",
      "Successfully processed chunk_2.pdf\n",
      "Processing: chunk_3.pdf\n",
      "Successfully processed chunk_3.pdf\n"
     ]
    }
   ],
   "source": [
    "chunk_dir = \"RAG/documents/chunks\"\n",
    "responses = []\n",
    "\n",
    "# Sort the list to ensure pages stay in order\n",
    "for chunk_filename in sorted(os.listdir(chunk_dir)):\n",
    "    # Construct the full path\n",
    "    chunk_path = os.path.join(chunk_dir, chunk_filename)\n",
    "    \n",
    "    # Skip directories or non-pdf files if any exist\n",
    "    if not chunk_filename.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    with open(chunk_path, \"rb\") as f:\n",
    "        # Correctly encode the specific chunk\n",
    "        base64_chunk = base64.b64encode(f.read()).decode('utf-8')\n",
    "        print(f\"Processing: {chunk_filename}\")\n",
    "\n",
    "    try:\n",
    "        # OCR Call\n",
    "        annotations_response = client.ocr.process(\n",
    "            model=\"mistral-ocr-latest\",\n",
    "            # Remove the 'pages' limit because the file IS the limit now\n",
    "            document={\n",
    "                \"type\": \"document_url\",\n",
    "                \"document_url\": f\"data:application/pdf;base64,{base64_chunk}\"\n",
    "            },\n",
    "            bbox_annotation_format=response_format_from_pydantic_model(Image),\n",
    "            document_annotation_format=response_format_from_pydantic_model(Document),\n",
    "            include_image_base64=True,\n",
    "            table_format=\"html\"  # take out tables as well\n",
    "        )\n",
    "        \n",
    "        response_dict = annotations_response.model_dump()\n",
    "        # Parse nested JSON strings in document_annotation\n",
    "        if isinstance(response_dict.get(\"document_annotation\"), str):\n",
    "            response_dict[\"document_annotation\"] = json.loads(response_dict[\"document_annotation\"])\n",
    "        # Parse nested JSON strings in image annotations\n",
    "        for page in response_dict.get(\"pages\", []):\n",
    "            for img in page.get(\"images\", []):\n",
    "                if isinstance(img.get(\"image_annotation\"), str):\n",
    "                    img[\"image_annotation\"] = json.loads(img[\"image_annotation\"])\n",
    "\n",
    "        responses.append(response_dict)\n",
    "        print(f\"Successfully processed {chunk_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {chunk_filename}: {e}\")\n",
    "\n",
    "# Save the responses\n",
    "output_path = \"RAG/OCR/responses.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True) # Ensure directory exists\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0251595",
   "metadata": {},
   "source": [
    "Now since we split the document into several parts, our images' and tables' indexes will start over at each chunk, and that will give us repeated indices - we do not want that. So we re-index with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6696d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def reindex_ocr_responses(responses_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Re-indexes images and tables across all OCR responses to have globally unique IDs.\n",
    "    Updates both the ID fields in objects and all markdown references.\n",
    "    \"\"\"\n",
    "    global_image_counter = 0\n",
    "    global_table_counter = 0\n",
    "    \n",
    "    for response in responses_list:\n",
    "        for page in response.get(\"pages\", []):\n",
    "            # Create mapping of old IDs to new IDs for this page\n",
    "            image_id_map = {}\n",
    "            table_id_map = {}\n",
    "            \n",
    "            # Re-index images\n",
    "            for img in page.get(\"images\", []):\n",
    "                old_id = img[\"id\"]\n",
    "                # Get file extension\n",
    "                ext = old_id.split('.')[-1] if '.' in old_id else 'jpeg'\n",
    "                new_id = f\"img-{global_image_counter}.{ext}\"\n",
    "                \n",
    "                # Update the image ID in the object\n",
    "                img[\"id\"] = new_id\n",
    "                image_id_map[old_id] = new_id\n",
    "                global_image_counter += 1\n",
    "            \n",
    "            # Re-index tables\n",
    "            for table in page.get(\"tables\", []):\n",
    "                old_id = table[\"id\"]\n",
    "                # Get file extension\n",
    "                ext = old_id.split('.')[-1] if '.' in old_id else 'html'\n",
    "                new_id = f\"tbl-{global_table_counter}.{ext}\"\n",
    "                \n",
    "                # Update the table ID in the object\n",
    "                table[\"id\"] = new_id\n",
    "                table_id_map[old_id] = new_id\n",
    "                global_table_counter += 1\n",
    "            \n",
    "            # Update markdown to reflect new IDs\n",
    "            markdown = page.get(\"markdown\", \"\")\n",
    "            \n",
    "            # Replace image references: ![img-0.jpeg](img-0.jpeg) format\n",
    "            for old_id, new_id in image_id_map.items():\n",
    "                markdown = markdown.replace(f\"![{old_id}]({old_id})\", f\"![{new_id}]({new_id})\")\n",
    "            \n",
    "            # Replace table references: [tbl-0.html](tbl-0.html) format\n",
    "            for old_id, new_id in table_id_map.items():\n",
    "                markdown = markdown.replace(f\"[{old_id}]({old_id})\", f\"[{new_id}]({new_id})\")\n",
    "            \n",
    "            page[\"markdown\"] = markdown\n",
    "    \n",
    "    print(f\"Re-indexed {global_image_counter} images and {global_table_counter} tables\")\n",
    "    return responses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b418611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-indexed 18 images and 7 tables\n",
      "Saved re-indexed responses to RAG/OCR/responses_reindexed.json\n"
     ]
    }
   ],
   "source": [
    "# Load the original JSON\n",
    "input_file = \"RAG/OCR/responses.json\"\n",
    "output_file = \"RAG/OCR/responses_reindexed.json\"  # or use the same file to overwrite\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    responses_list = json.load(f)\n",
    "\n",
    "# Re-index\n",
    "responses_list = reindex_ocr_responses(responses_list)\n",
    "\n",
    "# Save the re-indexed version\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(responses_list, f, indent=4)\n",
    "\n",
    "print(f\"Saved re-indexed responses to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d4c7a",
   "metadata": {},
   "source": [
    "Let's check the results. Notice we are also already parsing the meatdata that we need later on, and we are also keeping the division in pages (we need that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4900c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def replace_images_in_markdown_annotated(\n",
    "    markdown_str: str,\n",
    "    images_dict: dict,\n",
    "    tables_dict: dict = None,\n",
    "    include_images: bool = True,\n",
    "    include_tables: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Replaces images and tables in the markdown string with their content/descriptions.\n",
    "\n",
    "    Args:\n",
    "        markdown_str: The markdown string to replace images in.\n",
    "        images_dict: A dictionary of images to replace, with their names as keys and data as values.\n",
    "        tables_dict: A dictionary of tables to replace, with their names as keys and data as values.\n",
    "        include_images: Whether to include images base64 data in the output.\n",
    "        include_tables: Whether to include table HTML content in the output.\n",
    "\n",
    "    Returns:\n",
    "        The markdown string with images and tables replaced.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace images: ![img-0.jpeg](img-0.jpeg) format\n",
    "    for img_name, data in images_dict.items():\n",
    "        placeholder = f\"![{img_name}]({img_name})\"\n",
    "        \n",
    "        # Get annotation description\n",
    "        annotation = data.get('annotation', {})      \n",
    "        description = annotation.get('description', '')\n",
    "        \n",
    "        if include_images:\n",
    "            replacement = f\"![{img_name}]({data['image']})\\n\\n**{description}**\"\n",
    "        else:\n",
    "            replacement = f\"**Figure: {img_name}**\\n\\n**{description}**\"\n",
    "        \n",
    "        markdown_str = markdown_str.replace(placeholder, replacement)\n",
    "    \n",
    "    # Replace tables: [tbl-0.html](tbl-0.html) format (no exclamation mark!)\n",
    "    if tables_dict:\n",
    "        for tbl_name, data in tables_dict.items():\n",
    "            placeholder = f\"[{tbl_name}]({tbl_name})\"\n",
    "            \n",
    "            if include_tables:\n",
    "                # Insert the actual HTML table content\n",
    "                replacement = f\"\\n\\n{data['content']}\\n\\n\"\n",
    "            else:\n",
    "                replacement = f\"**Table: {tbl_name}**\"\n",
    "            \n",
    "            markdown_str = markdown_str.replace(placeholder, replacement)\n",
    "    \n",
    "    return markdown_str\n",
    "\n",
    "def process_saved_ocr_json(json_path: str, range: list[int] = None, include_images: bool = True, include_tables: bool = True):\n",
    "    \"\"\"\n",
    "    Reads the saved JSON list of responses and returns a list of page texts with metadata.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing OCR responses\n",
    "        range: Optional range on the number of responses to process\n",
    "        include_images: Whether to include images in the output\n",
    "        include_tables: Whether to include tables in the output\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of page texts, list of page metadata dicts)\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        responses_list = json.load(f)\n",
    "\n",
    "    page_texts = []\n",
    "    page_metadata = []\n",
    "    global_page_number = 0\n",
    "\n",
    "    # Handle None range - process all responses\n",
    "    responses_to_process = responses_list[range[0]:range[1]] if range is not None else responses_list\n",
    "\n",
    "    for chunk_idx, resp in enumerate(responses_to_process):\n",
    "        # 1. Extract the Document-level Annotation/Summary\n",
    "        doc_anno = resp.get(\"document_annotation\", \"\")\n",
    "        \n",
    "        # Extract document-level info\n",
    "        doc_summary = doc_anno.get('summary', '') if isinstance(doc_anno, dict) else ''\n",
    "        doc_authors = doc_anno.get('authors', []) if isinstance(doc_anno, dict) else []\n",
    "\n",
    "        # 2. Iterate through pages in this chunk\n",
    "        for page in resp.get(\"pages\", []):\n",
    "            page_idx = page.get(\"index\", 0)\n",
    "            \n",
    "            image_data = {}\n",
    "            image_ids = []\n",
    "            # Extract image data for replacement\n",
    "            for img in page.get(\"images\", []):\n",
    "                img_id = img[\"id\"]\n",
    "                image_ids.append(img_id)\n",
    "                image_data[img_id] = {\n",
    "                    \"image\": img.get(\"image_base64\", \"\"), \n",
    "                    \"annotation\": img.get(\"image_annotation\", {})\n",
    "                }\n",
    "            \n",
    "            table_data = {}\n",
    "            table_ids = []\n",
    "            for tbl in page.get(\"tables\", []):\n",
    "                tbl_id = tbl[\"id\"]\n",
    "                table_ids.append(tbl_id)\n",
    "                table_data[tbl_id] = {\n",
    "                    \"content\": tbl.get(\"content\", \"\"),\n",
    "                }\n",
    "            \n",
    "            # 3. Process the markdown for this specific page\n",
    "            page_md = page.get(\"markdown\", \"\")\n",
    "            processed_page = replace_images_in_markdown_annotated(\n",
    "                page_md, image_data, table_data, include_images, include_tables\n",
    "            )\n",
    "            page_texts.append(processed_page)\n",
    "            \n",
    "            # 4. Store metadata for this page\n",
    "            # Prepare images data with base64 and annotations\n",
    "            images_data = []\n",
    "            for img_id in image_ids:\n",
    "                img_info = {\n",
    "                    \"id\": img_id,\n",
    "                    \"base64\": image_data[img_id][\"image\"],\n",
    "                    \"annotation\": image_data[img_id][\"annotation\"]\n",
    "                }\n",
    "                images_data.append(img_info)\n",
    "            \n",
    "            # Prepare tables data with HTML content\n",
    "            tables_data = []\n",
    "            for tbl_id in table_ids:\n",
    "                tbl_info = {\n",
    "                    \"id\": tbl_id,\n",
    "                    \"html_content\": table_data[tbl_id][\"content\"]\n",
    "                }\n",
    "                tables_data.append(tbl_info)\n",
    "            \n",
    "            metadata = {\n",
    "                \"source\": json_path,    \n",
    "                \"chunk_index\": chunk_idx,  # index of the chunk (we split whole document in 4 chunks)\n",
    "                \"page_index\": page_idx,  # index of the page in the chunk\n",
    "                \"global_page_number\": global_page_number,  # page index in the entire document\n",
    "                \"document_summary\": doc_summary,\n",
    "                \"document_authors\": doc_authors,\n",
    "                \"num_images\": len(image_ids),\n",
    "                \"num_tables\": len(table_ids),\n",
    "                \"image_ids\": image_ids,\n",
    "                \"table_ids\": table_ids,\n",
    "                \"images\": images_data,  # Full image data with base64 and annotations\n",
    "                \"tables\": tables_data,  # Full table data with HTML content\n",
    "            }\n",
    "            page_metadata.append(metadata)\n",
    "            global_page_number += 1\n",
    "\n",
    "    return page_texts, page_metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18528ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "K KIMI K2: OPEN AGENTIC INTELLIGENCE\n",
       "\n",
       "# TECHNICAL REPORT OF KIMI K2\n",
       "\n",
       "# Kimi Team\n",
       "\n",
       "# ABSTRACT\n",
       "\n",
       "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.\n",
       "\n",
       "Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual  surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints $^{1}$  to facilitate future research and applications of agentic intelligence.\n",
       "\n",
       "**Figure: img-0.jpeg**\n",
       "\n",
       "**This image shows a series of bar charts comparing the performance of various AI models across different benchmarks. The benchmarks are categorized into 'Agentic and Competitive Coding', 'Tool Use', and 'Math & STEM'. Each bar chart represents a specific benchmark, such as SWE-bench Verified, SWE-bench Multilingual, LiveCodeBench v6, OJBench, Tau2-bench micro-average, AceBench (en), AIME 2025, and GPQA-Diamond. The AI models compared include Kimi-K2-Instruct, DeepSeekV3-0324, Owen3-235B-A22B, OpenAI GPT-4.1, Claude 4 Opus, Claude 4 Sonnet, and Gemini 2.5 Flash non-thinking. Each bar chart shows the performance scores of these models, with Kimi-K2-Instruct generally performing the best across most benchmarks.**\n",
       "Figure 1: Kimi K2 main results.2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display (just first page)\n",
    "page_texts, metadata = process_saved_ocr_json(\"RAG/OCR/responses_reindexed.json\", range=[0, 1], include_images=False, include_tables=True)\n",
    "display(Markdown(page_texts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257c98b",
   "metadata": {},
   "source": [
    "### 2.2 Preparing Document's text for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4619e",
   "metadata": {},
   "source": [
    "We need another step before going into the embeddings: \n",
    "\n",
    "**We want to replace images and tables with a textual description for our RAG.** \n",
    "\n",
    "As a matter of fact, Mistral gave us descriptions for the images, which we already put in the text, but no descriptions for tables. The RAG model will have troubles reading from an html representation of a table and will get confused. \n",
    "\n",
    "*So we want to give the pdf of the paper, together with the extracted tables, to a multimodal model that will provide a summary of the given tables and an index. Then we swap the result in the text*.\n",
    "\n",
    "Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27aaeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field, SecretStr\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"google/gemini-2.5-flash\", \n",
    "    \n",
    "    # redirect LangChain to OpenRouter\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "\n",
    "    # pass the OpenRouter key\n",
    "    api_key=SecretStr(os.environ[\"OPENROUTER_API_KEY\"])\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a document indexing assistant. \n",
    "I will provide a document and its corresponding OCR Markdown. \n",
    "Your task is to find every table placeholder (e.g., [tbl-x.html]) in the text, identify what that table represents, and return a JSON mapping.\n",
    "\n",
    "Specifically, you must return an answer composed of 2 lists:\n",
    "- titles: the titles of the tables, extracted from the document.\n",
    "- descriptions: the descriptions of the tables. \n",
    "These descriptions must be thorough and include all the information in the table, highlighting the main points and the context.\n",
    "\"\"\"\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    titles : list[str] = Field(\"The titles of the tables, extracted from the document.\")\n",
    "    descriptions : list[str] = Field(\"The descriptions of the tables.\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    system_prompt=prompt,\n",
    "    response_format=ResponseFormat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8580c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# get the full text with no base64 image data but with table contents (as html)\n",
    "# we want this llm to check the tables content and have full context \n",
    "full_text = process_saved_ocr_json(\"RAG/OCR/responses_reindexed.json\", include_images=False, include_tables=True)\n",
    "# we also pass the pdf in order to have full context (maybe not needed)\n",
    "base64_pdf = encode_pdf(\"RAG/documents/KimiK2.pdf\")\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Here is the structured OCR text and tables for context:\\n\\n{full_text}\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image_url\", # OpenAI uses this block for PDF vision support\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_state = {\"messages\" : [message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Table 0:\n",
      "Title: SimpleQA Accuracy under three rephrasing-epoch configurations\n",
      "Description: This table compares the SimpleQA Accuracy under three different configurations related to data rephrasing and the number of training epochs. The first row shows the accuracy for raw wiki-text repeated for 10 epochs (23.76). The second row shows the accuracy for data rephrased once and repeated for 10 epochs (27.39). The third row shows the accuracy for data rephrased 10 times with a single training pass (28.94). The table demonstrates that rephrasing data leads to improved accuracy, with more rephrasing and fewer epochs yielding better results.\n",
      "----------------------------------------\n",
      "Table 1:\n",
      "Title: Architectural comparison between Kimi K2 and DeepSeek-V3\n",
      "Description: This table provides a detailed comparison of architectural parameters between Kimi K2 and DeepSeek-V3. It includes metrics such as #Layers, Total Parameters, Activated Parameters, Experts (total), Experts Active per Token, Shared Experts, Attention Heads, Number of Dense Layers, and Expert Grouping. Kimi K2 has 61 layers, 1.04T total parameters (54% from DeepSeek-V3), 32.6B activated parameters (13% from DeepSeek-V3), 384 total experts (50% from DeepSeek-V3), 8 active experts per token, 1 shared expert, 64 attention heads (50% from DeepSeek-V3), and 1 dense layer (67% from DeepSeek-V3). Unlike DeepSeek-V3, Kimi K2 does not use expert grouping.\n",
      "----------------------------------------\n",
      "Table 2:\n",
      "Title: Performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across diverse tasks.\n",
      "Description: This table presents a comprehensive performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across a diverse set of tasks categorized into Coding Tasks, Tool Use Tasks, Math & STEM Tasks, and General Tasks. For each benchmark, the table lists the scores for Kimi-K2-Instruct, DeepSeek-V3-0324, Qwen3-235B-A22B (open-source), Claude Sonnet 4, Claude Opus 4, GPT-4.1, and Gemini 2.5 Flash (proprietary). Key highlights in Coding Tasks include Kimi-K2-Instruct's strong performance on LiveCodeBench v6 (53.7) and OJBench (27.1), and notable scores on SWE-bench Verified. In Tool Use Tasks, Kimi-K2-Instruct leads on Tau2 retail (70.6), Tau2 airline (56.5), Tau2 telecom (65.8), and AceBench (76.5). For Math & STEM Tasks, Kimi-K2-Instruct shows excellent results on AIME 2024 (69.6), AIME 2025 (49.5), MATH-500 (97.4), HMMT 2025 (38.8), CNMO 2024 (74.3), PolyMath-en (65.1), ZebraLogic (89.0), AutoLogi (89.5), GPQA-Diamond (75.1), and SuperGPQA (57.2). In General Tasks, Kimi-K2-Instruct performs well on MMLU (89.5), MMLU-Redux (92.7), MMLU-Pro (81.1), IFEval (89.8), Multi-Challenge (54.1), SimpleQA (31.0), Livebench (76.4), Arena Hard v2.0 (54.5% win rate for hard prompts, 85.0% for creative writing), FACTS Grounding (88.5), HHEM v2.1 (98.9), and FaithJudge (92.6). Long context tasks like LongBench v2 (49.1), FRAMES (77.1), MRCR (55.0), and DROP (93.5) are also included. Bold values indicate global SOTA, and underlined bold indicates the best open-source result.\n",
      "----------------------------------------\n",
      "Table 3:\n",
      "Title: Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks.\n",
      "Description: This table provides a comprehensive comparison of Kimi-K2-Base against leading open-source models across diverse tasks, grouped by English, Code, Math, and Chinese capabilities. For each benchmark, the table includes the number of shots, and scores for Kimi-K2-Base, DeepSeek-V3-Base, Llama4-Maverick-Base, and Qwen2.5-72B-Base. The architecture for Kimi-K2-Base, DeepSeek-V3-Base, and Llama4-Maverick-Base is MoE, while Qwen2.5-72B-Base is Dense. Key highlights include Kimi-K2-Base achieving state-of-the-art performance in English language understanding for MMLU (87.79), MMLU-pro (69.17), MMLU-redux (90.17), SuperGPQA (44.67), SimpleQA (35.25), TriviaQA (85.09), BBH (88.71) and ARC-Challenge (95.73). For coding capabilities, Kimi-K2-Base leads in CRUXEval-I-cot (74.00), CRUXEval-O-cot (83.50), LiveCodeBench (v6) (26.29), and EvalPlus (80.33). In mathematical reasoning, Kimi-K2-Base excels in MATH (70.22), GSM8k (92.12), and GSM8k-platinum (94.21). For Chinese language capabilities, Kimi-K2-Base achieves top scores in C-Eval (92.50), CMMLU (90.90), and CSimpleQA (77.57).\n",
      "----------------------------------------\n",
      "Table 4:\n",
      "Title: Enabled Plugins and Strategies\n",
      "Description: This table lists the plugins and strategies evaluated in the safety assessment. The plugins include Harmful (Graphic Content, Harassment and Bullying, Hate Speech, Insults, Profanity, Radicalization, Self Harm, Sexual Content, ToxicChat), Criminal (Chemical & Biological Weapons, Child Exploitation, Copyright Violations, Cybercrime, Illegal Activities, Illegal Drugs, Indiscriminate Weapons, Intellectual Property Violation, Non-Violent Crime, Violent Crime, Sex Crimes), Misinformation (Competitor Endorsement, Unsupervised Contracts, Excessive Agency, Hallucination, Misinformation and Disinformation, Specialized Advice, Unsafe Practices, Imitation, Overreliance, Political Opinions, Religious Sensitivity), Privacy (Privacy Violation, PII in API/Database, Direct PII Exposure, PII in Session Data, PII via Social Engineering), and Security (ASCII Smuggling, CyberSecEval, Harmbench, Debug Access, Divergent Repetition, DoNotAnswer, Malicious Code, Pliny, Prompt Extraction, Reasoning DoS, Tool Discovery). The strategies evaluated alongside these plugins are Basic, Prompt Injection, Iterative Jailbreak, and Crescendo. Each plugin is paired with all listed strategies for evaluation.\n",
      "----------------------------------------\n",
      "Table 5:\n",
      "Title: Safety Evaluation Results\n",
      "Description: This table presents the safety evaluation results, showing the passing rates (%) of different models (Kimi-K2-Instruct, DeepSeek-V3-0324, DeepSeek-R1, Qwen3-235B-A22B) across various plugin-strategy combinations. For Harmful content, Kimi-K2-Instruct generally performs well on Basic and Base64 strategies but shows lower passing rates on Iterative Jailbreak and Crescendo (64.71). For Criminal content, Kimi-K2-Instruct is 100% on Basic but drops significantly on Iterative Jailbreak (57.57) and Crescendo (56.06). For Misinformation, Kimi-K2-Instruct maintains high passing rates for Basic (97.28), Base64 (98.48), and Prompt Injection (98.39), with lower rates on Iterative Jailbreak (63.97) and Crescendo (85.71). For Privacy, all models show high passing rates, with Kimi-K2-Instruct achieving 100% on Basic and Base64 and high scores for other strategies. For Security, Kimi-K2-Instruct has varying performance, with Base64 scoring 82.93 and Iterative Jailbreak being the lowest at 43.90. The table suggests that complex strategies like Iterative Jailbreak and Crescendo pose greater challenges for some models.\n",
      "----------------------------------------\n",
      "Table 6:\n",
      "Title: Kimi-K2-Instruct Open-Ended Evaluation (aggregated)\n",
      "Description: This bar chart titled 'Kimi-K2-Instruct Open-Ended Evaluation (aggregated)' compares the win, tie, and loss percentages of Kimi-K2-Instruct against DeepSeek-V3-0324, Claude-Sonnet-4, and ChatGPT-4o-latest on an aggregated Chinese in-house benchmark. Against DeepSeek-V3-0324, Kimi-K2-Instruct has a 59.6% win rate, 23.5% tie rate, and 16.9% loss rate. Against Claude-Sonnet-4, Kimi-K2-Instruct has a 64.6% win rate, 18.8% tie rate, and 16.6% loss rate. Against ChatGPT-4o-latest, Kimi-K2-Instruct shows a 65.4% win rate, 17.6% tie rate, and 17.0% loss rate. The chart visually represents Kimi-K2-Instruct's strong performance in Chinese open-ended evaluations, consistently achieving higher win rates and lower loss rates compared to the other models tested.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 40)\n",
    "for i, (title, description) in enumerate(zip(result['structured_response'].titles, result['structured_response'].descriptions)):\n",
    "    print(f\"Table {i}:\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# save this to file in RAG/OCR/tables_descriptions.json\n",
    "# Create a list of dictionaries to preserve the relationship between titles and descriptions\n",
    "tables_data = [\n",
    "    {\n",
    "        \"index\": i,\n",
    "        \"title\": title,\n",
    "        \"description\": description\n",
    "    }\n",
    "    for i, (title, description) in enumerate(zip(\n",
    "        result['structured_response'].titles, \n",
    "        result['structured_response'].descriptions\n",
    "    ))\n",
    "]\n",
    "\n",
    "with open(\"RAG/OCR/tables_descriptions.json\", \"w\") as f:\n",
    "    json.dump(tables_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72a1cd",
   "metadata": {},
   "source": [
    "Nice! now we swap these descriptions with our table indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc8ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tables_descriptions(page_text: str, tables_data: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Insert the tables descriptions into a page text at the correct positions.\n",
    "    \n",
    "    Args:\n",
    "        page_text: Text of a single page\n",
    "        tables_data: List of dicts with 'index', 'title', 'description'\n",
    "    \n",
    "    Returns:\n",
    "        Page text with table placeholders replaced by descriptions\n",
    "    \"\"\"\n",
    "    for table in tables_data:\n",
    "        i = table['index']\n",
    "        title = table['title']\n",
    "        description = table['description']\n",
    "        placeholder = f\"tbl-{i}.html\"\n",
    "        replacement = f\"**Table {i}: {title}**\\n\\n{description}\"\n",
    "        page_text = page_text.replace(placeholder, replacement)\n",
    "    return page_text\n",
    "\n",
    "# Load table descriptions\n",
    "with open(\"RAG/OCR/tables_descriptions.json\", \"r\") as f:\n",
    "    tables_data = json.load(f)\n",
    "\n",
    "# Get pages without images and tables (just placeholders)\n",
    "page_texts, page_metadata = process_saved_ocr_json(\n",
    "    \"RAG/OCR/responses_reindexed.json\", \n",
    "    include_images=False, \n",
    "    include_tables=False\n",
    ")\n",
    "\n",
    "# Apply table descriptions to each page\n",
    "pages_with_descriptions = [\n",
    "    insert_tables_descriptions(page_text, tables_data) \n",
    "    for page_text in page_texts\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ba73c",
   "metadata": {},
   "source": [
    "Perfect! Our text contains all the descriptions necessary for our rag, and we got the indexed images and tables separately as metadata!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1befa2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "chunk_index\n",
      "page_index\n",
      "global_page_number\n",
      "document_summary\n",
      "document_authors\n",
      "num_images\n",
      "num_tables\n",
      "image_ids\n",
      "table_ids\n",
      "images\n",
      "tables\n"
     ]
    }
   ],
   "source": [
    "for key in page_metadata[0].keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12df37",
   "metadata": {},
   "source": [
    "There is a catch: we cannot embed heavy metadata in vector stores like Chroma: they are optimized for efficient search and only accept primitive types (str, bool, int..) so what can we do? \n",
    "\n",
    "We separate the 'heavy' metadata from the 'lightweight' and use pages ids and images/tables indices to map them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63925af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chiavi che il Vector Store pu gestire\n",
    "LIGHT_KEYS = {\n",
    "    \"source\", \n",
    "    \"chunk_index\", \n",
    "    \"page_index\", \n",
    "    \"global_page_number\", \n",
    "    \"num_images\", \n",
    "    \"num_tables\"\n",
    "}\n",
    "\n",
    "# Chiavi \"pesanti\" o con liste \n",
    "HEAVY_KEYS = {\n",
    "    \"document_summary\", \n",
    "    \"document_authors\", \n",
    "    \"image_ids\", \n",
    "    \"table_ids\", \n",
    "    \"images\", \n",
    "    \"tables\"\n",
    "}\n",
    "\n",
    "vector_metadata = []\n",
    "heavy_metadata = []\n",
    "\n",
    "for meta in page_metadata:\n",
    "    # Creiamo il dizionario leggero filtrando solo le chiavi ammesse\n",
    "    light = {k: v for k, v in meta.items() if k in LIGHT_KEYS}\n",
    "    \n",
    "    # Creiamo il dizionario pesante con tutto il resto\n",
    "    heavy = {k: v for k, v in meta.items() if k in HEAVY_KEYS}\n",
    "\n",
    "    # per connettere light e heavy aggiungiamo anche un doc_id univoco (opzionale)\n",
    "    import uuid\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    light[\"doc_id\"] = doc_id\n",
    "    heavy[\"doc_id\"] = doc_id\n",
    "\n",
    "    vector_metadata.append(light)\n",
    "    heavy_metadata.append(heavy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f07bd3",
   "metadata": {},
   "source": [
    "All we need to do now is convert our objects into LangChain's `Document` objects in order to be able to embed it in a vector store. Luckily, this is the easiest part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652d2b6",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02dfe0",
   "metadata": {},
   "source": [
    "### 3.1 Transform into `Document`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9370f5",
   "metadata": {},
   "source": [
    "It's this simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010ac254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# only light metadata is embedded in the vector store\n",
    "documents = [\n",
    "    Document(page_content=t, metadata=m)\n",
    "    for t,m in zip(page_texts, vector_metadata)\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42a1d7",
   "metadata": {},
   "source": [
    "Then we need to split the text in order to create the chunks for our vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f40c1e",
   "metadata": {},
   "source": [
    "### 3.2 Split Text\n",
    "\n",
    "We will use LangChain's `RecursiveCharacterTextSplitter` (check all splitters [here](https://docs.langchain.com/oss/python/integrations/splitters#text-structure-based))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ada9964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ab4f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1e2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "K KIMI K2: OPEN AGENTIC INTELLIGENCE\n",
      "\n",
      "# TECHNICAL REPORT OF KIMI K2\n",
      "\n",
      "# Kimi Team\n",
      "\n",
      "# ABSTRACT\n",
      "\n",
      "We int\n",
      "{'source': 'RAG/OCR/responses_reindexed.json', 'chunk_index': 0, 'page_index': 0, 'global_page_number': 0, 'num_images': 1, 'num_tables': 0, 'doc_id': 'd906f527-dd0a-4bc7-894e-c5059a5e049a'}\n"
     ]
    }
   ],
   "source": [
    "# checks\n",
    "print(len(all_splits))\n",
    "print(all_splits[0].page_content[:100])\n",
    "print(all_splits[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b88811",
   "metadata": {},
   "source": [
    "### 3.3 Create Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5306206",
   "metadata": {},
   "source": [
    "Now we create the vector db with our chunks. We will use [Chroma](https://docs.trychroma.com/docs/overview/introduction) (it's free)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3baf718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "chroma_dir = \"RAG/chroma\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")  \n",
    "\n",
    "# do not recreate vector store if it already exists (!)\n",
    "if os.path.exists(chroma_dir):\n",
    "    raise ValueError(\"Vector store already exists!\")\n",
    "\n",
    "os.makedirs(chroma_dir, exist_ok=True)\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=chroma_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97268a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a1c58f21-1661-4758-8ff4-05bd901d532f', metadata={'global_page_number': 15, 'page_index': 7, 'chunk_index': 1, 'doc_id': 'dcb7ea3a-b740-49d1-b217-7be3095a4297', 'num_tables': 1, 'source': 'RAG/OCR/responses_reindexed.json', 'num_images': 0}, page_content=\"Kimi K2\\n\\nTECHNICAL REPORT\\n\\nTable 3: Performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across diverse tasks. Bold denotes the global SOTA; underlined bold indicates the best open-source result. Data points marked with * are taken directly from the model's technical report or blog.\\n\\n**Table: tbl-2.html**\"),\n",
       " Document(id='755c4de0-c415-423f-9c21-5c29a35dffe6', metadata={'num_images': 1, 'doc_id': '22a845d4-16b5-4a8d-bc40-70bd32ab4c64', 'page_index': 4, 'source': 'RAG/OCR/responses_reindexed.json', 'chunk_index': 3, 'global_page_number': 28, 'num_tables': 0}, page_content=\"Kimi K2\\n\\nTECHNICAL REPORT\\n\\n**Figure: img-13.jpeg**\\n\\n**The image is a bar chart titled 'Kimi-K2-Instruct Open-Ended Evaluation (aggregated)'. It compares the performance of Kimi-K2-Instruct against three other models: DeepSeek-V3-0324, Claude-Sonnet-4, and ChatGPT-4o-latest. The chart is divided into three horizontal bars, each representing a different comparison. Each bar is segmented into three sections: Win (blue), Tie (gray), and Loss (red). The percentages for each segment are as follows: Kimi-K2-Instruct vs DeepSeek-V3-0324 has 59.6% Win, 23.5% Tie, and 16.9% Loss; Kimi-K2-Instruct vs Claude-Sonnet-4 has 64.6% Win, 18.8% Tie, and 16.6% Loss; Kimi-K2-Instruct vs ChatGPT-4o-latest has 65.4% Win, 17.6% Tie, and 17.0% Loss. The x-axis represents the percentage win rate, ranging from 0% to 100%.**\\nFigure 11: Chinese in-house benchmark evaluation.\"),\n",
       " Document(id='28fe0d00-c9b2-40d6-84de-8f9838ccfe73', metadata={'doc_id': 'c9538cda-797d-4c10-85ed-1faba97a2a65', 'global_page_number': 6, 'chunk_index': 0, 'num_tables': 0, 'num_images': 2, 'source': 'RAG/OCR/responses_reindexed.json', 'page_index': 6}, page_content='Kimi K2\\n\\nTECHNICAL REPORT'),\n",
       " Document(id='7d479329-65de-4c66-91ce-97ac052dae0c', metadata={'page_index': 7, 'source': 'RAG/OCR/responses_reindexed.json', 'doc_id': 'eb17d1c1-45f9-435d-91fb-98c158eff64c', 'num_images': 0, 'chunk_index': 2, 'global_page_number': 23, 'num_tables': 0}, page_content='Kimi K2\\n\\nTECHNICAL REPORT')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = vector_store.similarity_search(\"Comparison between Kimi K2 and deepseek\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8980b",
   "metadata": {},
   "source": [
    "All right! We finished the first part of our RAG system, which is the most tiring and difficult actually. The next steps are all in LangGraph, we'll see them in the next notebook.\n",
    "\n",
    "Let's just save the heavy metadata that was not embedded: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb07a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"RAG/OCR\", exist_ok=True)\n",
    "with open(\"RAG/OCR/heavy_metadata.json\", \"w\") as f:\n",
    "    json.dump(heavy_metadata, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
