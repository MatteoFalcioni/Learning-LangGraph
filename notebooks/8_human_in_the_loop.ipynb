{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375e9bcc",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MatteoFalcioni/Learning-LangGraph/blob/main/notebooks/8_human_in_the_loop.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb06cb0",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91699c4e",
   "metadata": {},
   "source": [
    "#### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b9c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U -r https://raw.githubusercontent.com/MatteoFalcioni/Learning-LangGraph/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a73a01",
   "metadata": {},
   "source": [
    "#### local (notebooks or files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fefcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load api keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f5c63",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2dff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[32m      4\u001b[39m REQUIRED_KEYS = [\n",
      "\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m'\u001b[39m,\n",
      "\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLANGSMITH_TRACING\u001b[39m\u001b[33m'\u001b[39m,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLANGSMITH_PROJECT\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[32m     10\u001b[39m ]\n",
      "\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "REQUIRED_KEYS = [\n",
    "    'OPENAI_API_KEY',\n",
    "    'LANGSMITH_TRACING',\n",
    "    'LANGSMITH_ENDPOINT',\n",
    "    'LANGSMITH_API_KEY',\n",
    "    'LANGSMITH_PROJECT'\n",
    "]\n",
    "\n",
    "def _set_colab_keys(key : str):\n",
    "    # Retrieve the secret value using its key/name\n",
    "    secret_value = userdata.get(key)\n",
    "    # set it as a standard OS environment variable\n",
    "    os.environ[key] = secret_value\n",
    "\n",
    "for key in REQUIRED_KEYS:\n",
    "    _set_colab_keys(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f47559",
   "metadata": {},
   "source": [
    "# Human In The Loop & Time Travel\n",
    "\n",
    "---\n",
    "*References:*\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/#enable-human-intervention\n",
    "\n",
    "- https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/#use-time-travel\n",
    "---\n",
    "\n",
    "In this notebook we'll go through 2 important notions related to **interrupting and resuming our graph workflow**: \n",
    "1. *Dynamic Interrupts*. These allow us to implement **human-in-the-loop** in our workflows, i.e. dynamical human intervention in our graph to aprove routing / review tool usage / review state updates.  \n",
    "2. *Time Travel*. We saw how we can resume conversations using `thread_id`'s. Time travel is more specific than that: it allows us to go back to a specific checkpoint (a specific node execution) in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69d1e5",
   "metadata": {},
   "source": [
    "## 1. Dynamic Interrupts - the `interrupt` primitive\n",
    "\n",
    "\n",
    "Motivations for `human-in-the-loop`:\n",
    "\n",
    "(1) `Approval` - We can interrupt our agent, surface state to a user, and allow the user to accept an action\n",
    "\n",
    "(2) `Debugging` - We can rewind the graph to reproduce or avoid issues\n",
    "\n",
    "(3) `Editing` - You can modify the state \n",
    "\n",
    "LangGraph offers several ways to get or update agent state to support various `human-in-the-loop` workflows.\n",
    "\n",
    "We will introduce [*dynamic interrupts*](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/#pause-using-interrupt), which are *the recommended way* of pausing graph workflows; they are internally managed by the graph, based on its current state.\n",
    "\n",
    "There also exist static breakpoints, or [*static interrupts*](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/#debug-with-interrupts). This is an approach that is explicitly *not recommended for human-in-the-loop workflows*, and recommended instead for debugging or testing. For this reason, we will not go over it.\n",
    "\n",
    "The workflow to use dynamic interrupts can be summarized as: **we must couple an `interrupt` primitive to stop our graph and a `Command.resume` to restart it**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8da311",
   "metadata": {},
   "source": [
    "### 1.1 Example\n",
    "\n",
    "To use `interrupt`, we must:\n",
    "\n",
    "1. Specify a checkpointer to save the graph state after each step.\n",
    "2. Call `interrupt()` in the appropriate place (we'll see how) \n",
    "3. Run the graph with a thread ID until the interrupt is hit.\n",
    "4. Resume execution using invoke/stream (using the `Command` primitive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791643f",
   "metadata": {},
   "source": [
    "Let's see an example direclt from LangGraph API's documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    some_text: str\n",
    "\n",
    "\n",
    "def human_node(state: State):\n",
    "    value = interrupt(      # interrupt(...) pauses execution at human_node, surfacing the given payload to a human.\n",
    "        {\n",
    "            \"text_to_revise\": state[\"some_text\"]  \n",
    "        }\n",
    "    )\n",
    "    return {\n",
    "        \"some_text\": value  \n",
    "    }\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"human_node\", human_node)\n",
    "graph_builder.add_edge(START, \"human_node\")\n",
    "checkpointer = InMemorySaver()  # memory\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "# Pass a thread ID to the graph to run it.\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "# Run the graph until the interrupt is hit.\n",
    "result = graph.invoke({\"some_text\": \"original text\"}, config=config)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe654605",
   "metadata": {},
   "source": [
    "When the graph hits the interrupt, it returns an Interrupt object with the payload and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df355bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Interrupt(value={'text_to_revise': 'original text'}, id='2dd75cb41471915a7198035cfde2c1af')]\n"
     ]
    }
   ],
   "source": [
    "print(result['__interrupt__']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b43b2f",
   "metadata": {},
   "source": [
    "> **Note:** Interrupts resemble Python's `input()` function in terms of developer experience, but they do not automatically resume execution from the interruption point. Instead, they rerun the entire node where the interrupt was used. For this reason, interrupts are typically best placed at the start of a node or in a dedicated node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418f17b",
   "metadata": {},
   "source": [
    "To resume execution, use the `Command` primitive, which can be supplied via the `invoke` or `stream` methods. \n",
    "\n",
    "The graph resumes execution from the beginning of the node where `interrupt(...)` was initially called. \n",
    "\n",
    "This time, the interrupt function will return the value provided in `Command(resume=value)` rather than pausing again. \n",
    "\n",
    "All code from the beginning of the node to the interrupt will be re-executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274778c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'some_text': 'Edited text'}\n"
     ]
    }
   ],
   "source": [
    "print(graph.invoke(Command(resume=\"Edited text\"), config=config)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0453d804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'some_text': 'Edited text'}, next=(), config={'configurable': {'thread_id': '65a605be-5c86-40ec-980a-823c6c98083c', 'checkpoint_ns': '', 'checkpoint_id': '1f0d7482-bbce-6ab1-8001-8b4a69b51ada'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-12-12T10:49:01.990301+00:00', parent_config={'configurable': {'thread_id': '65a605be-5c86-40ec-980a-823c6c98083c', 'checkpoint_ns': '', 'checkpoint_id': '1f0d7481-f759-6783-8000-d5453f4a74aa'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f17f28",
   "metadata": {},
   "source": [
    "### 1.2. Resume multiple interrupts with one invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c49f72",
   "metadata": {},
   "source": [
    "When nodes with interrupt conditions are run in parallel, it's possible to have multiple interrupts in the task queue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c0def",
   "metadata": {},
   "source": [
    "Once your graph has been interrupted and is stalled, you can resume all the interrupts at once with `Command.resume`, passing a dictionary mapping of interrupt ids to resume values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220fc024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'text_1': 'original text 1', 'text_2': 'original text 2'}, next=('human_node_1', 'human_node_2'), config={'configurable': {'thread_id': '2ea53065-e607-4990-90ce-19b28e88c1e6', 'checkpoint_ns': '', 'checkpoint_id': '1f0d7483-07e9-61d8-8000-bcf66c602332'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-12-12T10:49:09.970355+00:00', parent_config={'configurable': {'thread_id': '2ea53065-e607-4990-90ce-19b28e88c1e6', 'checkpoint_ns': '', 'checkpoint_id': '1f0d7483-07e5-6dd0-bfff-e1f8de2fab6a'}}, tasks=(PregelTask(id='62a6babc-ecfe-6ffc-dcc4-98964c466c4f', name='human_node_1', path=('__pregel_pull', 'human_node_1'), error=None, interrupts=(Interrupt(value={'text_to_revise': 'original text 1'}, id='a8b198cd0b4a6dab6236848c6c801f50'),), state=None, result=None), PregelTask(id='9d07f82d-a702-6cc8-c3e9-5add3a50521b', name='human_node_2', path=('__pregel_pull', 'human_node_2'), error=None, interrupts=(Interrupt(value={'text_to_revise': 'original text 2'}, id='056ecdba545a42a20c94a837c46a608a'),), state=None, result=None)), interrupts=(Interrupt(value={'text_to_revise': 'original text 1'}, id='a8b198cd0b4a6dab6236848c6c801f50'), Interrupt(value={'text_to_revise': 'original text 2'}, id='056ecdba545a42a20c94a837c46a608a')))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.constants import START\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    text_1: str\n",
    "    text_2: str\n",
    "\n",
    "\n",
    "def human_node_1(state: State):\n",
    "    value = interrupt({\"text_to_revise\": state[\"text_1\"]})\n",
    "    return {\"text_1\": value}\n",
    "\n",
    "\n",
    "def human_node_2(state: State):\n",
    "    value = interrupt({\"text_to_revise\": state[\"text_2\"]})\n",
    "    return {\"text_2\": value}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"human_node_1\", human_node_1)\n",
    "graph_builder.add_node(\"human_node_2\", human_node_2)\n",
    "\n",
    "# Add both nodes in parallel from START\n",
    "graph_builder.add_edge(START, \"human_node_1\")\n",
    "graph_builder.add_edge(START, \"human_node_2\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "result = graph.invoke(\n",
    "    {\"text_1\": \"original text 1\", \"text_2\": \"original text 2\"}, config=config\n",
    ")\n",
    "\n",
    "graph.get_state(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e8b592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_1': \"human input for {'text_to_revise': 'original text 1'}\", 'text_2': \"human input for {'text_to_revise': 'original text 2'}\"}\n"
     ]
    }
   ],
   "source": [
    "# Resume with mapping of interrupt IDs to values\n",
    "resume_map = {\n",
    "    i.id: f\"human input for {i.value}\"\n",
    "    for i in graph.get_state(config=config).interrupts\n",
    "}\n",
    "print(graph.invoke(Command(resume=resume_map), config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4489ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'text_1': \"human input for {'text_to_revise': 'original text 1'}\", 'text_2': \"human input for {'text_to_revise': 'original text 2'}\"}, next=(), config={'configurable': {'thread_id': '2ea53065-e607-4990-90ce-19b28e88c1e6', 'checkpoint_ns': '', 'checkpoint_id': '1f0d7483-270a-631d-8001-953f7f249af2'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-12-12T10:49:13.234493+00:00', parent_config={'configurable': {'thread_id': '2ea53065-e607-4990-90ce-19b28e88c1e6', 'checkpoint_ns': '', 'checkpoint_id': '1f0d7483-07e9-61d8-8000-bcf66c602332'}}, tasks=(), interrupts=())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18707626",
   "metadata": {},
   "source": [
    "### 1.3 Common patterns \n",
    "\n",
    "Below we show different design patterns that can be implemented using `interrupt` and `Command`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f4f730",
   "metadata": {},
   "source": [
    "#### Approval or rejection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4c5a0",
   "metadata": {},
   "source": [
    "<img src=\"images/interrupt1.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3b9f77",
   "metadata": {},
   "source": [
    "Pause the graph before a critical step, such as an API call, to review and approve the action. \n",
    "\n",
    "If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d573f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Interrupt(value={'question': 'Do you approve the following output?', 'llm_output': 'This is the generated output.'}, id='d242f311881ea6ab0a793e858732025f')]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Define the shared graph state\n",
    "class State(TypedDict):\n",
    "    llm_output: str\n",
    "    decision: str\n",
    "\n",
    "# Simulate an LLM output node\n",
    "def generate_llm_output(state: State) -> State:\n",
    "    return {\"llm_output\": \"This is the generated output.\"}\n",
    "\n",
    "# Human approval node\n",
    "def human_approval(state: State) -> Command[Literal[\"approved_path\", \"rejected_path\"]]:\n",
    "    decision = interrupt({\n",
    "        \"question\": \"Do you approve the following output?\",\n",
    "        \"llm_output\": state[\"llm_output\"]\n",
    "    })\n",
    "\n",
    "    if decision == \"approve\":\n",
    "        return Command(goto=\"approved_path\", update={\"decision\": \"approved\"})\n",
    "    else:\n",
    "        return Command(goto=\"rejected_path\", update={\"decision\": \"rejected\"})\n",
    "\n",
    "# Next steps after approval\n",
    "def approved_node(state: State) -> State:\n",
    "    print(\"‚úÖ Approved path taken.\")\n",
    "    return state\n",
    "\n",
    "# Alternative path after rejection\n",
    "def rejected_node(state: State) -> State:\n",
    "    print(\"‚ùå Rejected path taken.\")\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate_llm_output\", generate_llm_output)\n",
    "builder.add_node(\"human_approval\", human_approval)\n",
    "builder.add_node(\"approved_path\", approved_node)\n",
    "builder.add_node(\"rejected_path\", rejected_node)\n",
    "\n",
    "builder.set_entry_point(\"generate_llm_output\")\n",
    "builder.add_edge(\"generate_llm_output\", \"human_approval\")\n",
    "builder.add_edge(\"approved_path\", END)\n",
    "builder.add_edge(\"rejected_path\", END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run until interrupt\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = graph.invoke({}, config=config)\n",
    "print(result[\"__interrupt__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e541921",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_value = result['__interrupt__'][0].value\n",
    "\n",
    "print(inter_value)\n",
    "print()\n",
    "\n",
    "print(f\"Graph interrupted with the question: {inter_value['question']} \\nOutput: {inter_value['llm_output']}. \\nPossible decisions: 'approve' or 'reject'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618a09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Approved path taken.\n",
      "{'llm_output': 'This is the generated output.', 'decision': 'approved'}\n"
     ]
    }
   ],
   "source": [
    "# Simulate resuming with human input\n",
    "# To test rejection, replace resume=\"approve\" with resume=\"reject\"\n",
    "final_result = graph.invoke(Command(resume=\"approve\"), config=config)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50710283",
   "metadata": {},
   "source": [
    "#### Review and edit state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd6f74",
   "metadata": {},
   "source": [
    "<img src=\"images/interrupt2.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218f987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Interrupt(value={'task': 'Please review and edit the generated summary if necessary.', 'generated_summary': 'The cat sat on the mat and looked at the stars.'}, id='22c3961634854968307dda2c89db74f6')]\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import uuid\n",
    "\n",
    "from langgraph.constants import START, END\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Define the graph state\n",
    "class State(TypedDict):\n",
    "    summary: str\n",
    "\n",
    "# Simulate an LLM summary generation\n",
    "def generate_summary(state: State) -> State:\n",
    "    return {\n",
    "        \"summary\": \"The cat sat on the mat and looked at the stars.\"\n",
    "    }\n",
    "\n",
    "# Human editing node\n",
    "def human_review_edit(state: State) -> State:\n",
    "    result = interrupt({\n",
    "        \"task\": \"Please review and edit the generated summary if necessary.\",\n",
    "        \"generated_summary\": state[\"summary\"]\n",
    "    })\n",
    "    return {\n",
    "        \"summary\": result[\"edited_summary\"]\n",
    "    }\n",
    "\n",
    "# Simulate downstream use of the edited summary\n",
    "def downstream_use(state: State) -> State:\n",
    "    print(f\"‚úÖ Using edited summary: {state['summary']}\")\n",
    "    return state\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"generate_summary\", generate_summary)\n",
    "builder.add_node(\"human_review_edit\", human_review_edit)\n",
    "builder.add_node(\"downstream_use\", downstream_use)\n",
    "\n",
    "builder.set_entry_point(\"generate_summary\")\n",
    "builder.add_edge(\"generate_summary\", \"human_review_edit\")\n",
    "builder.add_edge(\"human_review_edit\", \"downstream_use\")\n",
    "builder.add_edge(\"downstream_use\", END)\n",
    "\n",
    "# Set up in-memory checkpointing for interrupt support\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph until it hits the interrupt\n",
    "config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = graph.invoke({}, config=config)\n",
    "\n",
    "# Output interrupt payload\n",
    "print(result[\"__interrupt__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using edited summary: The cat lay on the rug, gazing peacefully at the night sky.\n",
      "{'summary': 'The cat lay on the rug, gazing peacefully at the night sky.'}\n"
     ]
    }
   ],
   "source": [
    "# Resume the graph with human-edited input\n",
    "edited_summary = \"The cat lay on the rug, gazing peacefully at the night sky.\"\n",
    "resumed_result = graph.invoke(\n",
    "    Command(resume={\"edited_summary\": edited_summary}),\n",
    "    config=config\n",
    ")\n",
    "print(resumed_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743713c",
   "metadata": {},
   "source": [
    "#### Review tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72ff46",
   "metadata": {},
   "source": [
    "<img src=\"images/interrupt3.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8eec6",
   "metadata": {},
   "source": [
    "To add a human approval step to a tool:\n",
    "\n",
    "1. Use `interrupt()` in the tool to pause execution.\n",
    "2. Resume with a `Command` to continue based on human input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f254db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.types import interrupt\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# An example of a sensitive tool that requires human review / approval\n",
    "@tool\n",
    "def book_hotel(hotel_name: str):\n",
    "    \"\"\"Book a hotel\"\"\"\n",
    "    response = interrupt(  \n",
    "        f\"Trying to call `book_hotel` with args {{'hotel_name': {hotel_name}}}. \"\n",
    "        \"Please approve or suggest edits.\"\n",
    "    )\n",
    "    if response[\"type\"] == \"accept\":\n",
    "        pass\n",
    "    elif response[\"type\"] == \"edit\":\n",
    "        hotel_name = response[\"args\"][\"hotel_name\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown response type: {response['type']}\")\n",
    "    return f\"Successfully booked a stay at {hotel_name}.\"\n",
    "\n",
    "checkpointer = InMemorySaver() \n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    tools=[book_hotel],\n",
    "    checkpointer=checkpointer, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbfad38",
   "metadata": {},
   "source": [
    "Stream the agent output, passing the config object to specify the thread ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea19aef",
   "metadata": {},
   "source": [
    "It will run until reaching interruption:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b6032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_GkVnN8Bb0KUwOM1byMULZNaj', 'function': {'arguments': '{\"hotel_name\":\"McKittrick hotel\"}', 'name': 'book_hotel'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 52, 'total_tokens': 72, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CCNV5HgHGunOmU9vcvobMBgKfVl9T', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1f0b13b3-72c4-41f1-9f16-6c40c5b78b45-0', tool_calls=[{'name': 'book_hotel', 'args': {'hotel_name': 'McKittrick hotel'}, 'id': 'call_GkVnN8Bb0KUwOM1byMULZNaj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 52, 'output_tokens': 20, 'total_tokens': 72, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "\n",
      "{'__interrupt__': (Interrupt(value=\"Trying to call `book_hotel` with args {'hotel_name': McKittrick hotel}. Please approve or suggest edits.\", id='22eeb4895d066455eae602f953088197'),)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "   \"configurable\": {\n",
    "      \"thread_id\": \"1\"\n",
    "   }\n",
    "}\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]},\n",
    "    config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2ff4c",
   "metadata": {},
   "source": [
    "Resume the agent with a `Command` to continue based on human input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f270a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tools': {'messages': [ToolMessage(content='Successfully booked a stay at McKittrick hotel.', name='book_hotel', id='42e0e213-685a-4bf3-8e07-421c1ef525dc', tool_call_id='call_GkVnN8Bb0KUwOM1byMULZNaj')]}}\n",
      "\n",
      "\n",
      "{'agent': {'messages': [AIMessage(content='Your stay at McKittrick Hotel has been successfully booked.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 92, 'total_tokens': 106, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_cbf1785567', 'id': 'chatcmpl-CCNV6tY9hHYty3LxnzBqjMXP5g3GI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a0cbc5cf-9bd9-4d2d-992a-7df07151c5d9-0', usage_metadata={'input_tokens': 92, 'output_tokens': 14, 'total_tokens': 106, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    Command(resume={\"type\": \"accept\"}),  \n",
    "    # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}),\n",
    "    config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00014aa2",
   "metadata": {},
   "source": [
    "# 2. Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837b24b",
   "metadata": {},
   "source": [
    "When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:\n",
    "\n",
    "- ü§î Understand reasoning: Analyze the steps that led to a successful result.\n",
    "- üêû Debug mistakes: Identify where and why errors occurred.\n",
    "- üîç Explore alternatives: Test different paths to uncover better solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38401f25",
   "metadata": {},
   "source": [
    "LangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint ‚Äî either replaying the same state or modifying it to explore alternatives. \n",
    "\n",
    "In all cases, resuming past execution produces a new fork in the history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f571d7b",
   "metadata": {},
   "source": [
    "### 2.1 How to Time Travel\n",
    "\n",
    "To use time-travel in LangGraph:\n",
    "\n",
    "1. Run the graph with initial inputs using `invoke` or `stream` methods.\n",
    "\n",
    "2. Identify a checkpoint in an existing thread: Use the `get_state_history()` method to retrieve the execution history for a specific `thread_id` and locate the desired `checkpoint_id`.\n",
    "Alternatively, set an `interrupt` before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt.\n",
    "\n",
    "3. Update the graph state (optional): Use the `update_state` method to modify the graph's state at the checkpoint and resume execution from alternative state.\n",
    "\n",
    "4. Resume execution from the checkpoint: Use the invoke or stream methods with an input of None and a configuration containing the appropriate `thread_id` and `checkpoint_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea56a2f",
   "metadata": {},
   "source": [
    "### 2.2 Example\n",
    "\n",
    "This example is taken directly from the LangGraph guide. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb976e",
   "metadata": {},
   "source": [
    "It builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb25108",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "176c61a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x736af86a0190>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from typing_extensions import TypedDict, NotRequired\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: NotRequired[str]\n",
    "    joke: NotRequired[str]\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"openai:gpt-4o\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_topic(state: State):\n",
    "    \"\"\"LLM call to generate a topic for the joke\"\"\"\n",
    "    msg = llm.invoke(\"Give me a funny topic for a joke\")\n",
    "    return {\"topic\": msg.content}\n",
    "\n",
    "\n",
    "def write_joke(state: State):\n",
    "    \"\"\"LLM call to write a joke based on the topic\"\"\"\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generate_topic\", generate_topic)\n",
    "workflow.add_node(\"write_joke\", write_joke)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "workflow.add_edge(START, \"generate_topic\")\n",
    "workflow.add_edge(\"generate_topic\", \"write_joke\")\n",
    "workflow.add_edge(\"write_joke\", END)\n",
    "\n",
    "# Compile\n",
    "checkpointer = InMemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07767b90",
   "metadata": {},
   "source": [
    "#### 1. Run the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38daa6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How about a joke about the unexpected challenges of working from home? For example, \"Why did the laptop apply for a job at the zoo? It wanted to work remotely, but not from the jungle of my living room!\"\n",
      "\n",
      "Why did the cat start attending virtual meetings? It wanted to remind everyone who's really in charge of the home office!\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": uuid.uuid4(),\n",
    "    }\n",
    "}\n",
    "state = graph.invoke({}, config)\n",
    "\n",
    "print(state[\"topic\"])\n",
    "print()\n",
    "print(state[\"joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b4101",
   "metadata": {},
   "source": [
    "#### 2. Identify a chechkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aec9c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "1f0d7496-11fa-62e0-8002-994cd258192c\n",
      "\n",
      "('write_joke',)\n",
      "1f0d7496-00b1-672c-8001-8be21b1fe218\n",
      "\n",
      "('generate_topic',)\n",
      "1f0d7495-e96a-6291-8000-33ab09a0b1a6\n",
      "\n",
      "('__start__',)\n",
      "1f0d7495-e967-6199-bfff-cd0b06894226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The states are returned in reverse chronological order.\n",
    "states = list(graph.get_state_history(config))\n",
    "\n",
    "for state in states:\n",
    "    print(state.next)\n",
    "    print(state.config[\"configurable\"][\"checkpoint_id\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec98345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('write_joke',)\n",
      "{'topic': 'How about a joke about the unexpected challenges of working from home? For example, \"Why did the laptop apply for a job at the zoo? It wanted to work remotely, but not from the jungle of my living room!\"'}\n"
     ]
    }
   ],
   "source": [
    "# This is the state before last (states are listed in chronological order)\n",
    "selected_state = states[1]\n",
    "print(selected_state.next)\n",
    "print(selected_state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9e4b5",
   "metadata": {},
   "source": [
    "#### 3. Update the state (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da67506",
   "metadata": {},
   "source": [
    "`update_state` will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5503f8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'configurable': {'thread_id': '78815b36-1327-4881-a44b-86d404f10bac', 'checkpoint_ns': '', 'checkpoint_id': '1f0d7496-125a-64a0-8002-07b6d19a7947'}}\n"
     ]
    }
   ],
   "source": [
    "new_config = graph.update_state(selected_state.config, values={\"topic\": \"chickens\"})\n",
    "print(new_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c552b58",
   "metadata": {},
   "source": [
    "#### 4. Resume execution from the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "532b709f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'chickens',\n",
       " 'joke': 'Why did the chicken join a band? Because it had the drumsticks!'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(None, new_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71afd9a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17e4d7",
   "metadata": {},
   "source": [
    "Find exercises for this chapter in [notebook 9.5](./9.5_exercises.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
