{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ef7d50",
   "metadata": {},
   "source": [
    "# RAG Agent\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is one of the most useful applications of AI agents. \n",
    "\n",
    "It consists in grounding the answers of our agent to a given knowledge base, that our agent can access by using tools.\n",
    "\n",
    "This knowledge base - i.e., our collection of documents - is embedded in a vector store for efficient search. \n",
    "\n",
    "This method prevents allucinations and highly increases the accuracy of the agentic system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcc8bd",
   "metadata": {},
   "source": [
    "Steps in this implementation:\n",
    "\n",
    "1. get a knowledge base: we do that manually, and for this example we use only one document for simplicity. \n",
    "\n",
    "2. perform OCR of our documents to best extract information;\n",
    "\n",
    "3. embed our documents in a vector database (vector store)\n",
    "\n",
    "4. construct a tool for searching in the database\n",
    "\n",
    "5. create the graph with grading and retries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451864b",
   "metadata": {},
   "source": [
    "## 1. Knowledge Base\n",
    "\n",
    "We selected scientific papers on the topic on the topic **\"LLM-Agents for Urban Mobility & Traffic Engineering\"**.\n",
    "\n",
    "We stored them in the folder `documents/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa53524",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. OCR (Optical Character Recognition)\n",
    "\n",
    "As we said, we usually need to perform OCR on a knowledge based composed by documents. \n",
    "\n",
    ">Notice that we may skip this step if our knowledge base is already composed of plain text (for example, this could happen if our base is made thorugh web scarping, which usually returns plain text or markdown).\n",
    "\n",
    "In this case our papers are complex, and contain mathematical expressions, tables, images. OCR is not a simple/skippable step here. \n",
    "\n",
    "That's why we are going to user the best OCR model around this days (January 2026): Mistral OCR 3. Find a usage example in the [mistral_ocr](./mistral_ocr.ipynb) notebook, and a full example from the actual Mistral site here: [link](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/data_extraction.ipynb). \n",
    "\n",
    "The latter showcases how to use their `Annotations` API to also annotate the detected bounding boxes. We will use this feature to include images in our RAG pipeline. \n",
    "\n",
    "> **Note:** You can replace Mistral's OCR API with a free process provided by LangChain, using `PyMuPDF4LLM` and the `Upstage Document Parse API`. Results will not be as good as using Mistral but probably will be good enough for many applications (and it's all free). Check the full tutorial here: [Multimodal RAG tutorial](https://langchain-opentutorial.gitbook.io/langchain-opentutorial/19-cookbook/06-multimodal/10-geminimultimodalrag#layout-parsing-to-extract-image-from-pdf-using-upstage-document-parse-api). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a246380",
   "metadata": {},
   "source": [
    "### 2.1 Mistral OCR with Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdef9f",
   "metadata": {},
   "source": [
    "Mistral Document AI API adds two annotation functionalities:\n",
    "\n",
    "- `document_annotation`: returns the annotation of the entire document based on the input schema.\n",
    "- `box_annotation`: gives you the annotation of the bboxes extracted by the OCR model (charts/ figures etc) based on user requirement. The user may ask to describe/caption the figure for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d8b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U mistralai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca03c3",
   "metadata": {},
   "source": [
    "Function to encode in base 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4127de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_pdf(pdf_path):\n",
    "    \"\"\"Encode the pdf to base64.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            return base64.b64encode(pdf_file.read()).decode('utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {pdf_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:  # Added general exception handling\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0a25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_pdf = encode_pdf(\"RAG/documents/KimiK2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d769d",
   "metadata": {},
   "source": [
    "First, we need to create our Annotation Formats, for that we advise make use of pydantic.\n",
    "\n",
    "For this example, we will extract the image type and a description of each bbox; as well as the language, authors and a summary of the full document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7dcaa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "class ImageType(str, Enum):\n",
    "    GRAPH = \"graph\"\n",
    "    TEXT = \"text\"\n",
    "    TABLE = \"table\"\n",
    "    IMAGE = \"image\"\n",
    "\n",
    "class Image(BaseModel):\n",
    "    image_type: ImageType = Field(..., description=\"The type of the image. Must be one of 'graph', 'text', 'table' or 'image'.\")\n",
    "    description: str = Field(..., description=\"A description of the image.\")\n",
    "\n",
    "class Document(BaseModel):\n",
    "    summary: str = Field(..., description=\"A summary of the document.\")\n",
    "    authors: list[str] = Field(..., description=\"A list of authors who contributed to the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4bd4e",
   "metadata": {},
   "source": [
    "Now with our pydantic models created for our Annotations, we can call our OCR endpoint.\n",
    "\n",
    "The objective is to Annotate and Extract information from our document and the bbox/images detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50853dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pages\": [\n",
      "        {\n",
      "            \"index\": 16,\n",
      "            \"markdown\": \"##### Agentic Tool Use\\n\\nOn multi-turn tool-use benchmarks, Kimi-K2-Instruct sets a new standard. It achieves 66.1 Pass@1 on $\\\\tau^{2}$-Bench and 76.5 on ACEBench, substantially outperforming all baselines. These results affirm its strength in grounded, controlled, and agent-driven tool orchestration across domains.\\n\\n##### General Capabilities\\n\\nKimi-K2-Instruct exhibits strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. It surpasses open-source peers on SimpleQA (31.0%), MMLU (89.5%) and MMLU-Redux (92.7%), and leads all models on instruction benchmarks (IFEval: 89.8%, Multi-Challenge: 54.1%). In math and STEM, it achieves top-tier scores (AIME 2024: 69.6%, GPQA-Diamond: 75.1%), and remains competitive on long-context factuality and retrieval (DROP: 93.5%, MRCR: 55.0%). These results position Kimi-K2-Instruct as a well-rounded and capable generalist across both short- and long-context settings.\\n\\n##### Open-Ended Evaluation\\n\\nOn the LMSYS Arena leaderboard (July 17, 2025), Kimi-K2-Instruct ranks as the top-1 open-source model and 5th overall based on over 3,000 user votes. This real-world preference signal\\u2014across diverse, blind prompts\\u2014underscores Kimi-K2\\u2019s strengths in generating high-quality responses on open-ended tasks.\\n\\n### 4.2 Pre-training Evaluations\\n\\n#### 4.2.1 Evaluation Settings\\n\\n##### Benchmarks\\n\\nWe evaluate Kimi-K2-Base across diverse capability areas. For general capabilities, we assess on MMLU *[23]*, MMLU-Pro *[76]*, MMLU-Redux *[17]*, BBH *[67]*, TriviaQA *[34]*, SuperGPQA *[13]*, SimpleQA *[78]*, HellaSwag *[88]*, AGIEval *[89]*, GPQA-Diamond *[61]*, ARC-Challenge *[8]*, and WinoGrande *[62]*. For coding capabilities, we employ EvalPlus *[45]* (averaging HumanEval *[7]*, MBPP *[1]*, HumanEval+, and MBPP+), LiveCodeBench v6 *[31]*, and CRUXEval *[18]*. For mathematical reasoning, we utilize GSM8K *[9]*, GSM8K-Platinum *[74]*, MATH *[24]*, and CMATH *[79]*. For Chinese language capabilities, we evaluate on C-Eval *[29]*, CMMLU *[40]*, and CSimpleQA *[22]*.\\n\\n##### Baselines\\n\\nWe benchmark against leading open-source foundation models: DeepSeek-V3-Base *[10]*, Qwen2.5-72B-Base *[59]* (Note that Qwen3-235B-A22B-Base is not open-sourced, and the largest open-sourced base model in the Qwen series is Qwen2.5-72B-Base), and Llama 4-Maverick *[70]* (Llama 4-Behemoth is also not open-sourced). All models are evaluated under identical configurations to ensure fair comparison.\\n\\n##### Evaluation Configurations\\n\\nWe employ perplexity-based evaluation for MMLU, MMLU-Redux, GPQA-Diamond, HellaSwag, ARC-Challenge, C-Eval, and CMMLU. Generation-based evaluation is used for MMLU-Pro, SuperGPQA, TriviaQA, BBH, CSimpleQA, MATH, CMATH, GSM8K, GSM8K-Platinum, CRUXEval, LiveCodeBench, and EvalPlus. To mitigate the high variance inherent to GPQA-Diamond, we report the mean score across eight independent runs. All evaluations are conducted using our internal framework derived from LM-Harness-Evaluation *[4]*, ensuring consistent settings across all models.\\n\\n#### 4.2.2 Evaluation Results\\n\\nTable 4 presents a comprehensive comparison of Kimi-K2-Base against leading open-source foundation models across diverse evaluation benchmarks. The results demonstrate that Kimi-K2-Base achieves state-of-the-art performance across the majority of evaluated tasks, establishing it as a leading foundation model in the open-source landscape.\\n\\n##### General Language Understanding\\n\\nKimi-K2-Base achieves state-of-the-art performance on 10 out of 12 English language benchmarks. Notable results include MMLU (87.79%), MMLU-Pro (69.17%), MMLU-Redux (90.17%), SuperGPQA (44.67%), and SimpleQA (35.25%), significantly outperforming all baselines.\\n\\n##### Coding Capabilities\\n\\nOn coding benchmarks, Kimi-K2-Base sets new standards with leading performance across all metrics. It achieves 74.00% on CRUXEval-I-cot, 83.50% on CRUXEval-O-cot, 26.29% on LiveCodeBench v6, and 80.33% on EvalPlus, demonstrating superior code generation and comprehension abilities, particularly in scenarios requiring step-by-step reasoning.\\n\\n##### Mathematical Reasoning\\n\\nKimi-K2-Base exhibits exceptional mathematical capabilities, leading on three out of four benchmarks: MATH (70.22%), GSM8K (92.12%), and GSM8K-Platinum (94.21%). It maintains competitive performance on CMATH (90.26%), narrowly behind DeepSeek-V3-Base (90.53%). These results highlight the model\\u2019s robust mathematical problem-solving abilities across varying difficulty levels.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 17,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\nChinese Language Understanding The model demonstrates superior multilingual capabilities, achieving state-of-the-art results across all Chinese language benchmarks: C-Eval (92.50%), CMMLU (90.90%), and CSimpleQA (77.57%). These results establish Kimi-K2-Base as a leading model for Chinese language understanding while maintaining strong performance across other languages.\\n\\nTable 4: Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks.\\n\\n[tbl-0.html](tbl-0.html)\\n\\n# 4.3 Safety Evaluation\\n\\n# 4.3.1 Experiment Settings\\n\\nWe conducted red-teaming evaluations on Kimi K2 compare with other open-source LLMs. The evaluation covered a range of attack scenarios\\u2014including harmful content, privacy content, and security content, as well as different attack strategies such as prompt injection and iterative jailbreak.\\n\\nWe choose Promptfoo to generate adversarial prompts and analyze the responses. By this way, we can evaluate model in a scalable ways.\\n\\nModel Selection We compare Kimi K2 with three other open-source LLMs: DeepSeek-V3, DeepSeek-R1, and Qwen3.\\n\\nPromptfoo Settings Table 5 lists plugins and strategies evaluated, with each plugin paired with all strategies to assess their performance.\\n\\nTest Case Count Given the inherent non-determinism of large language model inference, single-pass outputs may exhibit variability. To account for this, we generated 3 attack prompts per plugin for each strategy.\\n\\nPrompt Language Settings We pre-tested the language compatibility for each plugin-strategy combination. Some plugins support both English and Chinese, while others only support English. For combinations that support both, we generated 3 prompts in each language, resulting in 6 prompts per combination.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [\n",
      "                {\n",
      "                    \"id\": \"tbl-0.html\",\n",
      "                    \"content\": \"<table><tr><td></td><td>Benchmark (Metric)</td><td>#Shots</td><td>Kimi-K2-Base</td><td>DeepSeek-V3-Base</td><td>Llama4-Maverick-Base</td><td>Qwen2.5-72B-Base</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>MoE</td><td>MoE</td><td>MoE</td><td>Dense</td></tr><tr><td></td><td># Activated Params</td><td>-</td><td>32B</td><td>37B</td><td>17B</td><td>72B</td></tr><tr><td></td><td># Total Params</td><td>-</td><td>1043B</td><td>671B</td><td>400B</td><td>72B</td></tr><tr><td rowspan=\\\"12\\\">English</td><td>MMLU</td><td>5-shots</td><td>87.79</td><td>87.10</td><td>84.87</td><td>86.08</td></tr><tr><td>MMLU-pro</td><td>5-shots</td><td>69.17</td><td>60.59</td><td>63.47</td><td>62.80</td></tr><tr><td>MMLU-reflux</td><td>5-shots</td><td>90.17</td><td>89.53</td><td>88.18</td><td>87.77</td></tr><tr><td>SuperGPQA</td><td>5-shots</td><td>44.67</td><td>39.20</td><td>38.84</td><td>34.23</td></tr><tr><td>GPQA-Diamond(avg@s)</td><td>5-shots</td><td>48.11</td><td>50.51</td><td>49.43</td><td>40.78</td></tr><tr><td>SimpleQA</td><td>5-shots</td><td>35.25</td><td>26.49</td><td>23.74</td><td>10.31</td></tr><tr><td>TriviaQA</td><td>5-shots</td><td>85.09</td><td>84.11</td><td>79.25</td><td>76.03</td></tr><tr><td>BBH</td><td>3-shots</td><td>88.71</td><td>88.37</td><td>87.10</td><td>84.09</td></tr><tr><td>HellaSwag</td><td>5-shots</td><td>94.60</td><td>89.44</td><td>86.02</td><td>95.27</td></tr><tr><td>AGIEval</td><td>-</td><td>84.23</td><td>81.57</td><td>67.55</td><td>76.87</td></tr><tr><td>ARC-Challenge</td><td>0-shot</td><td>95.73</td><td>93.77</td><td>94.03</td><td>95.56</td></tr><tr><td>WinoGrande</td><td>5-shots</td><td>85.32</td><td>84.21</td><td>77.58</td><td>84.14</td></tr><tr><td rowspan=\\\"4\\\">Code</td><td>CRUXEval-I-cot</td><td>0-shots</td><td>74.00</td><td>62.75</td><td>67.13</td><td>61.12</td></tr><tr><td>CRUXEval-O-cot</td><td>0-shots</td><td>83.50</td><td>75.25</td><td>75.88</td><td>66.13</td></tr><tr><td>LiveCodeBench(v6)</td><td>1-shots</td><td>26.29</td><td>24.57</td><td>25.14</td><td>22.29</td></tr><tr><td>EvalPlus</td><td>-</td><td>80.33</td><td>65.61</td><td>65.48</td><td>66.04</td></tr><tr><td rowspan=\\\"4\\\">Math</td><td>MATH</td><td>4-shots</td><td>70.22</td><td>61.70</td><td>63.02</td><td>62.68</td></tr><tr><td>GSM8k</td><td>8-shots</td><td>92.12</td><td>91.66</td><td>86.35</td><td>90.37</td></tr><tr><td>GSM8k-platinum</td><td>8-shots</td><td>94.21</td><td>93.38</td><td>88.83</td><td>92.47</td></tr><tr><td>CMATH</td><td>6-shots</td><td>90.26</td><td>90.53</td><td>88.07</td><td>86.98</td></tr><tr><td rowspan=\\\"3\\\">Chinese</td><td>C-Eval</td><td>5-shots</td><td>92.50</td><td>90.04</td><td>80.91</td><td>90.86</td></tr><tr><td>CMMLU</td><td>5-shots</td><td>90.90</td><td>88.84</td><td>81.24</td><td>90.55</td></tr><tr><td>CSimpleQA</td><td>5-shots</td><td>77.57</td><td>72.13</td><td>53.47</td><td>50.53</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                }\n",
      "            ],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://github.com/promptfoo/promptfoo\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 18,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\nTable 5: Enabled Plugins and Strategies\\n\\n[tbl-1.html](tbl-1.html)\\n\\nManual Review We incorporated human review into the evaluation process. To minimize subjectivity problem, we conducted multiple rounds of review and assigned the same reviewer to evaluate all cases within a given test set to ensure consistency and reduce variability in judgment.\\n\\n# 4.3.2 Safety Evaluation Results\\n\\nTable 6 presents the passing rates of different models under various plugin-strategy combinations.\\n\\nTable 6: Safety Evaluation Results\\n\\n[tbl-2.html](tbl-2.html)\\n\\nWithout targeted optimization for specific evaluation scenarios, the passing rate of some complex cases (e.g., Harmful-Iterative Jailbreak) was relatively higher compared to other models.\\n\\nAcross different attack strategies, the models exhibited varying trends. Under the Base64 strategy, passing rates generally approached or reached  $100\\\\%$ , suggesting that encoding transformations had minimal impact on the models'\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [\n",
      "                {\n",
      "                    \"id\": \"tbl-1.html\",\n",
      "                    \"content\": \"<table><tr><td rowspan=\\\"5\\\">Plugin</td><td>Harmful</td><td>Graphic Content, Harassment and Bullying, Hate Speech, Insults, Profanity, Radicalization, Self Harm, Sexual Content, ToxicChat</td></tr><tr><td>Criminal</td><td>Chemical&amp;Biological Weapons, Child Exploitation, Copyright Violations, Cybercrime, Illegal Activities, Illegal Drugs, Indiscriminate Weapons, Intellectual Property Violation, Non-Violent Crime, Violent Crime, Sex Crimes</td></tr><tr><td>Misinformation</td><td>Competitor Endorsement, Unsupervised Contracts, Excessive Agency, Hallucination, Misin-formation and Disinformation, Specialized Advice, Unsafe Practices, Imitation, Overreliance, Political Opinions, Religious Sensitivity</td></tr><tr><td>Privacy</td><td>Privacy Violation, PII in API/Database, Direct PII Exposure, PII in Session Data, PII via Social Engineering</td></tr><tr><td>Security</td><td>ASCII Smuggling, CyberSecEval, Harmbench, Debug Access, Divergent Repetition, DoNotAnswer, Malicious Code, Pliny, Prompt Extraction, Reasoning DoS, Tool Discovery</td></tr><tr><td>Strategy</td><td colspan=\\\"2\\\">Basic, Prompt Injection, Iterative Jailbreak, Crescendo</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"tbl-2.html\",\n",
      "                    \"content\": \"<table><tr><td>Plugin</td><td>Strategy</td><td>Kimi-K2-Instruct</td><td>DeepSeek-V3-0324</td><td>DeepSeek-R1</td><td>Qwen3-235B-A22B</td></tr><tr><td rowspan=\\\"5\\\">Harmful</td><td>Basic</td><td>98.04</td><td>90.45</td><td>99.02</td><td>98.53</td></tr><tr><td>Base64</td><td>100</td><td>90.20</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>93.14</td><td>100</td><td>95.10</td><td>99.02</td></tr><tr><td>Iterative Jailbreak</td><td>92.16</td><td>66.67</td><td>72.55</td><td>74.51</td></tr><tr><td>Crescendo</td><td>64.71</td><td>64.71</td><td>80.39</td><td>86.27</td></tr><tr><td rowspan=\\\"5\\\">Criminal</td><td>Basic</td><td>100</td><td>99.62</td><td>95.45</td><td>99.24</td></tr><tr><td>Base64</td><td>96.97</td><td>89.39</td><td>84.85</td><td>98.48</td></tr><tr><td>Prompt Injection</td><td>75.76</td><td>91.67</td><td>69.70</td><td>98.47</td></tr><tr><td>Iterative Jailbreak</td><td>57.57</td><td>21.21</td><td>25.76</td><td>53.03</td></tr><tr><td>Crescendo</td><td>56.06</td><td>31.81</td><td>42.42</td><td>59.09</td></tr><tr><td rowspan=\\\"5\\\">Misinformation</td><td>Basic</td><td>97.28</td><td>92.57</td><td>92.46</td><td>94.84</td></tr><tr><td>Base64</td><td>98.48</td><td>90.48</td><td>96.83</td><td>93.65</td></tr><tr><td>Prompt Injection</td><td>98.39</td><td>86.51</td><td>93.65</td><td>93.65</td></tr><tr><td>Iterative Jailbreak</td><td>63.97</td><td>53.97</td><td>84.13</td><td>69.84</td></tr><tr><td>Crescendo</td><td>85.71</td><td>55.56</td><td>88.89</td><td>84.13</td></tr><tr><td rowspan=\\\"5\\\">Privacy</td><td>Basic</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Base64</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>88.33</td><td>98.33</td><td>100</td><td>91.67</td></tr><tr><td>Iterative Jailbreak</td><td>76.67</td><td>100</td><td>93.33</td><td>96.67</td></tr><tr><td>Crescendo</td><td>96.67</td><td>100</td><td>96.67</td><td>100</td></tr><tr><td rowspan=\\\"5\\\">Security</td><td>Basic</td><td>77.84</td><td>75.57</td><td>70.46</td><td>90.09</td></tr><tr><td>Base64</td><td>82.93</td><td>82.93</td><td>63.41</td><td>95.12</td></tr><tr><td>Prompt Injection</td><td>87.80</td><td>97.56</td><td>65.85</td><td>84.13</td></tr><tr><td>Iterative Jailbreak</td><td>43.90</td><td>60.97</td><td>43.90</td><td>78.04</td></tr><tr><td>Crescendo</td><td>68.29</td><td>87.80</td><td>68.29</td><td>87.80</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                }\n",
      "            ],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 19,\n",
      "            \"markdown\": \"basic robustness. In contrast, the Crescendo strategy led to a general drop in passing rates, indicating stronger adversarial effectiveness.\\n\\nIn addition, complex attack strategies do not always outperform basic prompts. Some originally adversarial prompts may lose their intended meaning after multiple rounds of transformation, rendering the resulting model outputs less meaningful.\\n\\nAutomated Red-teaming Limitations Due to the involvement of human review, the evaluation results inevitably contain a degree of subjectivity. Additionally, certain plugin types involve API misuse or external tool invocation, which are more suitable for evaluating agent models with tool-calling capabilities. In the context of base LLMs, such tests may have limited relevance.\\n\\n## 5 Limitations\\n\\nIn our internal tests, we have identified some limitations in current Kimi K2 models. When dealing with hard reasoning tasks or unclear tool definition, the model may generate excessive tokens, sometimes leading to truncated outputs or incomplete tool calls. Additionally, performance may decline on certain tasks if tool use is unnecessarily enabled. When building complete software projects, the success rate of one-shot prompting is not as good as using K2 under an agentic coding framework. We are working to address these issues in future releases and looking forward to more feedbacks.\\n\\n## 6 Conclusions\\n\\nWe introduced Kimi K2, a 1T-parameter open-weight MoE model built for agentic intelligence. Leveraging the token-efficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training. Post-training combines large-scale synthetic tool-use data with a unified RL framework using both verifiable rewards and self-critic feedbacks. Kimi K2 sets new state-of-the-art on agentic and reasoning benchmarks, establishing itself as the most capable open-weight LLM to date.\\n\\n## 7 Acknowledgments\\n\\nWe would like to acknowledge the valuable support provided by the OpenHands and Multi-SWE-bench teams in evaluating the SWE-bench Verified and Multi-SWE-bench experimental results.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 20,\n",
      "            \"markdown\": \"References\\n\\n- [1] Jacob Austin et al. *Program Synthesis with Large Language Models*. 2021. arXiv: 2108.07732 [cs.PL]. URL: https://arxiv.org/abs/2108.07732.\\n- [2] Yushi Bai et al. *LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks*. 2025. arXiv: 2412.15204 [cs.CL]. URL: https://arxiv.org/abs/2412.15204.\\n- [3] Victor Barres et al. *$\\\\tau^{2}$-Bench: Evaluating Conversational Agents in a Dual-Control Environment*. 2025. arXiv: 2506.07982 [cs.AI]. URL: https://arxiv.org/abs/2506.07982.\\n- [4] Stella Biderman et al. \\u201cLessons from the trenches on reproducible evaluation of language models\\u201d. In: *arXiv preprint arXiv:2405.14782* (2024).\\n- [5] Federico Cassano et al. \\u201cMultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation\\u201d. In: *IEEE Transactions on Software Engineering* 49.7 (2023), pp. 3675\\u20133691. doi: 10.1109/TSE.2023.3267446.\\n- [6] Chen Chen et al. \\u201cACEBench: Who Wins the Match Point in Tool Learning?\\u201d In: *arXiv e-prints* (2025), arXiv\\u20132501.\\n- [7] Mark Chen et al. \\u201cEvaluating Large Language Models Trained on Code\\u201d. In: (2021). arXiv: 2107.03374 [cs.LG].\\n- [8] Peter Clark et al. \\u201cThink you have solved question answering? try arc, the ai2 reasoning challenge\\u201d. In: *arXiv preprint arXiv:1803.05457* (2018).\\n- [9] Karl Cobbe et al. *Training Verifiers to Solve Math Word Problems*. 2021. arXiv: 2110.14168 [cs.LG]. URL: https://arxiv.org/abs/2110.14168.\\n- [10] DeepSeek-AI. *DeepSeek-V3 Technical Report*. 2024. arXiv: 2412.19437 [cs.CL]. URL: https://arxiv.org/abs/2412.19437.\\n- [11] Mostafa Dehghani et al. \\u201cScaling vision transformers to 22 billion parameters\\u201d. In: *International conference on machine learning*. PMLR. 2023, pp. 7480\\u20137512.\\n- [12] Guanting Dong et al. *Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models*. 2024. arXiv: 2406.13542 [cs.CL]. URL: https://arxiv.org/abs/2406.13542.\\n- [13] Xinrun Du et al. \\u201cSupergpqa: Scaling llm evaluation across 285 graduate disciplines\\u201d. In: *arXiv preprint arXiv:2502.14739* (2025).\\n- [14] Dheeru Dua et al. \\u201cDROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\\u201d. In: *CoRR* abs/1903.00161 (2019). arXiv: 1903.00161. URL: http://arxiv.org/abs/1903.00161.\\n- [15] Kazuki Fujii et al. *Rewriting Pre-Training Data Boosts LLM Performance in Math and Code*. 2025. arXiv: 2505.02881 [cs.LG]. URL: https://arxiv.org/abs/2505.02881.\\n- [16] Paul Gauthier. *Aider LLM Leaderboards*. https://aider.chat/docs/leaderboards/. 2025.\\n- [17] Aryo Pradipta Gema et al. \\u201cAre we done with mmlu?\\u201d In: *arXiv preprint arXiv:2406.04127* (2024).\\n- [18] Alex Gu et al. \\u201cCruxeval: A benchmark for code reasoning, understanding and execution\\u201d. In: *arXiv preprint arXiv:2401.03065* (2024).\\n- [19] Daya Guo et al. \\u201cDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning\\u201d. In: *arXiv preprint arXiv:2501.12948* (2025).\\n- [20] Zhicheng Guo et al. \\u201cStableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models\\u201d. In: *arXiv preprint arXiv:2403.07714* (2025).\\n- [21] Aaron Harlap et al. \\u201cPipedream: Fast and efficient pipeline parallel dnn training\\u201d. In: *arXiv preprint arXiv:1806.03377* (2018).\\n- [22] Y He et al. \\u201cChinese simpleqa: A chinese factuality evaluation for large language models, 2024a\\u201d. In: *URL https://arxiv. org/abs/2411.07140* ().\\n- [23] Dan Hendrycks et al. \\u201cMeasuring massive multitask language understanding\\u201d. In: *arXiv preprint arXiv:2009.03300* (2020).\\n- [24] Dan Hendrycks et al. *Measuring Mathematical Problem Solving With the MATH Dataset*. 2021. arXiv: 2103.03874 [cs.LG]. URL: https://arxiv.org/abs/2103.03874.\\n- [25] Shengding Hu et al. \\u201cMinicpm: Unveiling the potential of small language models with scalable training strategies\\u201d. In: *arXiv preprint arXiv:2404.06395* (2024).\\n- [26] Jiaxin Huang et al. \\u201cLarge language models can self-improve\\u201d. In: *arXiv preprint arXiv:2210.11610* (2022).\\n- [27] Siming Huang et al. *OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models*. 2025. arXiv: 2411.04905 [cs.CL]. URL: https://arxiv.org/abs/2411.04905.\\n-\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2108.07732\",\n",
      "                \"https://arxiv.org/abs/2108.07732\",\n",
      "                \"https://arxiv.org/abs/2412.15204\",\n",
      "                \"https://arxiv.org/abs/2412.15204\",\n",
      "                \"https://arxiv.org/abs/2506.07982\",\n",
      "                \"https://arxiv.org/abs/2506.07982\",\n",
      "                \"https://doi.org/10.1109/TSE.2023.3267446\",\n",
      "                \"https://doi.org/10.1109/TSE.2023.3267446\",\n",
      "                \"https://arxiv.org/abs/2107.03374\",\n",
      "                \"https://arxiv.org/abs/2107.03374\",\n",
      "                \"https://arxiv.org/abs/2110.14168\",\n",
      "                \"https://arxiv.org/abs/2110.14168\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2406.13542\",\n",
      "                \"https://arxiv.org/abs/2406.13542\",\n",
      "                \"https://arxiv.org/abs/1903.00161\",\n",
      "                \"http://arxiv.org/abs/1903.00161\",\n",
      "                \"https://arxiv.org/abs/2505.02881\",\n",
      "                \"https://arxiv.org/abs/2505.02881\",\n",
      "                \"https://aider.chat/docs/leaderboards/\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2411.04905\",\n",
      "                \"https://arxiv.org/abs/2411.04905\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 21,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\n[28] Yanping Huang et al. \\\"Gpipe: Efficient training of giant neural networks using pipeline parallelism\\\". In: Advances in neural information processing systems 32 (2019).\\n[29] Yuzhen Huang et al. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. 2023. arXiv: 2305.08322 [cs.CL]. URL: https://arxiv.org/abs/2305.08322.\\n[30] Alon Jacovi et al. The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input. 2025. arXiv: 2501.03200 [cs.CL]. URL: https://arxiv.org/abs/2501.03200.\\n[31] Naman Jain et al. \\\"Livecodebench: Holistic and contamination free evaluation of large language models for code\\\". In: arXiv preprint arXiv:2403.07974 (2024).\\n[32] Carlos E Jimenez et al. \\\"SWE-bench: Can Language Models Resolve Real-world Github Issues?\\\" In: The Twelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=VTF8yNQM66.\\n[33] Keller Jordan et al. Muon: An optimizer for hidden layers in neural networks. 2024. URL: https://kellerjordan.github.io/posts/muon/.\\n[34] Mandar Joshi et al. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. 2017. arXiv: 1705.03551 [cs.CL]. URL: https://arxiv.org/abs/1705.03551.\\n[35] Kimi Team. \\\"Kimi k1. 5: Scaling reinforcement learning with llms\\\". In: arXiv preprint arXiv:2501.12599 (2025).\\n[36] Diederik P. Kingma and Jimmy Ba. \\u201cAdam: A Method for Stochastic Optimization\\u201d. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Ed. by Yoshua Bengio and Yann LeCun. 2015. URL: http://arxiv.org/abs/1412.6980.\\n[37] Satyapriya Krishna et al. Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation. 2025. arXiv: 2409.12941 [cs.CL]. URL: https://arxiv.org/abs/2409.12941.\\n[38] Joel Lamy-Poirier. \\u201cBreadth-first pipeline parallelism\\u201d. In: Proceedings of Machine Learning and Systems 5 (2023), pp. 48\\u201367.\\n[39] Dmitry Lepikhin et al. \\\"Gshard: Scaling giant models with conditional computation and automatic sharding\\\". In: arXiv preprint arXiv:2006.16668 (2020).\\n[40] Haonan Li et al. CMMLU: Measuring massive multitask language understanding in Chinese. 2024. arXiv: 2306.09212 [cs.CL]. URL: https://arxiv.org/abs/2306.09212.\\n[41] Jia Li et al. \\\"Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions\\\". In: Hugging Face repository 13.9 (2024), p. 9.\\n[42] Tianle Li et al. \\\"From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline\\\". In: arXiv preprint arXiv:2406.11939 (2024).\\n[43] Bill Yuchen Lin et al. ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning. 2025. arXiv: 2502.01100 [cs.AI]. URL: https://arxiv.org/abs/2502.01100.\\n[44] Aixin Liu et al. \\\"Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model\\\". In: arXiv preprint arXiv:2405.04434 (2024).\\n[45] Jiawei Liu et al. \\\"Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation\\\". In: Advances in Neural Information Processing Systems 36 (2023), pp. 21558-21572.\\n[46] Jingyuan Liu et al. \\\"Muon is scalable for LLM training\\\". In: arXiv preprint arXiv:2502.16982 (2025).\\n[47] Ziming Liu et al. \\\"Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced Large Model Training Efficiency\\\". In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC '23. ACM, Nov. 2023, pp. 1-13. DOI: 10.1145/3581784.3607073. URL: http://dx.doi.org/10.1145/3581784.3607073.\\n[48] Ilya Loshchilov and Frank Hutter. \\\"Decoupled Weight Decay Regularization\\\". In: International Conference on Learning Representations. 2019. URL: https://openreview.net/forum?id=Bkg6RiCqY7.\\n[49] Jan Ludziejewski et al. OpenAI Gym. 2025. arXiv: 2502.05172 [cs.LG]. URL: https://arxiv.org/abs/2502.05172.\\n[50] Samuel Miserendino et al. \\\"SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?\\\" In: arXiv preprint arXiv:2502.12115 (2025).\\n[51] Arindam Mitra et al. \\\"Agentinstruct: Toward generative teaching with agentic flows\\\". In: arXiv preprint arXiv:2407.03502 (2024).\\n[52] Ivan Moshkov et al. \\\"Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset\\\". In: arXiv preprint arXiv:2504.16891 (2025).\\n[53] Deepak Narayanan et al. \\\"Efficient large-scale language model training ongpu clusters using megatron-lm\\\". In: Proceedings of the international conference for high performance computing, networking, storage and analysis. 2021, pp. 1-15.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2305.08322\",\n",
      "                \"https://arxiv.org/abs/2305.08322\",\n",
      "                \"https://arxiv.org/abs/2501.03200\",\n",
      "                \"https://arxiv.org/abs/2501.03200\",\n",
      "                \"https://openreview.net/forum?id=VTF8yNQM66\",\n",
      "                \"https://openreview.net/forum?id=VTF8yNQM66\",\n",
      "                \"https://kellerjordan.github.io/posts/muon/\",\n",
      "                \"https://kellerjordan.github.io/posts/muon/\",\n",
      "                \"https://arxiv.org/abs/1705.03551\",\n",
      "                \"https://arxiv.org/abs/1705.03551\",\n",
      "                \"http://arxiv.org/abs/1412.6980\",\n",
      "                \"https://arxiv.org/abs/2409.12941\",\n",
      "                \"https://arxiv.org/abs/2409.12941\",\n",
      "                \"https://arxiv.org/abs/2306.09212\",\n",
      "                \"https://arxiv.org/abs/2306.09212\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://doi.org/10.1145/3581784.3607073\",\n",
      "                \"http://dx.doi.org/10.1145/3581784.3607073\",\n",
      "                \"http://dx.doi.org/10.1145/3581784.3607073\",\n",
      "                \"https://openreview.net/forum?id=Bkg6RiCqY7\",\n",
      "                \"https://arxiv.org/abs/2502.05172\",\n",
      "                \"https://arxiv.org/abs/2502.05172\",\n",
      "                \"https://arxiv.org/abs/2502.05172\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 22,\n",
      "            \"markdown\": \"[54] Long Ouyang et al. \\u201cTraining language models to follow instructions with human feedback\\u201d. In: Advances in neural information processing systems 35 (2022), pp. 27730\\u201327744.\\n- [55] Bowen Peng et al. \\u201cYarn: Efficient context window extension of large language models\\u201d. In: arXiv preprint arXiv:2309.00071 (2023).\\n- [56] Long Phan et al. Humanity\\u2019s Last Exam. 2025. arXiv: 2501.14249 [cs.LG]. URL: https://arxiv.org/abs/2501.14249.\\n- [57] Penghui Qi et al. \\u201cZero bubble pipeline parallelism\\u201d. In: arXiv preprint arXiv:2401.10241 (2023).\\n- [58] Yujia Qin et al. \\u201cToolllm: Facilitating large language models to master 16000+ real-world apis\\u201d. In: arXiv preprint arXiv:2307.16789 (2023).\\n- [59] Qwen et al. Qwen2.5 Technical Report. 2025. arXiv: 2412.15115 [cs.CL]. URL: https://arxiv.org/abs/2412.15115.\\n- [60] Samyam Rajbhandari et al. \\u201cZero: Memory optimizations toward training trillion parameter models\\u201d. In: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE. 2020, pp. 1\\u201316.\\n- [61] David Rein et al. \\u201cGpqa: A graduate-level google-proof q&a benchmark\\u201d. In: First Conference on Language Modeling. 2024.\\n- [62] Keisuke Sakaguchi et al. \\u201cWinogrande: An adversarial winograd schema challenge at scale\\u201d. In: Communications of the ACM 64.9 (2021), pp. 99\\u2013106.\\n- [63] David Silver and Richard S Sutton. \\u201cWelcome to the era of experience\\u201d. In: Google AI 1 (2025).\\n- [64] Ved Sirdeshmukh et al. MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs. 2025. arXiv: 2501.17399 [cs.CL]. URL: https://arxiv.org/abs/2501.17399.\\n- [65] Giulio Starace et al. \\u201cPaperBench: Evaluating AI\\u2019s Ability to Replicate AI Research\\u201d. In: arXiv preprint arXiv:2504.01848 (2025).\\n- [66] Hao Sun et al. ZeroSearch: Incentivize the Search Capability of LLMs without Searching. 2025. arXiv: 2505.04588 [cs.CL]. URL: https://arxiv.org/abs/2505.04588.\\n- [67] Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. 2022. arXiv: 2210.09261 [cs.CL]. URL: https://arxiv.org/abs/2210.09261.\\n- [68] Manveer Singh Tamber et al. \\u201cBenchmarking LLM Faithfulness in RAG with Evolving Leaderboards\\u201d. In: arXiv preprint arXiv:2505.04847 (2025).\\n- [69] Gemma Team et al. \\u201cGemma 2: Improving open language models at a practical size\\u201d. In: arXiv preprint arXiv:2408.00118 (2024).\\n- [70] LlaMA Team. The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation \\u2014 ai.meta.com. https://ai.meta.com/blog/llama-4-multimodal-intelligence/. [Accessed 15-07-2025].\\n- [71] The Terminal-Bench Team. Terminal-Bench: A Benchmark for AI Agents in Terminal Environments. Apr. 2025. URL: https://github.com/laude-institute/terminal-bench.\\n- [72] Ashish Vaswani et al. \\u201cAttention is All you Need\\u201d. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. URL: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n- [73] Vectara. Hallucination Evaluation Model (Revision 7437011). 2024. URL: https://huggingface.co/vectara/hallucination_evaluation_model.\\n- [74] Joshua Vendrow et al. \\u201cDo large language model benchmarks test reliability?\\u201d In: arXiv preprint arXiv:2502.03461 (2025).\\n- [75] Yizhong Wang et al. \\u201cSelf-instruct: Aligning language models with self-generated instructions\\u201d. In: arXiv preprint arXiv:2212.10560 (2022).\\n- [76] Yubo Wang et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. 2024. arXiv: 2406.01574 [cs.CL]. URL: https://arxiv.org/abs/2406.01574.\\n- [77] Zhexu Wang et al. OJBench: A Competition Level Code Benchmark For Large Language Models. 2025. arXiv: 2506.16395 [cs.CL]. URL: https://arxiv.org/abs/2506.16395.\\n- [78] Jason Wei et al. \\u201cMeasuring short-form factuality in large language models\\u201d. In: arXiv preprint arXiv:2411.04368 (2024).\\n- [79] Tianwen Wei et al. CMATH: Can Your Language Model Pass Chinese Elementary School Math Test? 2023. arXiv: 2306.16636 [cs.CL]. URL: https://arxiv.org/abs/2306.16636.\\n- [80] Colin White et al. \\u201cLiveBench: A Challenging, Contamination-Free LLM Benchmark\\u201d. In: The Thirteenth International Conference on Learning Representations. 2025.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2501.17399\",\n",
      "                \"https://arxiv.org/abs/2501.17399\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2210.09261\",\n",
      "                \"https://arxiv.org/abs/2210.09261\",\n",
      "                \"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\",\n",
      "                \"https://github.com/laude-institute/terminal-bench\",\n",
      "                \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\n",
      "                \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\n",
      "                \"https://huggingface.co/vectara/hallucination_evaluation_model\",\n",
      "                \"https://huggingface.co/vectara/hallucination_evaluation_model\",\n",
      "                \"https://arxiv.org/abs/2406.01574\",\n",
      "                \"https://arxiv.org/abs/2406.01574\",\n",
      "                \"https://arxiv.org/abs/2506.16395\",\n",
      "                \"https://arxiv.org/abs/2506.16395\",\n",
      "                \"https://arxiv.org/abs/2306.16636\",\n",
      "                \"https://arxiv.org/abs/2306.16636\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 23,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\n[81] Mitchell Wortsman et al. \\\"Small-scale proxies for large-scale transformer training instabilities, 2023\\\". In: URL https://arxiv.org/abs/2309.14322 ().\\n[82] Can Xu et al. WizardLM: Empowering large pre-trained language models to follow complex instructions. 2025. arXiv: 2304.12244 [cs.CL]. URL: https://arxiv.org/abs/2304.12244.\\n[83] Zhangchen Xu et al. KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding. 2025. arXiv: 2503.02951 [cs.LG]. URL: https://arxiv.org/abs/2503.02951.\\n[84] John Yang et al. SWE-smith: Scaling Data for Software Engineering Agents. 2025. arXiv: 2504.21798 [cs.SE]. URL: https://arxiv.org/abs/2504.21798.\\n[85] Shunyu Yao et al. \\\"tau-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains\\\". In: arXiv preprint arXiv:2406.12045 (2024).\\n[86] Daoguang Zan et al. \\\"Multi-swe-bench: A multilingual benchmark for issue resolving\\\". In: arXiv preprint arXiv:2504.02605 (2025).\\n[87] Eric Zelikman et al. \\\"Star: Bootstrapping reasoning with reasoning\\\". In: Advances in Neural Information Processing Systems 35 (2022), pp. 15476-15488.\\n[88] Rowan Zellers et al. \\u201cHellaswag: Can a machine really finish your sentence?\\u201d In: arXiv preprint arXiv:1905.07830 (2019).\\n[89] Wanjun Zhong et al. \\\"Agieval: A human-centric benchmark for evaluating foundation models\\\". In: arXiv preprint arXiv:2304.06364 (2023).\\n[90] Jeffrey Zhou et al. \\\"Instruction-Following Evaluation for Large Language Models\\\". In: ArXiv abs/2311.07911 (2023). URL: https://arxiv.org/abs/2311.07911.\\n[91] Qin Zhu et al. AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models. 2025. arXiv: 2502.16906 [cs.CL]. URL: https://arxiv.org/abs/2502.16906.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2304.12244\",\n",
      "                \"https://arxiv.org/abs/2304.12244\",\n",
      "                \"https://arxiv.org/abs/2503.02951\",\n",
      "                \"https://arxiv.org/abs/2503.02951\",\n",
      "                \"https://arxiv.org/abs/2504.21798\",\n",
      "                \"https://arxiv.org/abs/2504.21798\",\n",
      "                \"https://arxiv.org/abs/2311.07911\",\n",
      "                \"https://arxiv.org/abs/2502.16906\",\n",
      "                \"https://arxiv.org/abs/2502.16906\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        }\n",
      "    ],\n",
      "    \"model\": \"mistral-ocr-latest\",\n",
      "    \"usage_info\": {\n",
      "        \"pages_processed\": 8,\n",
      "        \"doc_size_bytes\": 6335018\n",
      "    },\n",
      "    \"document_annotation\": \"{\\n  \\\"summary\\\": \\\"The document presents a comprehensive evaluation of the Kimi-K2-Instruct and Kimi-K2-Base models, highlighting their superior performance across various benchmarks and tasks. Kimi-K2-Instruct excels in multi-turn tool-use benchmarks, general capabilities, and open-ended evaluations, outperforming all baselines in grounded, controlled, and agent-driven tool orchestration. It also shows strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. Kimi-K2-Base demonstrates state-of-the-art performance in general language understanding, coding capabilities, mathematical reasoning, and Chinese language understanding. The models were evaluated using diverse benchmarks and configurations, ensuring fair comparisons. Safety evaluations were conducted using red-teaming techniques, with Kimi K2 showing robust performance against various attack strategies. The document also acknowledges some limitations and areas for future improvement, such as handling hard reasoning tasks and unclear tool definitions. Overall, Kimi K2 is positioned as a leading open-weight model for agentic intelligence.\\\",\\n  \\\"authors\\\": [\\n    \\\"Kimi-K2 Team\\\"\\n  ]\\n}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Mistral client with API key\n",
    "from mistralai import Mistral\n",
    "load_dotenv()\n",
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "# OCR Call with Annotations\n",
    "annotations_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    pages=list(range(16, 24)), # Document Annotations has a limit of 8 pages, we recommend spliting your documents when using it; bbox annotations does not have the same limit\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "    },\n",
    "    bbox_annotation_format=response_format_from_pydantic_model(BBox),\n",
    "    document_annotation_format=response_format_from_pydantic_model(Document),\n",
    "    include_image_base64=True, # Let's also include the images in the response\n",
    "    table_format=\"html\"\n",
    "  )\n",
    "\n",
    "# Convert response to JSON format\n",
    "response_dict = json.loads(annotations_response.model_dump_json())\n",
    "\n",
    "print(json.dumps(response_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a27a3",
   "metadata": {},
   "source": [
    "Let's split the pdf into 8 pages batches first as they advice to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8df55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9499b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 saved to RAG/documents/chunks/chunk_0.pdf\n",
      "Chunk 1 saved to RAG/documents/chunks/chunk_1.pdf\n",
      "Chunk 2 saved to RAG/documents/chunks/chunk_2.pdf\n",
      "Chunk 3 saved to RAG/documents/chunks/chunk_3.pdf\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "def split_pdf(input_path, chunk_size=8, output_dir=\"RAG/documents/chunks\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    reader = PdfReader(input_path)\n",
    "    for i in range(0, len(reader.pages), chunk_size):\n",
    "        writer = PdfWriter()\n",
    "        for page in reader.pages[i : i + chunk_size]:\n",
    "            writer.add_page(page)\n",
    "        \n",
    "        chunk_filename = f\"chunk_{i//chunk_size}.pdf\"\n",
    "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        print(f\"Chunk {i//chunk_size} saved to {chunk_path}\")\n",
    "\n",
    "split_pdf(\"RAG/documents/KimiK2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6311d",
   "metadata": {},
   "source": [
    "Let's actually parse and annotate tables as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f9052e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: chunk_0.pdf\n",
      "Successfully processed chunk_0.pdf\n",
      "Processing: chunk_1.pdf\n",
      "Successfully processed chunk_1.pdf\n",
      "Processing: chunk_2.pdf\n",
      "Successfully processed chunk_2.pdf\n",
      "Processing: chunk_3.pdf\n",
      "Successfully processed chunk_3.pdf\n"
     ]
    }
   ],
   "source": [
    "chunk_dir = \"RAG/documents/chunks\"\n",
    "responses = []\n",
    "\n",
    "# Sort the list to ensure pages stay in order\n",
    "for chunk_filename in sorted(os.listdir(chunk_dir)):\n",
    "    # Construct the full path\n",
    "    chunk_path = os.path.join(chunk_dir, chunk_filename)\n",
    "    \n",
    "    # Skip directories or non-pdf files if any exist\n",
    "    if not chunk_filename.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    with open(chunk_path, \"rb\") as f:\n",
    "        # Correctly encode the specific chunk\n",
    "        base64_chunk = base64.b64encode(f.read()).decode('utf-8')\n",
    "        print(f\"Processing: {chunk_filename}\")\n",
    "\n",
    "    try:\n",
    "        # OCR Call\n",
    "        annotations_response = client.ocr.process(\n",
    "            model=\"mistral-ocr-latest\",\n",
    "            # Remove the 'pages' limit because the file IS the limit now\n",
    "            document={\n",
    "                \"type\": \"document_url\",\n",
    "                \"document_url\": f\"data:application/pdf;base64,{base64_chunk}\"\n",
    "            },\n",
    "            bbox_annotation_format=response_format_from_pydantic_model(Image),\n",
    "            document_annotation_format=response_format_from_pydantic_model(Document),\n",
    "            include_image_base64=True,\n",
    "            table_format=\"html\"  # take out tables as well\n",
    "        )\n",
    "        \n",
    "        response_dict = annotations_response.model_dump()\n",
    "        # Parse nested JSON strings in document_annotation\n",
    "        if isinstance(response_dict.get(\"document_annotation\"), str):\n",
    "            response_dict[\"document_annotation\"] = json.loads(response_dict[\"document_annotation\"])\n",
    "        # Parse nested JSON strings in image annotations\n",
    "        for page in response_dict.get(\"pages\", []):\n",
    "            for img in page.get(\"images\", []):\n",
    "                if isinstance(img.get(\"image_annotation\"), str):\n",
    "                    img[\"image_annotation\"] = json.loads(img[\"image_annotation\"])\n",
    "\n",
    "        responses.append(response_dict)\n",
    "        print(f\"Successfully processed {chunk_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {chunk_filename}: {e}\")\n",
    "\n",
    "# Save the responses\n",
    "output_path = \"RAG/OCR/responses.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True) # Ensure directory exists\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0251595",
   "metadata": {},
   "source": [
    "Now since we split the document into several parts, our images' and tables' indexes will start over at each chunk, and that will give us repeated indices - we do not want that. So we re-index with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6696d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def reindex_ocr_responses(responses_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Re-indexes images and tables across all OCR responses to have globally unique IDs.\n",
    "    Updates both the ID fields in objects and all markdown references.\n",
    "    \"\"\"\n",
    "    global_image_counter = 0\n",
    "    global_table_counter = 0\n",
    "    \n",
    "    for response in responses_list:\n",
    "        for page in response.get(\"pages\", []):\n",
    "            # Create mapping of old IDs to new IDs for this page\n",
    "            image_id_map = {}\n",
    "            table_id_map = {}\n",
    "            \n",
    "            # Re-index images\n",
    "            for img in page.get(\"images\", []):\n",
    "                old_id = img[\"id\"]\n",
    "                # Get file extension\n",
    "                ext = old_id.split('.')[-1] if '.' in old_id else 'jpeg'\n",
    "                new_id = f\"img-{global_image_counter}.{ext}\"\n",
    "                \n",
    "                # Update the image ID in the object\n",
    "                img[\"id\"] = new_id\n",
    "                image_id_map[old_id] = new_id\n",
    "                global_image_counter += 1\n",
    "            \n",
    "            # Re-index tables\n",
    "            for table in page.get(\"tables\", []):\n",
    "                old_id = table[\"id\"]\n",
    "                # Get file extension\n",
    "                ext = old_id.split('.')[-1] if '.' in old_id else 'html'\n",
    "                new_id = f\"tbl-{global_table_counter}.{ext}\"\n",
    "                \n",
    "                # Update the table ID in the object\n",
    "                table[\"id\"] = new_id\n",
    "                table_id_map[old_id] = new_id\n",
    "                global_table_counter += 1\n",
    "            \n",
    "            # Update markdown to reflect new IDs\n",
    "            markdown = page.get(\"markdown\", \"\")\n",
    "            \n",
    "            # Replace image references: ![img-0.jpeg](img-0.jpeg) format\n",
    "            for old_id, new_id in image_id_map.items():\n",
    "                markdown = markdown.replace(f\"![{old_id}]({old_id})\", f\"![{new_id}]({new_id})\")\n",
    "            \n",
    "            # Replace table references: [tbl-0.html](tbl-0.html) format\n",
    "            for old_id, new_id in table_id_map.items():\n",
    "                markdown = markdown.replace(f\"[{old_id}]({old_id})\", f\"[{new_id}]({new_id})\")\n",
    "            \n",
    "            page[\"markdown\"] = markdown\n",
    "    \n",
    "    print(f\"Re-indexed {global_image_counter} images and {global_table_counter} tables\")\n",
    "    return responses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b418611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-indexed 18 images and 7 tables\n",
      "Saved re-indexed responses to RAG/OCR/responses_reindexed.json\n"
     ]
    }
   ],
   "source": [
    "# Load the original JSON\n",
    "input_file = \"RAG/OCR/responses.json\"\n",
    "output_file = \"RAG/OCR/responses_reindexed.json\"  # or use the same file to overwrite\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    responses_list = json.load(f)\n",
    "\n",
    "# Re-index\n",
    "responses_list = reindex_ocr_responses(responses_list)\n",
    "\n",
    "# Save the re-indexed version\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(responses_list, f, indent=4)\n",
    "\n",
    "print(f\"Saved re-indexed responses to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d4c7a",
   "metadata": {},
   "source": [
    "Let's check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4900c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def replace_images_in_markdown_annotated(\n",
    "    markdown_str: str,\n",
    "    images_dict: dict,\n",
    "    tables_dict: dict = None,\n",
    "    include_images: bool = True,\n",
    "    include_tables: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Replaces images and tables in the markdown string with their content/descriptions.\n",
    "\n",
    "    Args:\n",
    "        markdown_str: The markdown string to replace images in.\n",
    "        images_dict: A dictionary of images to replace, with their names as keys and data as values.\n",
    "        tables_dict: A dictionary of tables to replace, with their names as keys and data as values.\n",
    "        include_images: Whether to include images base64 data in the output.\n",
    "        include_tables: Whether to include table HTML content in the output.\n",
    "\n",
    "    Returns:\n",
    "        The markdown string with images and tables replaced.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace images: ![img-0.jpeg](img-0.jpeg) format\n",
    "    for img_name, data in images_dict.items():\n",
    "        placeholder = f\"![{img_name}]({img_name})\"\n",
    "        \n",
    "        # Get annotation description\n",
    "        annotation = data.get('annotation', {})      \n",
    "        description = annotation.get('description', '')\n",
    "        \n",
    "        if include_images:\n",
    "            replacement = f\"![{img_name}]({data['image']})\\n\\n**{description}**\"\n",
    "        else:\n",
    "            replacement = f\"**Figure: {img_name}**\\n\\n**{description}**\"\n",
    "        \n",
    "        markdown_str = markdown_str.replace(placeholder, replacement)\n",
    "    \n",
    "    # Replace tables: [tbl-0.html](tbl-0.html) format (no exclamation mark!)\n",
    "    if tables_dict:\n",
    "        for tbl_name, data in tables_dict.items():\n",
    "            placeholder = f\"[{tbl_name}]({tbl_name})\"\n",
    "            \n",
    "            if include_tables:\n",
    "                # Insert the actual HTML table content\n",
    "                replacement = f\"\\n\\n{data['content']}\\n\\n\"\n",
    "            else:\n",
    "                replacement = f\"**Table: {tbl_name}**\"\n",
    "            \n",
    "            markdown_str = markdown_str.replace(placeholder, replacement)\n",
    "    \n",
    "    return markdown_str\n",
    "\n",
    "def process_saved_ocr_json(json_path: str, range: list[int] = None, include_images: bool = True, include_tables: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Reads the saved JSON list of responses and merges them into one Markdown string.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing OCR responses\n",
    "        range: Optional range on the number of responses to process\n",
    "        include_images: Whether to include images in the output\n",
    "        include_tables: Whether to include tables in the output\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Combined Markdown string with all OCR responses, Document-level Annotation/Summary)\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        responses_list = json.load(f)\n",
    "\n",
    "    full_markdown_parts = []\n",
    "\n",
    "    # Handle None range - process all responses\n",
    "    responses_to_process = responses_list[range[0]:range[1]] if range is not None else responses_list\n",
    "\n",
    "    for resp in responses_to_process:\n",
    "        # 1. Extract the Document-level Annotation/Summary (return separately)\n",
    "        doc_anno = resp.get(\"document_annotation\", \"\")\n",
    "\n",
    "        # 2. Iterate through pages in this chunk\n",
    "        for page in resp.get(\"pages\", []):\n",
    "            image_data = {}\n",
    "            # Extract image data for replacement\n",
    "            for img in page.get(\"images\", []):\n",
    "                image_data[img[\"id\"]] = {\n",
    "                    \"image\": img.get(\"image_base64\", \"\"), \n",
    "                    \"annotation\": img.get(\"image_annotation\", {})\n",
    "                }\n",
    "            \n",
    "            table_data = {}\n",
    "            for tbl in page.get(\"tables\", []):\n",
    "                table_data[tbl[\"id\"]] = {\n",
    "                    \"content\": tbl.get(\"content\", \"\"),\n",
    "                }\n",
    "            \n",
    "            # 3. Process the markdown for this specific page\n",
    "            page_md = page.get(\"markdown\", \"\")\n",
    "            processed_page = replace_images_in_markdown_annotated(\n",
    "                page_md, image_data, table_data, include_images, include_tables\n",
    "            )\n",
    "            full_markdown_parts.append(processed_page)\n",
    "\n",
    "    return \"\\n\".join(full_markdown_parts), doc_anno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d18528ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "K KIMI K2: OPEN AGENTIC INTELLIGENCE\n",
       "\n",
       "# TECHNICAL REPORT OF KIMI K2\n",
       "\n",
       "# Kimi Team\n",
       "\n",
       "# ABSTRACT\n",
       "\n",
       "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.\n",
       "\n",
       "Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual  surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints $^{1}$  to facilitate future research and applications of agentic intelligence.\n",
       "\n",
       "**Figure: img-0.jpeg**\n",
       "\n",
       "**This image shows a series of bar charts comparing the performance of various AI models across different benchmarks. The benchmarks are categorized into 'Agentic and Competitive Coding', 'Tool Use', and 'Math & STEM'. Each bar chart represents a specific benchmark, such as SWE-bench Verified, SWE-bench Multilingual, LiveCodeBench v6, OJBench, Tau2-bench micro-average, AceBench (en), AIME 2025, and GPQA-Diamond. The AI models compared include Kimi-K2-Instruct, DeepSeekV3-0324, Owen3-235B-A22B, OpenAI GPT-4.1, Claude 4 Opus, Claude 4 Sonnet, and Gemini 2.5 Flash non-thinking. Each bar chart shows the performance scores of these models, with Kimi-K2-Instruct generally performing the best across most benchmarks.**\n",
       "Figure 1: Kimi K2 main results.2\n",
       "1 Introduction\n",
       "\n",
       "The development of Large Language Models (LLMs) is undergoing a profound paradigm shift towards *Agentic Intelligence*  the capabilities for models to autonomously perceive, plan, reason, and act within complex and dynamic environments. This transition marks a departure from static imitation learning towards models that actively learn through interactions, acquire new skills beyond their training distribution, and adapt behavior through experiences *[63]*. It is believed that this approach allows an AI agent to go beyond the limitation of static human-generated data, and acquire superhuman capabilities through its own exploration and exploitation. Agentic intelligence is thus rapidly emerging as a defining capability for the next generation of foundation models, with wide-ranging implications across tool use, software development, and real-world autonomy.\n",
       "\n",
       "Achieving agentic intelligence introduces challenges in both pre-training and post-training. Pre-training must endow models with broad general-purpose priors under constraints of limited high-quality data, elevating token efficiencylearning signal per tokenas a critical scaling coefficient. Post-training must transform those priors into actionable behaviors, yet agentic capabilities such as multi-step reasoning, long-term planning, and tool use are rare in natural data and costly to scale. Scalable synthesis of structured, high-quality agentic trajectories, combined with general reinforcement learning (RL) techniques that incorporate preferences and self-critique, are essential to bridge this gap.\n",
       "\n",
       "In this work, we introduce Kimi K2, a 1.04 trillion-parameter Mixture-of-Experts (MoE) LLM with 32 billion activated parameters, purposefully designed to address the core challenges and push the boundaries of agentic capability. Our contributions span both the pre-training and post-training frontiers:\n",
       "\n",
       "- We present MuonClip, a novel optimizer that integrates the token-efficient Muon algorithm with a stability-enhancing mechanism called QK-Clip. Using MuonClip, we successfully pre-trained Kimi K2 on 15.5 trillion tokens without a single loss spike.\n",
       "- We introduce a large-scale agentic data synthesis pipeline that systematically generates tool-use demonstrations via simulated and real-world environments. This system constructs diverse tools, agents, tasks, and trajectories to create high-fidelity, verifiably correct agentic interactions at scale.\n",
       "- We design a general reinforcement learning framework that combines verifiable rewards (RLVR) with a self-critique rubric reward mechanism. The model learns not only from externally defined tasks but also from evaluating its own outputs, extending alignment from static into open-ended domains.\n",
       "\n",
       "Kimi K2 demonstrates strong performance across a broad spectrum of agentic and frontier benchmarks. It achieves scores of 66.1 on Tau2-bench, 76.5 on ACEBench (en), 65.8 on SWE-bench Verified, and 47.3 on SWE-bench Multilingual, outperforming most open- and closed-weight baselines under non-thinking evaluation settings, closing the gap with Claude 4 Opus and Sonnet. In coding, mathematics, and broader STEM domains, Kimi K2 achieves 53.7 on LiveCodeBench v6, 27.1 on OJBench, 49.5 on AIME 2025, and 75.1 on GPQA-Diamond, further highlighting its capabilities in general tasks. On the LMSYS Arena leaderboard (July 17, 2025), Kimi K2 ranks as the top 1 open-source model and 5th overall based on over 3,000 user votes.\n",
       "\n",
       "To spur further progress in Agentic Intelligence, we are open-sourcing our base and post-trained checkpoints, enabling the community to explore, refine, and deploy agentic intelligence at scale.\n",
       "\n",
       "## 2 Pre-training\n",
       "\n",
       "The base model of Kimi K2 is a trillion-parameter mixture-of-experts (MoE) transformer *[72]* model, pre-trained on 15.5 trillion high-quality tokens. Given the increasingly limited availability of high-quality human data, we posit that token efficiency is emerging as a critical coefficient in the scaling of large language models. To address this, we introduce a suite of pre-training techniques explicitly designed for maximizing token efficiency. Specifically, we employ the token-efficient Muon optimizer *[33, 46]* and mitigate its training instabilities through the introduction of QK-Clip. Additionally, we incorporate synthetic data generation to further squeeze the intelligence out of available high-quality tokens. The model architecture follows an ultra-sparse MoE with multi-head latent attention (MLA) similar to DeepSeek-V3 *[10]* , derived from empirical scaling law analysis. The underlying infrastructure is built to optimize both training efficiency and research efficiency.\n",
       "2.1 MuonClip: Stable Training with Weight Clipping\n",
       "\n",
       "We train Kimi K2 using the token-efficient Muon optimizer *[33]*, incorporating weight decay and consistent update RMS scaling *[46]*. Experiments in our previous work Moonlight *[46]* show that, under the same compute budget and model size  and therefore the same amount of training data  Muon substantially outperforms AdamW *[36, 48]*, making it an effective choice for improving token efficiency in large language model training.\n",
       "\n",
       "#### Training instability when scaling Muon\n",
       "\n",
       "Despite its efficiency, scaling up Muon training reveals a challenge: training instability due to exploding attention logits, an issue that occurs more frequently with Muon but less with AdamW in our experiments. Existing mitigation strategies are insufficient. For instance, logit soft-cap *[69]* directly clips the attention logits, but the dot products between queries and keys can still grow excessively before capping is applied. On the other hand, Query-Key Normalization (QK-Norm) *[11, 81]* is not applicable to multi-head latent attention (MLA), because its Key matrices are not fully materialized during inference.\n",
       "\n",
       "#### Taming Muon with QK-Clip\n",
       "\n",
       "To address this issue, we propose a novel weight-clipping mechanism *QK-Clip* to explicitly constrain attention logits. QK-Clip works by rescaling the query and key projection weights post-update to bound the growth of attention logits.\n",
       "\n",
       "Let the input representation of a transformer layer be $\\mathbf{X}$. For each attention head $h$, its query, key, and value projections are computed as\n",
       "\n",
       "$\\mathbf{Q}^{h}=\\mathbf{X}\\mathbf{W}_{q}^{h},\\quad\\mathbf{K}^{h}=\\mathbf{X}\\mathbf{W}_{k}^{h},\\quad\\mathbf{V}^{h}=\\mathbf{X}\\mathbf{W}_{v}^{h}.$\n",
       "\n",
       "where $\\mathbf{W}_{q},\\mathbf{W}_{k},\\mathbf{W}_{v}$ are model parameters. The attention output is:\n",
       "\n",
       "$\\mathbf{O}^{h}=\\operatorname{softmax}\\left(\\frac{1}{\\sqrt{d}}\\mathbf{Q}^{h}\\mathbf{K}^{h\\top}\\right)\\mathbf{V}^{h}.$\n",
       "\n",
       "We define the max logit, a per-head scalar, as the maximum input to softmax in this batch $B$:\n",
       "\n",
       "$S_{\\max}^{h}=\\frac{1}{\\sqrt{d}}\\max_{\\mathbf{X}\\in B}\\max_{i,j}\\mathbf{Q}_{i}^{h}\\mathbf{K}_{j}^{h\\top}$\n",
       "\n",
       "where $i,j$ are indices of different tokens in a training sample $\\mathbf{X}$.\n",
       "\n",
       "The core idea of QK-Clip is to rescale $\\mathbf{W}_{k},\\mathbf{W}_{q}$ whenever $S_{\\max}^{h}$ exceeds a target threshold $\\tau$. Importantly, this operation does not alter the forward/backward computation in the current step  we merely use the max logit as a guiding signal to determine the strength to control the weight growth.\n",
       "\n",
       "A nave implementation clips all heads at the same time:\n",
       "\n",
       "$\\mathbf{W}_{q}^{h}\\leftarrow\\gamma^{\\alpha}\\mathbf{W}_{q}^{h}\\qquad\\mathbf{W}_{k}^{h}\\leftarrow\\gamma^{1-\\alpha}\\mathbf{W}_{k}^{h}$\n",
       "\n",
       "where $\\gamma=\\min(1,\\tau/S_{\\max})$ with $S_{\\max}=\\max_{h}S_{\\max}^{h}$, and $\\alpha$ is a balancing parameter typically set to $0.5$, applying equal scaling to queries and keys.\n",
       "\n",
       "However, we observe that in practice, only a small subset of heads exhibit exploding logits. In order to minimize our intervention on model training, we determine a per-head scaling factor $\\gamma_{h}=\\min(1,\\tau/S_{\\max}^{h})$, and opt to apply per-head QK-Clip. Such clipping is straightforward for regular multi-head attention (MHA). For MLA, we apply clipping only on unshared attention head components:\n",
       "\n",
       "- $\\mathbf{q}^{C}$ and $\\mathbf{k}^{C}$ (head-specific components): each scaled by $\\sqrt{\\gamma_{h}}$\n",
       "- $\\mathbf{q}^{R}$ (head-specific rotary): scaled by $\\gamma_{h}$,\n",
       "- $\\mathbf{k}^{R}$ (shared rotary): left untouched to avoid effect across heads.\n",
       "\n",
       "#### MuonClip: The New Optimizer\n",
       "\n",
       "We integrate Muon with weight decay, consistent RMS matching, and QK-Clip into a single optimizer, which we refer to as MuonClip (see Algorithm 1).\n",
       "\n",
       "We demonstrate the effectiveness of MuonClip from several scaling experiments. First, we train a mid-scale 9B activated and 53B total parameters Mixture-of-Experts (MoE) model using the vanilla Muon. As shown in Figure 2 (Left), we observe that the maximum attention logits quickly exceed a magnitude of 1000, showing that attention logits explosion is already evident in Muon training to this scale. Max logits at this level usually result in instability during training, including significant loss spikes and occasional divergence.\n",
       "Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "Algorithm 1 MuonClip Optimizer\n",
       "1: for each training step  $t$  do\n",
       "2: // 1. Muon optimizer step\n",
       "3: for each weight  $\\mathbf{W} \\in \\mathbb{R}^{n \\times m}$  do\n",
       "4:  $\\mathbf{M}_t = \\mu \\mathbf{M}_{t-1} + \\mathbf{G}_t$ $\\triangleright \\mathbf{M}_0 = \\mathbf{0}, \\mathbf{G}_t$  is the grad of  $\\mathbf{W}_t, \\mu$  is momentum\n",
       "5:  $\\mathbf{O}_t = \\text{Newton-Schulz}(\\mathbf{M}_t) \\cdot \\sqrt{\\max(n, m)} \\cdot 0.2$ $\\triangleright \\text{Match Adam RMS}$\n",
       "6:  $\\mathbf{W}_t = \\mathbf{W}_{t-1} - \\eta(\\mathbf{O}_t + \\lambda \\mathbf{W}_{t-1})$ $\\triangleright \\text{learning rate } \\eta$ , weight decay  $\\lambda$\n",
       "7: end for\n",
       "8: // 2. QK-Clip\n",
       "9: for each attention head  $h$  in every attention layer of the model do\n",
       "10: Obtain  $S_{\\max}^h$  already computed during forward\n",
       "11: if  $S_{\\max}^h &gt; \\tau$  then\n",
       "12:  $\\gamma \\gets \\tau / S_{\\max}^h$\n",
       "13:  $\\mathbf{W}_{qc}^h \\gets \\mathbf{W}_{qc}^h \\cdot \\sqrt{\\gamma}$\n",
       "14:  $\\mathbf{W}_{kc}^h \\gets \\mathbf{W}_{kc}^h \\cdot \\sqrt{\\gamma}$\n",
       "15:  $\\mathbf{W}_{qr}^h \\gets \\mathbf{W}_{qr}^h \\cdot \\gamma$\n",
       "16: end if\n",
       "17: end for\n",
       "18: end for\n",
       "\n",
       "**Figure: img-1.jpeg**\n",
       "\n",
       "**This graph shows the relationship between training steps and max logits in a machine learning model run with Muon. The x-axis represents the number of training steps, ranging from 0 to 17500, while the y-axis represents the max logits, ranging from 0 to 1200. The data series, labeled 'Vanilla run with Muon,' shows a trend where the max logits increase gradually at first and then more rapidly as the number of training steps increases, indicating an exponential growth pattern.**\n",
       "Figure 2: Left: During a mid-scale training run, attention logits rapidly exceed 1000, which could lead to potential numerical instabilities and even training divergence. Right: Maximum logits for Kimi K2 with MuonClip and  $\\tau = 100$  over the entire training run. The max logits rapidly increase to the capped value of 100, and only decay to a stable range after approximately  $30\\%$  of the training steps, demonstrating the effective regulation effect of QK-Clip.\n",
       "\n",
       "**Figure: img-2.jpeg**\n",
       "\n",
       "**This graph shows the performance of the Kimi K2 model with MuonClip over training steps. The x-axis represents the number of training steps, ranging from 0 to 200,000. The y-axis represents the max logits, ranging from 0 to 100. The graph shows a sharp increase in max logits initially, reaching a peak and then experiencing a significant drop around the 50,000 training steps mark. After the drop, the max logits stabilize and fluctuate at a lower level for the remainder of the training steps.**\n",
       "\n",
       "Next, we demonstrate that QK-Clip does not degrade model performance and confirm that the MuonClip optimizer preserves the optimization characteristics of Muon without adversely affecting the loss trajectory. A detailed discussion of the experiment designs and findings is provided in the Appendix D.\n",
       "\n",
       "Finally, we train Kimi K2, a large-scale MoE model, using MuonClip with  $\\tau = 100$  and monitor the maximum attention logits throughout the training run (Figure 2 (Right)). Initially, the logits are capped at 100 due to QK-Clip. Over the course of training, the maximum logits gradually decay to a typical operating range without requiring any adjustment to  $\\tau$ . Importantly, the training loss remains smooth and stable, with no observable spikes, as shown in Figure 3, validating that MuonClip provides robust and scalable control over attention dynamics in large-scale language model training.\n",
       "\n",
       "# 2.2 Pre-training Data: Improving Token Utility with Rephrasing\n",
       "\n",
       "Token efficiency in pre-training refers to how much performance improvement is achieved for each token consumed during training. Increasing token utilitythe effective learning signal each token contributesenhances the per-token impact on model updates, thereby directly improving token efficiency. This is particularly important when the supply of high-quality tokens is limited and must be maximally leveraged. A naive approach to increasing token utility is through repeated exposure to the same tokens, which can lead to overfitting and reduced generalization.\n",
       "Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "**Figure: img-3.jpeg**\n",
       "\n",
       "**This graph shows the loss over the number of tokens (in trillions) processed. The y-axis represents the loss, while the x-axis represents the number of tokens. The graph indicates a decreasing trend in loss as the number of tokens increases, with some fluctuations.**\n",
       "Figure 3: Per-step training loss curve of Kimi K2, without smoothing or sub-sampling. It shows no spikes throughout the entire training process. Note that we omit the very beginning of training for clarity.\n",
       "\n",
       "A key advancement in the pre-training data of Kimi K2 over Kimi K1.5 is the introduction of a synthetic data generation strategy to increase token utility. Specifically, a carefully designed rephrasing pipeline is employed to amplify the volume of high-quality tokens without inducing significant overfitting. In this report, we describe two domain-specialized rephrasing techniquestargeted respectively at the Knowledge and Mathematics domainsthat enable this controlled data augmentation.\n",
       "\n",
       "Knowledge Data Rephrasing Pre-training on natural, knowledge-intensive text presents a trade-off: a single epoch is insufficient for comprehensive knowledge absorption, while multi-epoch repetition yields diminishing returns and increases the risk of overfitting. To improve the token utility of high-quality knowledge tokens, we propose a synthetic rephrasing framework composed of the following key components:\n",
       "\n",
       "- Style- and perspective-diverse prompting: To enhance linguistic diversity while maintaining factual integrity, we apply a range of carefully engineered prompts. These prompts guide a large language model to generate faithful rephrasings of the original texts in varied styles and from different perspectives.\n",
       "- Chunk-wise autoregressive generation: To preserve global coherence and avoid information loss in long documents, we adopt a chunk-based autoregressive rewriting strategy. Texts are divided into segments, rephrased individually, and then stitched back together to form complete passages. This method mitigates implicit output length limitations that typically exist with LLMs. An overview of this pipeline is presented in Figure 4.\n",
       "- Fidelity verification: To ensure consistency between original and rewritten content, we perform fidelity checks that compare the semantic alignment of each rephrased passage with its source. This serves as an initial quality control step prior to training.\n",
       "\n",
       "We compare data rephrasing with multi-epoch repetition by testing their corresponding accuracy on SimpleQA. We experiment with an early checkpoint of K2 and evaluate three training strategies: (1) repeating the original dataset for 10 epochs, (2) rephrasing the data once and repeating it for 10 epochs, and (3) rephrasing the data 10 times with a single training pass. As shown in Table 1, the accuracy consistently improves across these strategies, demonstrating the efficacy of our rephrasing-based augmentation. We extended this method to other large-scale knowledge corpora and observed similarly encouraging results, and each corpora is rephrased at most twice.\n",
       "\n",
       "Table 1: SimpleQA Accuracy under three rephrasing-epoch configurations\n",
       "\n",
       "\n",
       "\n",
       "<table><tr><td># Rephrasings</td><td># Epochs</td><td>SimpleQA Accuracy</td></tr><tr><td>0 (raw wiki-text)</td><td>10</td><td>23.76</td></tr><tr><td>1</td><td>10</td><td>27.39</td></tr><tr><td>10</td><td>1</td><td>28.94</td></tr></table>\n",
       "\n",
       "\n",
       "Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "**Figure: img-4.jpeg**\n",
       "\n",
       "**This diagram illustrates a process of splitting a large input text into smaller segments for processing through a rewrite model. The full input excerpt, consisting of 4096 tokens, is divided into multiple partial input excerpts, each containing 256 tokens. These partial input excerpts are then processed individually by the rewrite model in an auto-regressive manner to produce corresponding partial output excerpts. The partial output excerpts are subsequently concatenated to form the full output excerpt. The diagram highlights the flow from splitting the input, processing through the rewrite model, and combining the outputs to achieve the final result.**\n",
       "Figure 4: Auto-regressive chunk-wise rephrasing pipeline for long input excerpts. The input is split into smaller chunks with preserved context, rewritten sequentially, and then concatenated into a full rewritten passage.\n",
       "\n",
       "Mathematics Data Rephrasing To enhance mathematical reasoning capabilities, we rewrite high-quality mathematical documents into a \"learning-note\" style, following the methodology introduced in SwallowMath [15]. In addition, we increased data diversity by translating high-quality mathematical materials from other languages into English.\n",
       "\n",
       "Although initial experiments with rephrased subsets of our datasets show promising results, the use of synthetic data as a strategy for continued scaling remains an active area of investigation. Key challenges include generalizing the approach to diverse source domains without compromising factual accuracy, minimizing hallucinations and unintended toxicity, and ensuring scalability to large-scale datasets.\n",
       "\n",
       "Pre-training Data Overall The Kimi K2 pre-training corpus comprises 15.5 trillion tokens of curated, high-quality data spanning four primary domains: Web Text, Code, Mathematics, and Knowledge. Most data processing pipelines follow the methodologies outlined in Kimi K1.5 [35]. For each domain, we performed rigorous correctness and quality validation and designed targeted data experiments to ensure the curated dataset achieved both high diversity and effectiveness.\n",
       "\n",
       "# 2.3 Model Architecture\n",
       "\n",
       "Kimi K2 is a 1.04 trillion-parameter Mixture-of-Experts (MoE) transformer model with 32 billion activated parameters. The architecture follows a similar design to DeepSeek-V3 [10], employing Multi-head Latent Attention (MLA) [44] as the attention mechanism, with a model hidden dimension of 7168 and an MoE expert hidden dimension of 2048. Our scaling law analysis reveals that continued increases in sparsity yield substantial performance improvements, which motivated us to increase the number of experts to 384, compared to 256 in DeepSeek-V3. To reduce computational overhead during inference, we cut the number of attention heads to 64, as opposed to 128 in DeepSeek-V3. Table 2 presents a detailed comparison of architectural parameters between Kimi K2 and DeepSeek-V3.\n",
       "\n",
       "Table 2: Architectural comparison between Kimi K2 and DeepSeek-V3\n",
       "\n",
       "\n",
       "\n",
       "<table><tr><td></td><td>DeepSeek-V3</td><td>Kimi K2</td><td></td></tr><tr><td>#Layers</td><td>61</td><td>61</td><td>=</td></tr><tr><td>Total Parameters</td><td>671B</td><td>1.04T</td><td>54%</td></tr><tr><td>Activated Parameters</td><td>37B</td><td>32.6B</td><td>13%</td></tr><tr><td>Experts (total)</td><td>256</td><td>384</td><td>50%</td></tr><tr><td>Experts Active per Token</td><td>8</td><td>8</td><td>=</td></tr><tr><td>Shared Experts</td><td>1</td><td>1</td><td>=</td></tr><tr><td>Attention Heads</td><td>128</td><td>64</td><td>50%</td></tr><tr><td>Number of Dense Layers</td><td>3</td><td>1</td><td>67%</td></tr><tr><td>Expert Grouping</td><td>Yes</td><td>No</td><td>-</td></tr></table>\n",
       "\n",
       "\n",
       "Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "Sparsity Scaling Law We develop a sparsity scaling law tailored for the Mixture-of-Experts (MoE) model family using Muon. Sparsity is defined as the ratio of the total number of experts to the number of activated experts. Through carefully controlled small-scale experiments, we observe that  under a fixed number of activated parameters (i.e., constant FLOPs)  increasing the total number of experts (i.e., increasing sparsity) consistently lowers both the training and validation loss, thereby enhancing overall model performance (Figure 5). Concretely, under the compute-optimal sparsity scaling law, achieving the same validation loss of 1.5, sparsity 48 reduces FLOPs by  $1.69 \\times$ ,  $1.39 \\times$ , and  $1.15 \\times$  compared to sparsity levels 8, 16, and 32, respectively. Though increasing sparsity leads to better performance, this gain comes with increased infrastructure complexity. To balance model performance with cost, we adopt a sparsity of 48 for Kimi K2, activating 8 out of 384 experts per forward pass.\n",
       "\n",
       "**Figure: img-5.jpeg**\n",
       "\n",
       "**This graph shows the relationship between Training FLOPs (Floating Point Operations) and Validation Loss for different levels of sparsity in a neural network. The x-axis represents the Training FLOPs on a logarithmic scale, while the y-axis represents the Validation Loss. Different colors and markers represent different sparsity levels: sparsity 8 (orange), sparsity 16 (green), sparsity 32 (purple), sparsity 48 (red), and sparsity 64 (blue). Each data point corresponds to a specific combination of Training FLOPs and Validation Loss for a given sparsity level. The dashed lines indicate the trend for each sparsity level. The graph suggests that as the number of Training FLOPs increases, the Validation Loss generally decreases, with varying rates of improvement depending on the sparsity level.**\n",
       "Figure 5: Sparsity Scaling Law. Increasing sparsity leads to improved model performance. We fixed the number of activated experts to 8 and the number of shared experts to 1, and varied the total number of experts, resulting in models with different sparsity levels.\n",
       "\n",
       "**Figure: img-6.jpeg**\n",
       "\n",
       "**This graph shows the relationship between the number of training tokens and validation loss for different models. The x-axis represents the number of training tokens on a logarithmic scale, while the y-axis represents the validation loss. The graph includes multiple data series distinguished by different markers and colors. The blue squares and circles represent models with the number of attention heads equal to the number of layers and their counterparts with doubled attention heads, respectively. The orange, green, pink, and blue data series represent models with different computational budgets (1.2e+20 FLOPs, 2.2e+20 FLOPs, 4.5e+20 FLOPs, and 9.0e+20 FLOPs, respectively). The dotted lines connecting the markers indicate trends within each data series. The graph shows that as the number of training tokens increases, the validation loss generally decreases, with variations depending on the model configuration and computational budget.**\n",
       "Figure 6: Scaling curves for models with number of attention heads equals to number of layers and their counterparts with doubled attention heads. Doubling the number of attention heads leads to a reduction in validation loss of approximately  $0.5\\%$  to  $1.2\\%$ .\n",
       "\n",
       "Number of Attention Heads DeepSeek-V3 [10] sets the number of attention heads to roughly twice the number of model layers to better utilize memory bandwidth and enhance computational efficiency. However, as the context length increases, doubling the number of attention heads leads to significant inference overhead, reducing efficiency at longer sequence lengths. This becomes a major limitation in agentic applications, where efficient long context processing is essential. For example, with a sequence length of 128k, increasing the number of attention heads from 64 to 128, while keeping the total expert count fixed at 384, leads to an  $83\\%$  increase in inference FLOPs. To evaluate the impact of this design, we conduct controlled experiments comparing configurations where the number of attention heads equals the number of layers against those with double number of heads, under varying training FLOPs. Under iso-token training conditions, we observe that doubling the attention heads yields only modest improvements in validation loss (ranging from  $0.5\\%$  to  $1.2\\%$ ) across different compute budgets (Figure 6). Given that sparsity 48 already offers strong performance, the marginal gains from doubling attention heads do not justify the inference cost. Therefore we choose to 64 attention heads.\n",
       "\n",
       "# 2.4 Training Infrastructure\n",
       "\n",
       "# 2.4.1 Compute Cluster\n",
       "\n",
       "Kimi K2 was trained on a cluster equipped with NVIDIA H800 GPUs. Each node in the H800 cluster contains 2 TB RAM and 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes,  $8 \\times 400$  Gbps RoCE interconnects are utilized to facilitate communications.\n",
       "\n",
       "# 2.4.2 Parallelism for Model Scaling\n",
       "\n",
       "Training of large language models often progresses under dynamic resource availability. Instead of optimizing one parallelism strategy that's only applicable under specific amount of resources, we pursue a flexible strategy that allows Kimi K2 to be trained on any number of nodes that is a multiple of 32. Our strategy leverages a combination of 16-way\n",
       "K Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "**Figure: img-7.jpeg**\n",
       "\n",
       "**This image depicts a timeline of computational, communication, and offloading processes in a machine learning pipeline. The timeline is divided into three main categories: Computation, Communication, and Offload. Each category is further divided into specific tasks such as Attention (Attn), Multi-Layer Perceptron (MLP), Error Propagation Dispatch (EP-D), Error Propagation Combine (EP-C), Weight Gradient (WGrad), and Pipeline Parallel (PP). The timeline is segmented into different stages, each marked by a number, indicating the sequence of operations. The color coding helps to distinguish between forward pass (blue), backward pass (red), and PP communication (green). The image also highlights the offloading and loading of tasks, showing how data and computations are managed across different stages.**\n",
       "Figure 7: Computation, communication and offloading overlapped in different PP phases.\n",
       "\n",
       "Pipeline Parallelism (PP) with virtual stages [28, 53, 38, 57, 47, 21], 16-way Expert Parallelism (EP) [39], and ZeRO-1 Data Parallelism [60].\n",
       "\n",
       "Under this setting, storing the model parameters in BF16 and their gradient accumulation buffer in FP32 requires approximately 6 TB of GPU memory, distributed over a model-parallel group of 256 GPUs. Placement of optimizer states depends on the training configurations. When the total number of training nodes is large, the optimizer states are distributed, reducing its per-device memory footprint to a negligible level. When the total number of training nodes is small (e.g., 32), we can offload some optimizer states to CPU.\n",
       "\n",
       "This approach allows us to reuse an identical parallelism configuration for both small- and large-scale experiments, while letting each GPU hold approximately 30 GB of GPU memory for all states. The rest of the GPU memory are used for activations, as described in Sec. 2.4.3. Such a consistent design is important for research efficiency, as it simplifies the system and substantially accelerates experimental iteration.\n",
       "\n",
       "EP communication overlap with interleaved 1F1B By increasing the number of warm-up micro-batches, we can overlap EP all-to-all communication with computation under the standard interleaved 1F1B schedule [21, 53]. In comparison, DualPipe [10] doubles the memory required for parameters and gradients, necessitating an increase in parallelism to compensate. Increasing PP introduces more bubbles, while increasing EP, as discussed below, incurs higher overhead. The additional costs are prohibitively high for training a large model with over 1 trillion parameters and thus we opted not to use DualPipe.\n",
       "\n",
       "However, interleaved 1F1B splits the model into more stages, introducing non-trivial PP communication overhead. To mitigate this cost, we decouple the weight-gradient computation from each micro-batch's backward pass and execute it in parallel with the corresponding PP communication. Consequently, all PP communications can be effectively overlapped except for the warm-up phase.\n",
       "\n",
       "Smaller EP size To ensure full computation-communication overlap during the 1F1B stage, the reduced attention computation time in K2 (which has 64 attention heads compared to 128 heads in DeepSeek-V3) necessitates minimizing the time of EP operations. This is achieved by adopting the smallest feasible EP parallelization strategy, specifically  $\\mathrm{EP} = 16$ . Utilizing a smaller EP group also relaxes expert-balance constraints, allowing for near-optimal speed to be achieved without further tuning.\n",
       "\n",
       "# 2.4.3 Activation Reduction\n",
       "\n",
       "After reserving space for parameters, gradient buffers, and optimizer states, the remaining GPU memory on each device is insufficient to hold the full MoE activations. To ensure the activation memory fits within the constraints, especially for the initial pipeline stages that accumulate the largest activations during the 1F1B warm-up phase, the following techniques are employed.\n",
       "\n",
       "Selective recomputation Recomputation is applied to inexpensive, high-footprint stages, including LayerNorm, SwiGLU, and MLA up-projections [10]. Additionally, MoE down-projections are recomputed during training to further reduce activation memory. While optional, this recomputation maintains adequate GPU memory, preventing crashes caused by expert imbalance in early training stages.\n",
       "\n",
       "FP8 storage for insensitive activations Inputs of MoE up-projections and SwiGLU are compressed to FP8-E4M3 in  $1 \\times 128$  tiles with FP32 scales. Small-scale experiments show no measurable loss increase. Due to potential risks of performance degradation that we observed during preliminary study, we do not apply FP8 in computation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display (just first page)\n",
    "final_markdown, doc_anno = process_saved_ocr_json(\"RAG/OCR/responses_reindexed.json\", range=[0, 1], include_images=False, include_tables=True)\n",
    "display(Markdown(final_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f2c9ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Document Summary\n",
      "**The document presents a technical report on Kimi K2, detailing its contributions, token template for tool calling, evaluation details, and other technical aspects. It includes a list of authors, an explanation of the token structure for tool calling, and performance metrics across various benchmarks. The report highlights Kimi K2's superior performance in coding tasks, tool use tasks, math and STEM tasks, general tasks, and long context and factuality tasks. It also discusses the minimal impact of QK-Clip on model quality and the engine switching pipeline for RL training.**\n",
      " ## Authors \n",
      " **['Yifan Bai', 'Yiping Bao', 'Guanduo Chen', 'Jiahao Chen', 'Ningxin Chen', 'Ruijue Chen', 'Yanru Chen', 'Yuankun Chen', 'Yutian Chen', 'Zhuofu Chen*', 'Jialei Cui', 'Hao Ding', 'Mengnan Dong', 'Ang`ang Du', 'Chenzhuang Du', 'Dikang Du', 'Yulun Du', 'Yu Fan', 'Yichen Feng', 'Kelin Fu', 'Bofei Gao', 'Hongcheng Gao', 'Peizhong Gao', 'Tong Gao', 'Xinran Gu', 'Longyu Guan', 'Haiqing \n"
     ]
    }
   ],
   "source": [
    "# authors are on the last page:\n",
    "final_page = process_saved_ocr_json(\"RAG/OCR/responses.json\", range=[3, 4])\n",
    "print(final_page[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257c98b",
   "metadata": {},
   "source": [
    "### 2.2 Parsing Metadata\n",
    "\n",
    "An important aspect of a RAG system is the metadata: we want to present data to the user with the given source, probably the page, images and tables if we parsed them... and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8ee8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ocr_json(json_path: str, range: list[int] = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Loads the saved JSON list of OCR responses.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing OCR responses\n",
    "        range: Optional range [start, end] to slice the responses list\n",
    "\n",
    "    Returns:\n",
    "        List of OCR response dictionaries\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        responses_list = json.load(f)\n",
    "    \n",
    "    # Return sliced or full list\n",
    "    return responses_list[range[0]:range[1]] if range is not None else responses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "300076dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'The document introduces Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. It highlights the MuonClip optimizer, which improves upon the Muon optimizer with a novel QK-clip technique to enhance training stability and token efficiency. Kimi K2 was pre-trained on 15.5 trillion tokens and underwent a multi-stage post-training process, including a large-scale agentic data synthesis pipeline and a joint reinforcement learning stage. The model demonstrates state-of-the-art performance on various benchmarks, particularly excelling in agentic capabilities and tasks related to software engineering. The document also discusses the technical aspects of the model, including the MuonClip optimizer, the architecture of Kimi K2, and the training infrastructure used. It concludes by mentioning the release of the base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.', 'authors': ['Kimi Team']}\n",
      "{'summary': 'The document discusses the technical aspects of a model named Kimi K2, focusing on its training, post-training, and reinforcement learning processes. It details the activation CPU offload mechanism, training recipe, supervised fine-tuning, and reinforcement learning strategies. The document also covers the data synthesis pipeline for tool use, domain evolution, agent diversification, and the infrastructure supporting these processes. It concludes with evaluations of Kimi-K2-Instruct against various benchmarks, highlighting its performance in coding, tool use, math & STEM tasks, and general tasks.', 'authors': ['Kimi K2 Technical Report']}\n",
      "{'summary': 'The document presents a comprehensive evaluation of the Kimi-K2-Instruct and Kimi-K2-Base models, highlighting their superior performance across various benchmarks and tasks. Kimi-K2-Instruct excels in multi-turn tool-use benchmarks, general capabilities, and open-ended evaluations, outperforming all baselines in grounded, controlled, and agent-driven tool orchestration. It also shows strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. Kimi-K2-Base demonstrates state-of-the-art performance in general language understanding, coding capabilities, mathematical reasoning, and Chinese language understanding. The document also details the evaluation settings and results for Kimi-K2-Base, showcasing its leading performance across diverse evaluation benchmarks. Additionally, it discusses safety evaluations, limitations, and conclusions, acknowledging the support of the OpenHands and Multi-SWE-bench teams.', 'authors': ['Kimi-K2 Team']}\n",
      "{'summary': \"The document presents a technical report on Kimi K2, detailing its contributions, token template for tool calling, evaluation details, and other technical aspects. It includes a list of authors, an explanation of the token structure for tool calling, and performance metrics across various benchmarks. The report highlights Kimi K2's superior performance in coding tasks, tool use tasks, math and STEM tasks, general tasks, and long context and factuality tasks. It also discusses the minimal impact of QK-Clip on model quality and the engine switching pipeline for RL training.\", 'authors': ['Yifan Bai', 'Yiping Bao', 'Guanduo Chen', 'Jiahao Chen', 'Ningxin Chen', 'Ruijue Chen', 'Yanru Chen', 'Yuankun Chen', 'Yutian Chen', 'Zhuofu Chen*', 'Jialei Cui', 'Hao Ding', 'Mengnan Dong', 'Ang`ang Du', 'Chenzhuang Du', 'Dikang Du', 'Yulun Du', 'Yu Fan', 'Yichen Feng', 'Kelin Fu', 'Bofei Gao', 'Hongcheng Gao', 'Peizhong Gao', 'Tong Gao', 'Xinran Gu', 'Longyu Guan', 'Haiqing Guo*', 'Jianhang Guo', 'Hao Hu', 'Xiaoru Hao', 'Tianhong He', 'Weiran He', 'Wenyang He', 'Chao Hong', 'Yangyang Hu', 'Zhenxing Hu', 'Weixiao Huang', 'Zhiqi Huang', 'Zihao Huang', 'Tao Jiang', 'Zhejun Jiang', 'Xinyi Jin', 'Yongsheng Kang*', 'Guokun Lai', 'Cheng Li', 'Fang Li', 'Haoyang Li', 'Ming Li', 'Wentao Li', 'Yanhao Li', 'Yiwei Li', 'Zhaowei Li', 'Zheming Li', 'Xiaohan Lin', 'Zongyu Lin', 'Chengyin Liu', 'Chenyu Liu', 'Dikang Liu', 'Jingyuan Liu*', 'Junqi Liu', 'Liang Liu', 'Shaowei Liu', 'T.Y. Liu', 'Tianwei Liu', 'Weizhou Liu', 'Yangyang Liu', 'Yibo Liu', 'Yiping Liu', 'Yue Liu', 'Zhengying Liu', 'Enzhe Lu', 'Lijun Lu', 'Shengling Ma', 'Xinyu Ma', 'Yingwei Ma', 'Shaoguang Mao', 'Jie Mei', 'Xin Men', 'Yibo Miao', 'Jinjing Xu', 'L.H. Xu', 'Lin Xu', 'Ruoyu Qin', 'Bowen Qu', 'Zeyu Shang', 'Lidong Shi', 'Shengyuan Shi', 'Feifan Song', 'Flood Sung', 'Heyu Tang', 'Jiawen Tao', 'Qifeng Teng', 'Chensi Wang', 'Dinglu Wang', 'Feng Wang', 'Haiming Wang', 'Jianzhou Wang*', 'Jiaxing Wang', 'Jinhong Wang', 'Shengjie Wang', 'Shuyi Wang', 'Yao Wang', 'Yejie Wang', 'Yiqin Wang', 'Yuxin Wang', 'Yuzhi Wang', 'Zhaoji Wang', 'Zhengtao Wang', 'Zhexu Wang', 'Chu Wei', 'Qianqian Wei', 'Wenhao Wu', 'Xingzhe Wu', 'Yuxin Wu', 'Yutong Zhang', 'Haotian Zhao', 'Yikai Zhao', 'Yuang Zhang', 'Yizhi Zhang', 'Yongting Zhang', 'Yu Zhang', 'Yutao Zhang', 'Zhen Zhang', 'Huabin Zheng', 'Shaojie Zheng', 'Jianren Zhou', 'Xinyu Zhou', 'Zaida Zhou', 'Zhen Zhu', 'Weiyu Zhuang', 'Xinxing Zu', 'Kimi K2']}\n"
     ]
    }
   ],
   "source": [
    "# Load all responses\n",
    "all_responses = load_ocr_json(\"RAG/OCR/responses.json\")\n",
    "# Access the data\n",
    "for resp in all_responses:\n",
    "    print(resp[\"document_annotation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4619e",
   "metadata": {},
   "source": [
    "This is a part of our metadata. Since we split the document into several parts, we may want to fuse together all the author fields, but maybe get the summary only from the first part (where the abstract is) or maybe get an llm to summarize the split summaries into one. Another part is the page, of the document, but that is easy to retrieve.\n",
    " \n",
    "But another important part of our metadata is the images and the tables. \n",
    "\n",
    "We want to replace images and tables with a textual description for our RAG. Then we'll map to the actual base64/html encoding with metadata.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5c5fb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img-0.jpeg\n",
      "{'image_type': 'graph', 'description': \"This image shows a series of bar charts comparing the performance of various AI models across different benchmarks. The benchmarks are categorized into 'Agentic and Competitive Coding', 'Tool Use', and 'Math & STEM'. Each bar chart represents a specific benchmark, such as SWE-bench Verified, SWE-bench Multilingual, LiveCodeBench v6, OJBench, Tau2-bench micro-average, AceBench (en), AIME 2025, and GPQA-Diamond. The AI models compared include Kimi-K2-Instruct, DeepSeekV3-0324, Owen3-235B-A22B, OpenAI GPT-4.1, Claude 4 Opus, Claude 4 Sonnet, and Gemini 2.5 Flash non-thinking. Each bar chart shows the performance scores of these models, with Kimi-K2-Instruct generally performing the best across most benchmarks.\"}\n"
     ]
    }
   ],
   "source": [
    "first_img = all_responses[0][\"pages\"][0][\"images\"][0]\n",
    "print(first_img['id'])\n",
    "print(first_img['image_annotation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3d246",
   "metadata": {},
   "source": [
    "We already parsed our OCR'd document in a nice way with the above functions: we are able to get a full markdown with indexed placeholders for images and tables, together with images' descriptions.\n",
    "\n",
    "But how about tables descriptions??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32be93",
   "metadata": {},
   "source": [
    "Well, we may have to do this manually, since mistral does not implement that specific function. \n",
    "\n",
    "*My idea is to give the pdf of the paper, together with the extracted tables, to a multimodal model that will provide a summary of the given tables and an index. Then we swap the result in with the tables extracted from Mistral.*\n",
    "\n",
    "Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27aaeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field, SecretStr\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"google/gemini-2.5-flash\", \n",
    "    \n",
    "    # redirect LangChain to OpenRouter\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "\n",
    "    # pass the OpenRouter key\n",
    "    api_key=SecretStr(os.environ[\"OPENROUTER_API_KEY\"])\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a document indexing assistant. \n",
    "I will provide a document and its corresponding OCR Markdown. \n",
    "Your task is to find every table placeholder (e.g., [tbl-x.html]) in the text, identify what that table represents, and return a JSON mapping.\n",
    "\n",
    "Specifically, you must return an answer composed of 2 lists:\n",
    "- titles: the titles of the tables, extracted from the document.\n",
    "- descriptions: the descriptions of the tables. \n",
    "These descriptions must be thorough and include all the information in the table, highlighting the main points and the context.\n",
    "\"\"\"\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    titles : list[str] = Field(\"The titles of the tables, extracted from the document.\")\n",
    "    descriptions : list[str] = Field(\"The descriptions of the tables.\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    system_prompt=prompt,\n",
    "    response_format=ResponseFormat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8580c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# get the full text with no base64 image data but with table contents (as html)\n",
    "# we want this llm to check the tables content and have full context \n",
    "full_text = process_saved_ocr_json(\"RAG/OCR/responses_reindexed.json\", include_images=False, include_tables=True)\n",
    "# we also pass the pdf in order to have full context (maybe not needed)\n",
    "base64_pdf = encode_pdf(\"RAG/documents/KimiK2.pdf\")\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Here is the structured OCR text and tables for context:\\n\\n{full_text}\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image_url\", # OpenAI uses this block for PDF vision support\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_state = {\"messages\" : [message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3794015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Table 0:\n",
      "Title: SimpleQA Accuracy under three rephrasing-epoch configurations\n",
      "Description: This table compares the SimpleQA Accuracy under three different configurations related to data rephrasing and the number of training epochs. The first row shows the accuracy for raw wiki-text repeated for 10 epochs (23.76). The second row shows the accuracy for data rephrased once and repeated for 10 epochs (27.39). The third row shows the accuracy for data rephrased 10 times with a single training pass (28.94). The table demonstrates that rephrasing data leads to improved accuracy, with more rephrasing and fewer epochs yielding better results.\n",
      "----------------------------------------\n",
      "Table 1:\n",
      "Title: Architectural comparison between Kimi K2 and DeepSeek-V3\n",
      "Description: This table provides a detailed comparison of architectural parameters between Kimi K2 and DeepSeek-V3. It includes metrics such as #Layers, Total Parameters, Activated Parameters, Experts (total), Experts Active per Token, Shared Experts, Attention Heads, Number of Dense Layers, and Expert Grouping. Kimi K2 has 61 layers, 1.04T total parameters (54% from DeepSeek-V3), 32.6B activated parameters (13% from DeepSeek-V3), 384 total experts (50% from DeepSeek-V3), 8 active experts per token, 1 shared expert, 64 attention heads (50% from DeepSeek-V3), and 1 dense layer (67% from DeepSeek-V3). Unlike DeepSeek-V3, Kimi K2 does not use expert grouping.\n",
      "----------------------------------------\n",
      "Table 2:\n",
      "Title: Performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across diverse tasks.\n",
      "Description: This table presents a comprehensive performance comparison of Kimi-K2-Instruct against leading open-source and proprietary models across a diverse set of tasks categorized into Coding Tasks, Tool Use Tasks, Math & STEM Tasks, and General Tasks. For each benchmark, the table lists the scores for Kimi-K2-Instruct, DeepSeek-V3-0324, Qwen3-235B-A22B (open-source), Claude Sonnet 4, Claude Opus 4, GPT-4.1, and Gemini 2.5 Flash (proprietary). Key highlights in Coding Tasks include Kimi-K2-Instruct's strong performance on LiveCodeBench v6 (53.7) and OJBench (27.1), and notable scores on SWE-bench Verified. In Tool Use Tasks, Kimi-K2-Instruct leads on Tau2 retail (70.6), Tau2 airline (56.5), Tau2 telecom (65.8), and AceBench (76.5). For Math & STEM Tasks, Kimi-K2-Instruct shows excellent results on AIME 2024 (69.6), AIME 2025 (49.5), MATH-500 (97.4), HMMT 2025 (38.8), CNMO 2024 (74.3), PolyMath-en (65.1), ZebraLogic (89.0), AutoLogi (89.5), GPQA-Diamond (75.1), and SuperGPQA (57.2). In General Tasks, Kimi-K2-Instruct performs well on MMLU (89.5), MMLU-Redux (92.7), MMLU-Pro (81.1), IFEval (89.8), Multi-Challenge (54.1), SimpleQA (31.0), Livebench (76.4), Arena Hard v2.0 (54.5% win rate for hard prompts, 85.0% for creative writing), FACTS Grounding (88.5), HHEM v2.1 (98.9), and FaithJudge (92.6). Long context tasks like LongBench v2 (49.1), FRAMES (77.1), MRCR (55.0), and DROP (93.5) are also included. Bold values indicate global SOTA, and underlined bold indicates the best open-source result.\n",
      "----------------------------------------\n",
      "Table 3:\n",
      "Title: Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks.\n",
      "Description: This table provides a comprehensive comparison of Kimi-K2-Base against leading open-source models across diverse tasks, grouped by English, Code, Math, and Chinese capabilities. For each benchmark, the table includes the number of shots, and scores for Kimi-K2-Base, DeepSeek-V3-Base, Llama4-Maverick-Base, and Qwen2.5-72B-Base. The architecture for Kimi-K2-Base, DeepSeek-V3-Base, and Llama4-Maverick-Base is MoE, while Qwen2.5-72B-Base is Dense. Key highlights include Kimi-K2-Base achieving state-of-the-art performance in English language understanding for MMLU (87.79), MMLU-pro (69.17), MMLU-redux (90.17), SuperGPQA (44.67), SimpleQA (35.25), TriviaQA (85.09), BBH (88.71) and ARC-Challenge (95.73). For coding capabilities, Kimi-K2-Base leads in CRUXEval-I-cot (74.00), CRUXEval-O-cot (83.50), LiveCodeBench (v6) (26.29), and EvalPlus (80.33). In mathematical reasoning, Kimi-K2-Base excels in MATH (70.22), GSM8k (92.12), and GSM8k-platinum (94.21). For Chinese language capabilities, Kimi-K2-Base achieves top scores in C-Eval (92.50), CMMLU (90.90), and CSimpleQA (77.57).\n",
      "----------------------------------------\n",
      "Table 4:\n",
      "Title: Enabled Plugins and Strategies\n",
      "Description: This table lists the plugins and strategies evaluated in the safety assessment. The plugins include Harmful (Graphic Content, Harassment and Bullying, Hate Speech, Insults, Profanity, Radicalization, Self Harm, Sexual Content, ToxicChat), Criminal (Chemical & Biological Weapons, Child Exploitation, Copyright Violations, Cybercrime, Illegal Activities, Illegal Drugs, Indiscriminate Weapons, Intellectual Property Violation, Non-Violent Crime, Violent Crime, Sex Crimes), Misinformation (Competitor Endorsement, Unsupervised Contracts, Excessive Agency, Hallucination, Misinformation and Disinformation, Specialized Advice, Unsafe Practices, Imitation, Overreliance, Political Opinions, Religious Sensitivity), Privacy (Privacy Violation, PII in API/Database, Direct PII Exposure, PII in Session Data, PII via Social Engineering), and Security (ASCII Smuggling, CyberSecEval, Harmbench, Debug Access, Divergent Repetition, DoNotAnswer, Malicious Code, Pliny, Prompt Extraction, Reasoning DoS, Tool Discovery). The strategies evaluated alongside these plugins are Basic, Prompt Injection, Iterative Jailbreak, and Crescendo. Each plugin is paired with all listed strategies for evaluation.\n",
      "----------------------------------------\n",
      "Table 5:\n",
      "Title: Safety Evaluation Results\n",
      "Description: This table presents the safety evaluation results, showing the passing rates (%) of different models (Kimi-K2-Instruct, DeepSeek-V3-0324, DeepSeek-R1, Qwen3-235B-A22B) across various plugin-strategy combinations. For Harmful content, Kimi-K2-Instruct generally performs well on Basic and Base64 strategies but shows lower passing rates on Iterative Jailbreak and Crescendo (64.71). For Criminal content, Kimi-K2-Instruct is 100% on Basic but drops significantly on Iterative Jailbreak (57.57) and Crescendo (56.06). For Misinformation, Kimi-K2-Instruct maintains high passing rates for Basic (97.28), Base64 (98.48), and Prompt Injection (98.39), with lower rates on Iterative Jailbreak (63.97) and Crescendo (85.71). For Privacy, all models show high passing rates, with Kimi-K2-Instruct achieving 100% on Basic and Base64 and high scores for other strategies. For Security, Kimi-K2-Instruct has varying performance, with Base64 scoring 82.93 and Iterative Jailbreak being the lowest at 43.90. The table suggests that complex strategies like Iterative Jailbreak and Crescendo pose greater challenges for some models.\n",
      "----------------------------------------\n",
      "Table 6:\n",
      "Title: Kimi-K2-Instruct Open-Ended Evaluation (aggregated)\n",
      "Description: This bar chart titled 'Kimi-K2-Instruct Open-Ended Evaluation (aggregated)' compares the win, tie, and loss percentages of Kimi-K2-Instruct against DeepSeek-V3-0324, Claude-Sonnet-4, and ChatGPT-4o-latest on an aggregated Chinese in-house benchmark. Against DeepSeek-V3-0324, Kimi-K2-Instruct has a 59.6% win rate, 23.5% tie rate, and 16.9% loss rate. Against Claude-Sonnet-4, Kimi-K2-Instruct has a 64.6% win rate, 18.8% tie rate, and 16.6% loss rate. Against ChatGPT-4o-latest, Kimi-K2-Instruct shows a 65.4% win rate, 17.6% tie rate, and 17.0% loss rate. The chart visually represents Kimi-K2-Instruct's strong performance in Chinese open-ended evaluations, consistently achieving higher win rates and lower loss rates compared to the other models tested.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 40)\n",
    "for i, (title, description) in enumerate(zip(result['structured_response'].titles, result['structured_response'].descriptions)):\n",
    "    print(f\"Table {i}:\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# save this to file in RAG/OCR/tables_descriptions.json\n",
    "# Create a list of dictionaries to preserve the relationship between titles and descriptions\n",
    "tables_data = [\n",
    "    {\n",
    "        \"index\": i,\n",
    "        \"title\": title,\n",
    "        \"description\": description\n",
    "    }\n",
    "    for i, (title, description) in enumerate(zip(\n",
    "        result['structured_response'].titles, \n",
    "        result['structured_response'].descriptions\n",
    "    ))\n",
    "]\n",
    "\n",
    "with open(\"RAG/OCR/tables_descriptions.json\", \"w\") as f:\n",
    "    json.dump(tables_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72a1cd",
   "metadata": {},
   "source": [
    "Nice! now we swap these descriptions with our table indices (also, I checked and mistral got 7 tables as well - they match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6dc8ca77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "K KIMI K2: OPEN AGENTIC INTELLIGENCE\n",
       "\n",
       "# TECHNICAL REPORT OF KIMI K2\n",
       "\n",
       "# Kimi Team\n",
       "\n",
       "# ABSTRACT\n",
       "\n",
       "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.\n",
       "\n",
       "Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual  surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints $^{1}$  to facilitate future research and applications of agentic intelligence.\n",
       "\n",
       "**Figure: img-0.jpeg**\n",
       "\n",
       "**This image shows a series of bar charts comparing the performance of various AI models across different benchmarks. The benchmarks are categorized into 'Agentic and Competitive Coding', 'Tool Use', and 'Math & STEM'. Each bar chart represents a specific benchmark, such as SWE-bench Verified, SWE-bench Multilingual, LiveCodeBench v6, OJBench, Tau2-bench micro-average, AceBench (en), AIME 2025, and GPQA-Diamond. The AI models compared include Kimi-K2-Instruct, DeepSeekV3-0324, Owen3-235B-A22B, OpenAI GPT-4.1, Claude 4 Opus, Claude 4 Sonnet, and Gemini 2.5 Flash non-thinking. Each bar chart shows the performance scores of these models, with Kimi-K2-Instruct generally performing the best across most benchmarks.**\n",
       "Figure 1: Kimi K2 main results.2\n",
       "1 Introduction\n",
       "\n",
       "The development of Large Language Models (LLMs) is undergoing a profound paradigm shift towards *Agentic Intelligence*  the capabilities for models to autonomously perceive, plan, reason, and act within complex and dynamic environments. This transition marks a departure from static imitation learning towards models that actively learn through interactions, acquire new skills beyond their training distribution, and adapt behavior through experiences *[63]*. It is believed that this approach allows an AI agent to go beyond the limitation of static human-generated data, and acquire superhuman capabilities through its own exploration and exploitation. Agentic intelligence is thus rapidly emerging as a defining capability for the next generation of foundation models, with wide-ranging implications across tool use, software development, and real-world autonomy.\n",
       "\n",
       "Achieving agentic intelligence introduces challenges in both pre-training and post-training. Pre-training must endow models with broad general-purpose priors under constraints of limited high-quality data, elevating token efficiencylearning signal per tokenas a critical scaling coefficient. Post-training must transform those priors into actionable behaviors, yet agentic capabilities such as multi-step reasoning, long-term planning, and tool use are rare in natural data and costly to scale. Scalable synthesis of structured, high-quality agentic trajectories, combined with general reinforcement learning (RL) techniques that incorporate preferences and self-critique, are essential to bridge this gap.\n",
       "\n",
       "In this work, we introduce Kimi K2, a 1.04 trillion-parameter Mixture-of-Experts (MoE) LLM with 32 billion activated parameters, purposefully designed to address the core challenges and push the boundaries of agentic capability. Our contributions span both the pre-training and post-training frontiers:\n",
       "\n",
       "- We present MuonClip, a novel optimizer that integrates the token-efficient Muon algorithm with a stability-enhancing mechanism called QK-Clip. Using MuonClip, we successfully pre-trained Kimi K2 on 15.5 trillion tokens without a single loss spike.\n",
       "- We introduce a large-scale agentic data synthesis pipeline that systematically generates tool-use demonstrations via simulated and real-world environments. This system constructs diverse tools, agents, tasks, and trajectories to create high-fidelity, verifiably correct agentic interactions at scale.\n",
       "- We design a general reinforcement learning framework that combines verifiable rewards (RLVR) with a self-critique rubric reward mechanism. The model learns not only from externally defined tasks but also from evaluating its own outputs, extending alignment from static into open-ended domains.\n",
       "\n",
       "Kimi K2 demonstrates strong performance across a broad spectrum of agentic and frontier benchmarks. It achieves scores of 66.1 on Tau2-bench, 76.5 on ACEBench (en), 65.8 on SWE-bench Verified, and 47.3 on SWE-bench Multilingual, outperforming most open- and closed-weight baselines under non-thinking evaluation settings, closing the gap with Claude 4 Opus and Sonnet. In coding, mathematics, and broader STEM domains, Kimi K2 achieves 53.7 on LiveCodeBench v6, 27.1 on OJBench, 49.5 on AIME 2025, and 75.1 on GPQA-Diamond, further highlighting its capabilities in general tasks. On the LMSYS Arena leaderboard (July 17, 2025), Kimi K2 ranks as the top 1 open-source model and 5th overall based on over 3,000 user votes.\n",
       "\n",
       "To spur further progress in Agentic Intelligence, we are open-sourcing our base and post-trained checkpoints, enabling the community to explore, refine, and deploy agentic intelligence at scale.\n",
       "\n",
       "## 2 Pre-training\n",
       "\n",
       "The base model of Kimi K2 is a trillion-parameter mixture-of-experts (MoE) transformer *[72]* model, pre-trained on 15.5 trillion high-quality tokens. Given the increasingly limited availability of high-quality human data, we posit that token efficiency is emerging as a critical coefficient in the scaling of large language models. To address this, we introduce a suite of pre-training techniques explicitly designed for maximizing token efficiency. Specifically, we employ the token-efficient Muon optimizer *[33, 46]* and mitigate its training instabilities through the introduction of QK-Clip. Additionally, we incorporate synthetic data generation to further squeeze the intelligence out of available high-quality tokens. The model architecture follows an ultra-sparse MoE with multi-head latent attention (MLA) similar to DeepSeek-V3 *[10]* , derived from empirical scaling law analysis. The underlying infrastruc"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def insert_tables_descriptions(full_text: str, tables_titles: list[str], tables_descriptions: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Insert the tables descriptions into the full text at the correct positions.\n",
    "    \"\"\"\n",
    "    for i, (title, description) in enumerate(zip(tables_titles, tables_descriptions)):\n",
    "        full_text = full_text.replace(f\"tbl-{i}.html\", f\"**Table {i}:**\\n\\n**{title}**\\n\\n**{description}**\")\n",
    "    return full_text\n",
    "\n",
    "text_no_imgs_no_tables, doc_anno = process_saved_ocr_json(\"RAG/OCR/responses_reindexed.json\", include_images=False, include_tables=False)\n",
    "text_w_descriptions = insert_tables_descriptions(text_no_imgs_no_tables, result['structured_response'].titles, result['structured_response'].descriptions)\n",
    "display(Markdown(text_w_descriptions[:7000]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ba73c",
   "metadata": {},
   "source": [
    "Perfect! Our text contains all the descriptions necessary for our rag, and we got the indexed images and tables separately as metadata!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
