{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ef7d50",
   "metadata": {},
   "source": [
    "# RAG Agent\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is one of the most useful applications of AI agents. \n",
    "\n",
    "It consists in grounding the answers of our agent to a given knowledge base, that our agent can access by using tools.\n",
    "\n",
    "This knowledge base - i.e., our collection of documents - is embedded in a vector store for efficient search. \n",
    "\n",
    "This method prevents allucinations and highly increases the accuracy of the agentic system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcc8bd",
   "metadata": {},
   "source": [
    "Steps in this implementation:\n",
    "\n",
    "1. get a knowledge base: we do that manually, and for this example we use only one document for simplicity. \n",
    "\n",
    "2. perform OCR of our documents to best extract information;\n",
    "\n",
    "3. embed our documents in a vector database (vector store)\n",
    "\n",
    "4. construct a tool for searching in the database\n",
    "\n",
    "5. create the graph with grading and retries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d451864b",
   "metadata": {},
   "source": [
    "## 1. Knowledge Base\n",
    "\n",
    "We selected scientific papers on the topic on the topic **\"LLM-Agents for Urban Mobility & Traffic Engineering\"**.\n",
    "\n",
    "We stored them in the folder `documents/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa53524",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. OCR (Optical Character Recognition)\n",
    "\n",
    "As we said, we usually need to perform OCR on a knowledge based composed by documents. \n",
    "\n",
    ">Notice that we may skip this step if our knowledge base is already composed of plain text (for example, this could happen if our base is made thorugh web scarping, which usually returns plain text or markdown).\n",
    "\n",
    "In this case our papers are complex, and contain mathematical expressions, tables, images. OCR is not a simple/skippable step here. \n",
    "\n",
    "That's why we are going to user the best OCR model around this days (January 2026): Mistral OCR 3. Find a usage example in the [mistral_ocr](./mistral_ocr.ipynb) notebook, and a full example from the actual Mistral site here: [link](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/data_extraction.ipynb). \n",
    "\n",
    "The latter showcases how to use their `Annotations` API to also annotate the detected bounding boxes. We will use this feature to include images in our RAG pipeline. \n",
    "\n",
    "> **Note:** You can replace Mistral's OCR API with a free process provided by LangChain, using `PyMuPDF4LLM` and the `Upstage Document Parse API`. Results will not be as good as using Mistral but probably will be good enough for many applications (and it's all free). Check the full tutorial here: [Multimodal RAG tutorial](https://langchain-opentutorial.gitbook.io/langchain-opentutorial/19-cookbook/06-multimodal/10-geminimultimodalrag#layout-parsing-to-extract-image-from-pdf-using-upstage-document-parse-api). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a246380",
   "metadata": {},
   "source": [
    "### 2.1 Mistral OCR with Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdef9f",
   "metadata": {},
   "source": [
    "Mistral Document AI API adds two annotation functionalities:\n",
    "\n",
    "- `document_annotation`: returns the annotation of the entire document based on the input schema.\n",
    "- `box_annotation`: gives you the annotation of the bboxes extracted by the OCR model (charts/ figures etc) based on user requirement. The user may ask to describe/caption the figure for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d8b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U mistralai "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca03c3",
   "metadata": {},
   "source": [
    "Function to encode in base 64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd4127de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def encode_pdf(pdf_path):\n",
    "    \"\"\"Encode the pdf to base64.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as pdf_file:\n",
    "            return base64.b64encode(pdf_file.read()).decode('utf-8')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {pdf_path} was not found.\")\n",
    "        return None\n",
    "    except Exception as e:  # Added general exception handling\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0a25c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_pdf = encode_pdf(\"RAG/documents/KimiK2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d769d",
   "metadata": {},
   "source": [
    "First, we need to create our Annotation Formats, for that we advise make use of pydantic.\n",
    "\n",
    "For this example, we will extract the image type and a description of each bbox; as well as the language, authors and a summary of the full document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7dcaa026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "\n",
    "class ImageType(str, Enum):\n",
    "    GRAPH = \"graph\"\n",
    "    TEXT = \"text\"\n",
    "    TABLE = \"table\"\n",
    "    IMAGE = \"image\"\n",
    "\n",
    "class Image(BaseModel):\n",
    "    image_type: ImageType = Field(..., description=\"The type of the image. Must be one of 'graph', 'text', 'table' or 'image'.\")\n",
    "    description: str = Field(..., description=\"A description of the image.\")\n",
    "\n",
    "class Document(BaseModel):\n",
    "    summary: str = Field(..., description=\"A summary of the document.\")\n",
    "    authors: list[str] = Field(..., description=\"A list of authors who contributed to the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4bd4e",
   "metadata": {},
   "source": [
    "Now with our pydantic models created for our Annotations, we can call our OCR endpoint.\n",
    "\n",
    "The objective is to Annotate and Extract information from our document and the bbox/images detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50853dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pages\": [\n",
      "        {\n",
      "            \"index\": 16,\n",
      "            \"markdown\": \"##### Agentic Tool Use\\n\\nOn multi-turn tool-use benchmarks, Kimi-K2-Instruct sets a new standard. It achieves 66.1 Pass@1 on $\\\\tau^{2}$-Bench and 76.5 on ACEBench, substantially outperforming all baselines. These results affirm its strength in grounded, controlled, and agent-driven tool orchestration across domains.\\n\\n##### General Capabilities\\n\\nKimi-K2-Instruct exhibits strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. It surpasses open-source peers on SimpleQA (31.0%), MMLU (89.5%) and MMLU-Redux (92.7%), and leads all models on instruction benchmarks (IFEval: 89.8%, Multi-Challenge: 54.1%). In math and STEM, it achieves top-tier scores (AIME 2024: 69.6%, GPQA-Diamond: 75.1%), and remains competitive on long-context factuality and retrieval (DROP: 93.5%, MRCR: 55.0%). These results position Kimi-K2-Instruct as a well-rounded and capable generalist across both short- and long-context settings.\\n\\n##### Open-Ended Evaluation\\n\\nOn the LMSYS Arena leaderboard (July 17, 2025), Kimi-K2-Instruct ranks as the top-1 open-source model and 5th overall based on over 3,000 user votes. This real-world preference signal\\u2014across diverse, blind prompts\\u2014underscores Kimi-K2\\u2019s strengths in generating high-quality responses on open-ended tasks.\\n\\n### 4.2 Pre-training Evaluations\\n\\n#### 4.2.1 Evaluation Settings\\n\\n##### Benchmarks\\n\\nWe evaluate Kimi-K2-Base across diverse capability areas. For general capabilities, we assess on MMLU *[23]*, MMLU-Pro *[76]*, MMLU-Redux *[17]*, BBH *[67]*, TriviaQA *[34]*, SuperGPQA *[13]*, SimpleQA *[78]*, HellaSwag *[88]*, AGIEval *[89]*, GPQA-Diamond *[61]*, ARC-Challenge *[8]*, and WinoGrande *[62]*. For coding capabilities, we employ EvalPlus *[45]* (averaging HumanEval *[7]*, MBPP *[1]*, HumanEval+, and MBPP+), LiveCodeBench v6 *[31]*, and CRUXEval *[18]*. For mathematical reasoning, we utilize GSM8K *[9]*, GSM8K-Platinum *[74]*, MATH *[24]*, and CMATH *[79]*. For Chinese language capabilities, we evaluate on C-Eval *[29]*, CMMLU *[40]*, and CSimpleQA *[22]*.\\n\\n##### Baselines\\n\\nWe benchmark against leading open-source foundation models: DeepSeek-V3-Base *[10]*, Qwen2.5-72B-Base *[59]* (Note that Qwen3-235B-A22B-Base is not open-sourced, and the largest open-sourced base model in the Qwen series is Qwen2.5-72B-Base), and Llama 4-Maverick *[70]* (Llama 4-Behemoth is also not open-sourced). All models are evaluated under identical configurations to ensure fair comparison.\\n\\n##### Evaluation Configurations\\n\\nWe employ perplexity-based evaluation for MMLU, MMLU-Redux, GPQA-Diamond, HellaSwag, ARC-Challenge, C-Eval, and CMMLU. Generation-based evaluation is used for MMLU-Pro, SuperGPQA, TriviaQA, BBH, CSimpleQA, MATH, CMATH, GSM8K, GSM8K-Platinum, CRUXEval, LiveCodeBench, and EvalPlus. To mitigate the high variance inherent to GPQA-Diamond, we report the mean score across eight independent runs. All evaluations are conducted using our internal framework derived from LM-Harness-Evaluation *[4]*, ensuring consistent settings across all models.\\n\\n#### 4.2.2 Evaluation Results\\n\\nTable 4 presents a comprehensive comparison of Kimi-K2-Base against leading open-source foundation models across diverse evaluation benchmarks. The results demonstrate that Kimi-K2-Base achieves state-of-the-art performance across the majority of evaluated tasks, establishing it as a leading foundation model in the open-source landscape.\\n\\n##### General Language Understanding\\n\\nKimi-K2-Base achieves state-of-the-art performance on 10 out of 12 English language benchmarks. Notable results include MMLU (87.79%), MMLU-Pro (69.17%), MMLU-Redux (90.17%), SuperGPQA (44.67%), and SimpleQA (35.25%), significantly outperforming all baselines.\\n\\n##### Coding Capabilities\\n\\nOn coding benchmarks, Kimi-K2-Base sets new standards with leading performance across all metrics. It achieves 74.00% on CRUXEval-I-cot, 83.50% on CRUXEval-O-cot, 26.29% on LiveCodeBench v6, and 80.33% on EvalPlus, demonstrating superior code generation and comprehension abilities, particularly in scenarios requiring step-by-step reasoning.\\n\\n##### Mathematical Reasoning\\n\\nKimi-K2-Base exhibits exceptional mathematical capabilities, leading on three out of four benchmarks: MATH (70.22%), GSM8K (92.12%), and GSM8K-Platinum (94.21%). It maintains competitive performance on CMATH (90.26%), narrowly behind DeepSeek-V3-Base (90.53%). These results highlight the model\\u2019s robust mathematical problem-solving abilities across varying difficulty levels.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 17,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\nChinese Language Understanding The model demonstrates superior multilingual capabilities, achieving state-of-the-art results across all Chinese language benchmarks: C-Eval (92.50%), CMMLU (90.90%), and CSimpleQA (77.57%). These results establish Kimi-K2-Base as a leading model for Chinese language understanding while maintaining strong performance across other languages.\\n\\nTable 4: Performance comparison of Kimi-K2-Base against leading open-source models across diverse tasks.\\n\\n[tbl-0.html](tbl-0.html)\\n\\n# 4.3 Safety Evaluation\\n\\n# 4.3.1 Experiment Settings\\n\\nWe conducted red-teaming evaluations on Kimi K2 compare with other open-source LLMs. The evaluation covered a range of attack scenarios\\u2014including harmful content, privacy content, and security content, as well as different attack strategies such as prompt injection and iterative jailbreak.\\n\\nWe choose Promptfoo to generate adversarial prompts and analyze the responses. By this way, we can evaluate model in a scalable ways.\\n\\nModel Selection We compare Kimi K2 with three other open-source LLMs: DeepSeek-V3, DeepSeek-R1, and Qwen3.\\n\\nPromptfoo Settings Table 5 lists plugins and strategies evaluated, with each plugin paired with all strategies to assess their performance.\\n\\nTest Case Count Given the inherent non-determinism of large language model inference, single-pass outputs may exhibit variability. To account for this, we generated 3 attack prompts per plugin for each strategy.\\n\\nPrompt Language Settings We pre-tested the language compatibility for each plugin-strategy combination. Some plugins support both English and Chinese, while others only support English. For combinations that support both, we generated 3 prompts in each language, resulting in 6 prompts per combination.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [\n",
      "                {\n",
      "                    \"id\": \"tbl-0.html\",\n",
      "                    \"content\": \"<table><tr><td></td><td>Benchmark (Metric)</td><td>#Shots</td><td>Kimi-K2-Base</td><td>DeepSeek-V3-Base</td><td>Llama4-Maverick-Base</td><td>Qwen2.5-72B-Base</td></tr><tr><td></td><td>Architecture</td><td>-</td><td>MoE</td><td>MoE</td><td>MoE</td><td>Dense</td></tr><tr><td></td><td># Activated Params</td><td>-</td><td>32B</td><td>37B</td><td>17B</td><td>72B</td></tr><tr><td></td><td># Total Params</td><td>-</td><td>1043B</td><td>671B</td><td>400B</td><td>72B</td></tr><tr><td rowspan=\\\"12\\\">English</td><td>MMLU</td><td>5-shots</td><td>87.79</td><td>87.10</td><td>84.87</td><td>86.08</td></tr><tr><td>MMLU-pro</td><td>5-shots</td><td>69.17</td><td>60.59</td><td>63.47</td><td>62.80</td></tr><tr><td>MMLU-reflux</td><td>5-shots</td><td>90.17</td><td>89.53</td><td>88.18</td><td>87.77</td></tr><tr><td>SuperGPQA</td><td>5-shots</td><td>44.67</td><td>39.20</td><td>38.84</td><td>34.23</td></tr><tr><td>GPQA-Diamond(avg@s)</td><td>5-shots</td><td>48.11</td><td>50.51</td><td>49.43</td><td>40.78</td></tr><tr><td>SimpleQA</td><td>5-shots</td><td>35.25</td><td>26.49</td><td>23.74</td><td>10.31</td></tr><tr><td>TriviaQA</td><td>5-shots</td><td>85.09</td><td>84.11</td><td>79.25</td><td>76.03</td></tr><tr><td>BBH</td><td>3-shots</td><td>88.71</td><td>88.37</td><td>87.10</td><td>84.09</td></tr><tr><td>HellaSwag</td><td>5-shots</td><td>94.60</td><td>89.44</td><td>86.02</td><td>95.27</td></tr><tr><td>AGIEval</td><td>-</td><td>84.23</td><td>81.57</td><td>67.55</td><td>76.87</td></tr><tr><td>ARC-Challenge</td><td>0-shot</td><td>95.73</td><td>93.77</td><td>94.03</td><td>95.56</td></tr><tr><td>WinoGrande</td><td>5-shots</td><td>85.32</td><td>84.21</td><td>77.58</td><td>84.14</td></tr><tr><td rowspan=\\\"4\\\">Code</td><td>CRUXEval-I-cot</td><td>0-shots</td><td>74.00</td><td>62.75</td><td>67.13</td><td>61.12</td></tr><tr><td>CRUXEval-O-cot</td><td>0-shots</td><td>83.50</td><td>75.25</td><td>75.88</td><td>66.13</td></tr><tr><td>LiveCodeBench(v6)</td><td>1-shots</td><td>26.29</td><td>24.57</td><td>25.14</td><td>22.29</td></tr><tr><td>EvalPlus</td><td>-</td><td>80.33</td><td>65.61</td><td>65.48</td><td>66.04</td></tr><tr><td rowspan=\\\"4\\\">Math</td><td>MATH</td><td>4-shots</td><td>70.22</td><td>61.70</td><td>63.02</td><td>62.68</td></tr><tr><td>GSM8k</td><td>8-shots</td><td>92.12</td><td>91.66</td><td>86.35</td><td>90.37</td></tr><tr><td>GSM8k-platinum</td><td>8-shots</td><td>94.21</td><td>93.38</td><td>88.83</td><td>92.47</td></tr><tr><td>CMATH</td><td>6-shots</td><td>90.26</td><td>90.53</td><td>88.07</td><td>86.98</td></tr><tr><td rowspan=\\\"3\\\">Chinese</td><td>C-Eval</td><td>5-shots</td><td>92.50</td><td>90.04</td><td>80.91</td><td>90.86</td></tr><tr><td>CMMLU</td><td>5-shots</td><td>90.90</td><td>88.84</td><td>81.24</td><td>90.55</td></tr><tr><td>CSimpleQA</td><td>5-shots</td><td>77.57</td><td>72.13</td><td>53.47</td><td>50.53</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                }\n",
      "            ],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://github.com/promptfoo/promptfoo\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 18,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\nTable 5: Enabled Plugins and Strategies\\n\\n[tbl-1.html](tbl-1.html)\\n\\nManual Review We incorporated human review into the evaluation process. To minimize subjectivity problem, we conducted multiple rounds of review and assigned the same reviewer to evaluate all cases within a given test set to ensure consistency and reduce variability in judgment.\\n\\n# 4.3.2 Safety Evaluation Results\\n\\nTable 6 presents the passing rates of different models under various plugin-strategy combinations.\\n\\nTable 6: Safety Evaluation Results\\n\\n[tbl-2.html](tbl-2.html)\\n\\nWithout targeted optimization for specific evaluation scenarios, the passing rate of some complex cases (e.g., Harmful-Iterative Jailbreak) was relatively higher compared to other models.\\n\\nAcross different attack strategies, the models exhibited varying trends. Under the Base64 strategy, passing rates generally approached or reached  $100\\\\%$ , suggesting that encoding transformations had minimal impact on the models'\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [\n",
      "                {\n",
      "                    \"id\": \"tbl-1.html\",\n",
      "                    \"content\": \"<table><tr><td rowspan=\\\"5\\\">Plugin</td><td>Harmful</td><td>Graphic Content, Harassment and Bullying, Hate Speech, Insults, Profanity, Radicalization, Self Harm, Sexual Content, ToxicChat</td></tr><tr><td>Criminal</td><td>Chemical&amp;Biological Weapons, Child Exploitation, Copyright Violations, Cybercrime, Illegal Activities, Illegal Drugs, Indiscriminate Weapons, Intellectual Property Violation, Non-Violent Crime, Violent Crime, Sex Crimes</td></tr><tr><td>Misinformation</td><td>Competitor Endorsement, Unsupervised Contracts, Excessive Agency, Hallucination, Misin-formation and Disinformation, Specialized Advice, Unsafe Practices, Imitation, Overreliance, Political Opinions, Religious Sensitivity</td></tr><tr><td>Privacy</td><td>Privacy Violation, PII in API/Database, Direct PII Exposure, PII in Session Data, PII via Social Engineering</td></tr><tr><td>Security</td><td>ASCII Smuggling, CyberSecEval, Harmbench, Debug Access, Divergent Repetition, DoNotAnswer, Malicious Code, Pliny, Prompt Extraction, Reasoning DoS, Tool Discovery</td></tr><tr><td>Strategy</td><td colspan=\\\"2\\\">Basic, Prompt Injection, Iterative Jailbreak, Crescendo</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                },\n",
      "                {\n",
      "                    \"id\": \"tbl-2.html\",\n",
      "                    \"content\": \"<table><tr><td>Plugin</td><td>Strategy</td><td>Kimi-K2-Instruct</td><td>DeepSeek-V3-0324</td><td>DeepSeek-R1</td><td>Qwen3-235B-A22B</td></tr><tr><td rowspan=\\\"5\\\">Harmful</td><td>Basic</td><td>98.04</td><td>90.45</td><td>99.02</td><td>98.53</td></tr><tr><td>Base64</td><td>100</td><td>90.20</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>93.14</td><td>100</td><td>95.10</td><td>99.02</td></tr><tr><td>Iterative Jailbreak</td><td>92.16</td><td>66.67</td><td>72.55</td><td>74.51</td></tr><tr><td>Crescendo</td><td>64.71</td><td>64.71</td><td>80.39</td><td>86.27</td></tr><tr><td rowspan=\\\"5\\\">Criminal</td><td>Basic</td><td>100</td><td>99.62</td><td>95.45</td><td>99.24</td></tr><tr><td>Base64</td><td>96.97</td><td>89.39</td><td>84.85</td><td>98.48</td></tr><tr><td>Prompt Injection</td><td>75.76</td><td>91.67</td><td>69.70</td><td>98.47</td></tr><tr><td>Iterative Jailbreak</td><td>57.57</td><td>21.21</td><td>25.76</td><td>53.03</td></tr><tr><td>Crescendo</td><td>56.06</td><td>31.81</td><td>42.42</td><td>59.09</td></tr><tr><td rowspan=\\\"5\\\">Misinformation</td><td>Basic</td><td>97.28</td><td>92.57</td><td>92.46</td><td>94.84</td></tr><tr><td>Base64</td><td>98.48</td><td>90.48</td><td>96.83</td><td>93.65</td></tr><tr><td>Prompt Injection</td><td>98.39</td><td>86.51</td><td>93.65</td><td>93.65</td></tr><tr><td>Iterative Jailbreak</td><td>63.97</td><td>53.97</td><td>84.13</td><td>69.84</td></tr><tr><td>Crescendo</td><td>85.71</td><td>55.56</td><td>88.89</td><td>84.13</td></tr><tr><td rowspan=\\\"5\\\">Privacy</td><td>Basic</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Base64</td><td>100</td><td>100</td><td>100</td><td>100</td></tr><tr><td>Prompt Injection</td><td>88.33</td><td>98.33</td><td>100</td><td>91.67</td></tr><tr><td>Iterative Jailbreak</td><td>76.67</td><td>100</td><td>93.33</td><td>96.67</td></tr><tr><td>Crescendo</td><td>96.67</td><td>100</td><td>96.67</td><td>100</td></tr><tr><td rowspan=\\\"5\\\">Security</td><td>Basic</td><td>77.84</td><td>75.57</td><td>70.46</td><td>90.09</td></tr><tr><td>Base64</td><td>82.93</td><td>82.93</td><td>63.41</td><td>95.12</td></tr><tr><td>Prompt Injection</td><td>87.80</td><td>97.56</td><td>65.85</td><td>84.13</td></tr><tr><td>Iterative Jailbreak</td><td>43.90</td><td>60.97</td><td>43.90</td><td>78.04</td></tr><tr><td>Crescendo</td><td>68.29</td><td>87.80</td><td>68.29</td><td>87.80</td></tr></table>\",\n",
      "                    \"format_\": \"html\"\n",
      "                }\n",
      "            ],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 19,\n",
      "            \"markdown\": \"basic robustness. In contrast, the Crescendo strategy led to a general drop in passing rates, indicating stronger adversarial effectiveness.\\n\\nIn addition, complex attack strategies do not always outperform basic prompts. Some originally adversarial prompts may lose their intended meaning after multiple rounds of transformation, rendering the resulting model outputs less meaningful.\\n\\nAutomated Red-teaming Limitations Due to the involvement of human review, the evaluation results inevitably contain a degree of subjectivity. Additionally, certain plugin types involve API misuse or external tool invocation, which are more suitable for evaluating agent models with tool-calling capabilities. In the context of base LLMs, such tests may have limited relevance.\\n\\n## 5 Limitations\\n\\nIn our internal tests, we have identified some limitations in current Kimi K2 models. When dealing with hard reasoning tasks or unclear tool definition, the model may generate excessive tokens, sometimes leading to truncated outputs or incomplete tool calls. Additionally, performance may decline on certain tasks if tool use is unnecessarily enabled. When building complete software projects, the success rate of one-shot prompting is not as good as using K2 under an agentic coding framework. We are working to address these issues in future releases and looking forward to more feedbacks.\\n\\n## 6 Conclusions\\n\\nWe introduced Kimi K2, a 1T-parameter open-weight MoE model built for agentic intelligence. Leveraging the token-efficient MuonClip optimizer and a 15.5T-token high-quality dataset, Kimi K2 achieves stable, scalable pre-training. Post-training combines large-scale synthetic tool-use data with a unified RL framework using both verifiable rewards and self-critic feedbacks. Kimi K2 sets new state-of-the-art on agentic and reasoning benchmarks, establishing itself as the most capable open-weight LLM to date.\\n\\n## 7 Acknowledgments\\n\\nWe would like to acknowledge the valuable support provided by the OpenHands and Multi-SWE-bench teams in evaluating the SWE-bench Verified and Multi-SWE-bench experimental results.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 20,\n",
      "            \"markdown\": \"References\\n\\n- [1] Jacob Austin et al. *Program Synthesis with Large Language Models*. 2021. arXiv: 2108.07732 [cs.PL]. URL: https://arxiv.org/abs/2108.07732.\\n- [2] Yushi Bai et al. *LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks*. 2025. arXiv: 2412.15204 [cs.CL]. URL: https://arxiv.org/abs/2412.15204.\\n- [3] Victor Barres et al. *$\\\\tau^{2}$-Bench: Evaluating Conversational Agents in a Dual-Control Environment*. 2025. arXiv: 2506.07982 [cs.AI]. URL: https://arxiv.org/abs/2506.07982.\\n- [4] Stella Biderman et al. \\u201cLessons from the trenches on reproducible evaluation of language models\\u201d. In: *arXiv preprint arXiv:2405.14782* (2024).\\n- [5] Federico Cassano et al. \\u201cMultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation\\u201d. In: *IEEE Transactions on Software Engineering* 49.7 (2023), pp. 3675\\u20133691. doi: 10.1109/TSE.2023.3267446.\\n- [6] Chen Chen et al. \\u201cACEBench: Who Wins the Match Point in Tool Learning?\\u201d In: *arXiv e-prints* (2025), arXiv\\u20132501.\\n- [7] Mark Chen et al. \\u201cEvaluating Large Language Models Trained on Code\\u201d. In: (2021). arXiv: 2107.03374 [cs.LG].\\n- [8] Peter Clark et al. \\u201cThink you have solved question answering? try arc, the ai2 reasoning challenge\\u201d. In: *arXiv preprint arXiv:1803.05457* (2018).\\n- [9] Karl Cobbe et al. *Training Verifiers to Solve Math Word Problems*. 2021. arXiv: 2110.14168 [cs.LG]. URL: https://arxiv.org/abs/2110.14168.\\n- [10] DeepSeek-AI. *DeepSeek-V3 Technical Report*. 2024. arXiv: 2412.19437 [cs.CL]. URL: https://arxiv.org/abs/2412.19437.\\n- [11] Mostafa Dehghani et al. \\u201cScaling vision transformers to 22 billion parameters\\u201d. In: *International conference on machine learning*. PMLR. 2023, pp. 7480\\u20137512.\\n- [12] Guanting Dong et al. *Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models*. 2024. arXiv: 2406.13542 [cs.CL]. URL: https://arxiv.org/abs/2406.13542.\\n- [13] Xinrun Du et al. \\u201cSupergpqa: Scaling llm evaluation across 285 graduate disciplines\\u201d. In: *arXiv preprint arXiv:2502.14739* (2025).\\n- [14] Dheeru Dua et al. \\u201cDROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs\\u201d. In: *CoRR* abs/1903.00161 (2019). arXiv: 1903.00161. URL: http://arxiv.org/abs/1903.00161.\\n- [15] Kazuki Fujii et al. *Rewriting Pre-Training Data Boosts LLM Performance in Math and Code*. 2025. arXiv: 2505.02881 [cs.LG]. URL: https://arxiv.org/abs/2505.02881.\\n- [16] Paul Gauthier. *Aider LLM Leaderboards*. https://aider.chat/docs/leaderboards/. 2025.\\n- [17] Aryo Pradipta Gema et al. \\u201cAre we done with mmlu?\\u201d In: *arXiv preprint arXiv:2406.04127* (2024).\\n- [18] Alex Gu et al. \\u201cCruxeval: A benchmark for code reasoning, understanding and execution\\u201d. In: *arXiv preprint arXiv:2401.03065* (2024).\\n- [19] Daya Guo et al. \\u201cDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning\\u201d. In: *arXiv preprint arXiv:2501.12948* (2025).\\n- [20] Zhicheng Guo et al. \\u201cStableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models\\u201d. In: *arXiv preprint arXiv:2403.07714* (2025).\\n- [21] Aaron Harlap et al. \\u201cPipedream: Fast and efficient pipeline parallel dnn training\\u201d. In: *arXiv preprint arXiv:1806.03377* (2018).\\n- [22] Y He et al. \\u201cChinese simpleqa: A chinese factuality evaluation for large language models, 2024a\\u201d. In: *URL https://arxiv. org/abs/2411.07140* ().\\n- [23] Dan Hendrycks et al. \\u201cMeasuring massive multitask language understanding\\u201d. In: *arXiv preprint arXiv:2009.03300* (2020).\\n- [24] Dan Hendrycks et al. *Measuring Mathematical Problem Solving With the MATH Dataset*. 2021. arXiv: 2103.03874 [cs.LG]. URL: https://arxiv.org/abs/2103.03874.\\n- [25] Shengding Hu et al. \\u201cMinicpm: Unveiling the potential of small language models with scalable training strategies\\u201d. In: *arXiv preprint arXiv:2404.06395* (2024).\\n- [26] Jiaxin Huang et al. \\u201cLarge language models can self-improve\\u201d. In: *arXiv preprint arXiv:2210.11610* (2022).\\n- [27] Siming Huang et al. *OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models*. 2025. arXiv: 2411.04905 [cs.CL]. URL: https://arxiv.org/abs/2411.04905.\\n-\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2108.07732\",\n",
      "                \"https://arxiv.org/abs/2108.07732\",\n",
      "                \"https://arxiv.org/abs/2412.15204\",\n",
      "                \"https://arxiv.org/abs/2412.15204\",\n",
      "                \"https://arxiv.org/abs/2506.07982\",\n",
      "                \"https://arxiv.org/abs/2506.07982\",\n",
      "                \"https://doi.org/10.1109/TSE.2023.3267446\",\n",
      "                \"https://doi.org/10.1109/TSE.2023.3267446\",\n",
      "                \"https://arxiv.org/abs/2107.03374\",\n",
      "                \"https://arxiv.org/abs/2107.03374\",\n",
      "                \"https://arxiv.org/abs/2110.14168\",\n",
      "                \"https://arxiv.org/abs/2110.14168\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2412.19437\",\n",
      "                \"https://arxiv.org/abs/2406.13542\",\n",
      "                \"https://arxiv.org/abs/2406.13542\",\n",
      "                \"https://arxiv.org/abs/1903.00161\",\n",
      "                \"http://arxiv.org/abs/1903.00161\",\n",
      "                \"https://arxiv.org/abs/2505.02881\",\n",
      "                \"https://arxiv.org/abs/2505.02881\",\n",
      "                \"https://aider.chat/docs/leaderboards/\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2103.03874\",\n",
      "                \"https://arxiv.org/abs/2411.04905\",\n",
      "                \"https://arxiv.org/abs/2411.04905\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 21,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\n[28] Yanping Huang et al. \\\"Gpipe: Efficient training of giant neural networks using pipeline parallelism\\\". In: Advances in neural information processing systems 32 (2019).\\n[29] Yuzhen Huang et al. C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. 2023. arXiv: 2305.08322 [cs.CL]. URL: https://arxiv.org/abs/2305.08322.\\n[30] Alon Jacovi et al. The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input. 2025. arXiv: 2501.03200 [cs.CL]. URL: https://arxiv.org/abs/2501.03200.\\n[31] Naman Jain et al. \\\"Livecodebench: Holistic and contamination free evaluation of large language models for code\\\". In: arXiv preprint arXiv:2403.07974 (2024).\\n[32] Carlos E Jimenez et al. \\\"SWE-bench: Can Language Models Resolve Real-world Github Issues?\\\" In: The Twelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=VTF8yNQM66.\\n[33] Keller Jordan et al. Muon: An optimizer for hidden layers in neural networks. 2024. URL: https://kellerjordan.github.io/posts/muon/.\\n[34] Mandar Joshi et al. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. 2017. arXiv: 1705.03551 [cs.CL]. URL: https://arxiv.org/abs/1705.03551.\\n[35] Kimi Team. \\\"Kimi k1. 5: Scaling reinforcement learning with llms\\\". In: arXiv preprint arXiv:2501.12599 (2025).\\n[36] Diederik P. Kingma and Jimmy Ba. \\u201cAdam: A Method for Stochastic Optimization\\u201d. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Ed. by Yoshua Bengio and Yann LeCun. 2015. URL: http://arxiv.org/abs/1412.6980.\\n[37] Satyapriya Krishna et al. Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation. 2025. arXiv: 2409.12941 [cs.CL]. URL: https://arxiv.org/abs/2409.12941.\\n[38] Joel Lamy-Poirier. \\u201cBreadth-first pipeline parallelism\\u201d. In: Proceedings of Machine Learning and Systems 5 (2023), pp. 48\\u201367.\\n[39] Dmitry Lepikhin et al. \\\"Gshard: Scaling giant models with conditional computation and automatic sharding\\\". In: arXiv preprint arXiv:2006.16668 (2020).\\n[40] Haonan Li et al. CMMLU: Measuring massive multitask language understanding in Chinese. 2024. arXiv: 2306.09212 [cs.CL]. URL: https://arxiv.org/abs/2306.09212.\\n[41] Jia Li et al. \\\"Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions\\\". In: Hugging Face repository 13.9 (2024), p. 9.\\n[42] Tianle Li et al. \\\"From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline\\\". In: arXiv preprint arXiv:2406.11939 (2024).\\n[43] Bill Yuchen Lin et al. ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning. 2025. arXiv: 2502.01100 [cs.AI]. URL: https://arxiv.org/abs/2502.01100.\\n[44] Aixin Liu et al. \\\"Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model\\\". In: arXiv preprint arXiv:2405.04434 (2024).\\n[45] Jiawei Liu et al. \\\"Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation\\\". In: Advances in Neural Information Processing Systems 36 (2023), pp. 21558-21572.\\n[46] Jingyuan Liu et al. \\\"Muon is scalable for LLM training\\\". In: arXiv preprint arXiv:2502.16982 (2025).\\n[47] Ziming Liu et al. \\\"Hanayo: Harnessing Wave-like Pipeline Parallelism for Enhanced Large Model Training Efficiency\\\". In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. SC '23. ACM, Nov. 2023, pp. 1-13. DOI: 10.1145/3581784.3607073. URL: http://dx.doi.org/10.1145/3581784.3607073.\\n[48] Ilya Loshchilov and Frank Hutter. \\\"Decoupled Weight Decay Regularization\\\". In: International Conference on Learning Representations. 2019. URL: https://openreview.net/forum?id=Bkg6RiCqY7.\\n[49] Jan Ludziejewski et al. OpenAI Gym. 2025. arXiv: 2502.05172 [cs.LG]. URL: https://arxiv.org/abs/2502.05172.\\n[50] Samuel Miserendino et al. \\\"SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?\\\" In: arXiv preprint arXiv:2502.12115 (2025).\\n[51] Arindam Mitra et al. \\\"Agentinstruct: Toward generative teaching with agentic flows\\\". In: arXiv preprint arXiv:2407.03502 (2024).\\n[52] Ivan Moshkov et al. \\\"Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset\\\". In: arXiv preprint arXiv:2504.16891 (2025).\\n[53] Deepak Narayanan et al. \\\"Efficient large-scale language model training ongpu clusters using megatron-lm\\\". In: Proceedings of the international conference for high performance computing, networking, storage and analysis. 2021, pp. 1-15.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2305.08322\",\n",
      "                \"https://arxiv.org/abs/2305.08322\",\n",
      "                \"https://arxiv.org/abs/2501.03200\",\n",
      "                \"https://arxiv.org/abs/2501.03200\",\n",
      "                \"https://openreview.net/forum?id=VTF8yNQM66\",\n",
      "                \"https://openreview.net/forum?id=VTF8yNQM66\",\n",
      "                \"https://kellerjordan.github.io/posts/muon/\",\n",
      "                \"https://kellerjordan.github.io/posts/muon/\",\n",
      "                \"https://arxiv.org/abs/1705.03551\",\n",
      "                \"https://arxiv.org/abs/1705.03551\",\n",
      "                \"http://arxiv.org/abs/1412.6980\",\n",
      "                \"https://arxiv.org/abs/2409.12941\",\n",
      "                \"https://arxiv.org/abs/2409.12941\",\n",
      "                \"https://arxiv.org/abs/2306.09212\",\n",
      "                \"https://arxiv.org/abs/2306.09212\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://arxiv.org/abs/2502.01100\",\n",
      "                \"https://doi.org/10.1145/3581784.3607073\",\n",
      "                \"http://dx.doi.org/10.1145/3581784.3607073\",\n",
      "                \"http://dx.doi.org/10.1145/3581784.3607073\",\n",
      "                \"https://openreview.net/forum?id=Bkg6RiCqY7\",\n",
      "                \"https://arxiv.org/abs/2502.05172\",\n",
      "                \"https://arxiv.org/abs/2502.05172\",\n",
      "                \"https://arxiv.org/abs/2502.05172\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 22,\n",
      "            \"markdown\": \"[54] Long Ouyang et al. \\u201cTraining language models to follow instructions with human feedback\\u201d. In: Advances in neural information processing systems 35 (2022), pp. 27730\\u201327744.\\n- [55] Bowen Peng et al. \\u201cYarn: Efficient context window extension of large language models\\u201d. In: arXiv preprint arXiv:2309.00071 (2023).\\n- [56] Long Phan et al. Humanity\\u2019s Last Exam. 2025. arXiv: 2501.14249 [cs.LG]. URL: https://arxiv.org/abs/2501.14249.\\n- [57] Penghui Qi et al. \\u201cZero bubble pipeline parallelism\\u201d. In: arXiv preprint arXiv:2401.10241 (2023).\\n- [58] Yujia Qin et al. \\u201cToolllm: Facilitating large language models to master 16000+ real-world apis\\u201d. In: arXiv preprint arXiv:2307.16789 (2023).\\n- [59] Qwen et al. Qwen2.5 Technical Report. 2025. arXiv: 2412.15115 [cs.CL]. URL: https://arxiv.org/abs/2412.15115.\\n- [60] Samyam Rajbhandari et al. \\u201cZero: Memory optimizations toward training trillion parameter models\\u201d. In: SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE. 2020, pp. 1\\u201316.\\n- [61] David Rein et al. \\u201cGpqa: A graduate-level google-proof q&a benchmark\\u201d. In: First Conference on Language Modeling. 2024.\\n- [62] Keisuke Sakaguchi et al. \\u201cWinogrande: An adversarial winograd schema challenge at scale\\u201d. In: Communications of the ACM 64.9 (2021), pp. 99\\u2013106.\\n- [63] David Silver and Richard S Sutton. \\u201cWelcome to the era of experience\\u201d. In: Google AI 1 (2025).\\n- [64] Ved Sirdeshmukh et al. MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs. 2025. arXiv: 2501.17399 [cs.CL]. URL: https://arxiv.org/abs/2501.17399.\\n- [65] Giulio Starace et al. \\u201cPaperBench: Evaluating AI\\u2019s Ability to Replicate AI Research\\u201d. In: arXiv preprint arXiv:2504.01848 (2025).\\n- [66] Hao Sun et al. ZeroSearch: Incentivize the Search Capability of LLMs without Searching. 2025. arXiv: 2505.04588 [cs.CL]. URL: https://arxiv.org/abs/2505.04588.\\n- [67] Mirac Suzgun et al. Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. 2022. arXiv: 2210.09261 [cs.CL]. URL: https://arxiv.org/abs/2210.09261.\\n- [68] Manveer Singh Tamber et al. \\u201cBenchmarking LLM Faithfulness in RAG with Evolving Leaderboards\\u201d. In: arXiv preprint arXiv:2505.04847 (2025).\\n- [69] Gemma Team et al. \\u201cGemma 2: Improving open language models at a practical size\\u201d. In: arXiv preprint arXiv:2408.00118 (2024).\\n- [70] LlaMA Team. The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation \\u2014 ai.meta.com. https://ai.meta.com/blog/llama-4-multimodal-intelligence/. [Accessed 15-07-2025].\\n- [71] The Terminal-Bench Team. Terminal-Bench: A Benchmark for AI Agents in Terminal Environments. Apr. 2025. URL: https://github.com/laude-institute/terminal-bench.\\n- [72] Ashish Vaswani et al. \\u201cAttention is All you Need\\u201d. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon et al. Vol. 30. Curran Associates, Inc., 2017. URL: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\n- [73] Vectara. Hallucination Evaluation Model (Revision 7437011). 2024. URL: https://huggingface.co/vectara/hallucination_evaluation_model.\\n- [74] Joshua Vendrow et al. \\u201cDo large language model benchmarks test reliability?\\u201d In: arXiv preprint arXiv:2502.03461 (2025).\\n- [75] Yizhong Wang et al. \\u201cSelf-instruct: Aligning language models with self-generated instructions\\u201d. In: arXiv preprint arXiv:2212.10560 (2022).\\n- [76] Yubo Wang et al. MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark. 2024. arXiv: 2406.01574 [cs.CL]. URL: https://arxiv.org/abs/2406.01574.\\n- [77] Zhexu Wang et al. OJBench: A Competition Level Code Benchmark For Large Language Models. 2025. arXiv: 2506.16395 [cs.CL]. URL: https://arxiv.org/abs/2506.16395.\\n- [78] Jason Wei et al. \\u201cMeasuring short-form factuality in large language models\\u201d. In: arXiv preprint arXiv:2411.04368 (2024).\\n- [79] Tianwen Wei et al. CMATH: Can Your Language Model Pass Chinese Elementary School Math Test? 2023. arXiv: 2306.16636 [cs.CL]. URL: https://arxiv.org/abs/2306.16636.\\n- [80] Colin White et al. \\u201cLiveBench: A Challenging, Contamination-Free LLM Benchmark\\u201d. In: The Thirteenth International Conference on Learning Representations. 2025.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2501.14249\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2412.15115\",\n",
      "                \"https://arxiv.org/abs/2501.17399\",\n",
      "                \"https://arxiv.org/abs/2501.17399\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2505.04588\",\n",
      "                \"https://arxiv.org/abs/2210.09261\",\n",
      "                \"https://arxiv.org/abs/2210.09261\",\n",
      "                \"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\",\n",
      "                \"https://github.com/laude-institute/terminal-bench\",\n",
      "                \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\n",
      "                \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\",\n",
      "                \"https://huggingface.co/vectara/hallucination_evaluation_model\",\n",
      "                \"https://huggingface.co/vectara/hallucination_evaluation_model\",\n",
      "                \"https://arxiv.org/abs/2406.01574\",\n",
      "                \"https://arxiv.org/abs/2406.01574\",\n",
      "                \"https://arxiv.org/abs/2506.16395\",\n",
      "                \"https://arxiv.org/abs/2506.16395\",\n",
      "                \"https://arxiv.org/abs/2306.16636\",\n",
      "                \"https://arxiv.org/abs/2306.16636\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        },\n",
      "        {\n",
      "            \"index\": 23,\n",
      "            \"markdown\": \"Kimi K2\\n\\nTECHNICAL REPORT\\n\\n[81] Mitchell Wortsman et al. \\\"Small-scale proxies for large-scale transformer training instabilities, 2023\\\". In: URL https://arxiv.org/abs/2309.14322 ().\\n[82] Can Xu et al. WizardLM: Empowering large pre-trained language models to follow complex instructions. 2025. arXiv: 2304.12244 [cs.CL]. URL: https://arxiv.org/abs/2304.12244.\\n[83] Zhangchen Xu et al. KodCode: A Diverse, Challenging, and Verifiable Synthetic Dataset for Coding. 2025. arXiv: 2503.02951 [cs.LG]. URL: https://arxiv.org/abs/2503.02951.\\n[84] John Yang et al. SWE-smith: Scaling Data for Software Engineering Agents. 2025. arXiv: 2504.21798 [cs.SE]. URL: https://arxiv.org/abs/2504.21798.\\n[85] Shunyu Yao et al. \\\"tau-bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains\\\". In: arXiv preprint arXiv:2406.12045 (2024).\\n[86] Daoguang Zan et al. \\\"Multi-swe-bench: A multilingual benchmark for issue resolving\\\". In: arXiv preprint arXiv:2504.02605 (2025).\\n[87] Eric Zelikman et al. \\\"Star: Bootstrapping reasoning with reasoning\\\". In: Advances in Neural Information Processing Systems 35 (2022), pp. 15476-15488.\\n[88] Rowan Zellers et al. \\u201cHellaswag: Can a machine really finish your sentence?\\u201d In: arXiv preprint arXiv:1905.07830 (2019).\\n[89] Wanjun Zhong et al. \\\"Agieval: A human-centric benchmark for evaluating foundation models\\\". In: arXiv preprint arXiv:2304.06364 (2023).\\n[90] Jeffrey Zhou et al. \\\"Instruction-Following Evaluation for Large Language Models\\\". In: ArXiv abs/2311.07911 (2023). URL: https://arxiv.org/abs/2311.07911.\\n[91] Qin Zhu et al. AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models. 2025. arXiv: 2502.16906 [cs.CL]. URL: https://arxiv.org/abs/2502.16906.\",\n",
      "            \"images\": [],\n",
      "            \"dimensions\": {\n",
      "                \"dpi\": 200,\n",
      "                \"height\": 2200,\n",
      "                \"width\": 1700\n",
      "            },\n",
      "            \"tables\": [],\n",
      "            \"hyperlinks\": [\n",
      "                \"https://arxiv.org/abs/2304.12244\",\n",
      "                \"https://arxiv.org/abs/2304.12244\",\n",
      "                \"https://arxiv.org/abs/2503.02951\",\n",
      "                \"https://arxiv.org/abs/2503.02951\",\n",
      "                \"https://arxiv.org/abs/2504.21798\",\n",
      "                \"https://arxiv.org/abs/2504.21798\",\n",
      "                \"https://arxiv.org/abs/2311.07911\",\n",
      "                \"https://arxiv.org/abs/2502.16906\",\n",
      "                \"https://arxiv.org/abs/2502.16906\"\n",
      "            ],\n",
      "            \"header\": null,\n",
      "            \"footer\": null\n",
      "        }\n",
      "    ],\n",
      "    \"model\": \"mistral-ocr-latest\",\n",
      "    \"usage_info\": {\n",
      "        \"pages_processed\": 8,\n",
      "        \"doc_size_bytes\": 6335018\n",
      "    },\n",
      "    \"document_annotation\": \"{\\n  \\\"summary\\\": \\\"The document presents a comprehensive evaluation of the Kimi-K2-Instruct and Kimi-K2-Base models, highlighting their superior performance across various benchmarks and tasks. Kimi-K2-Instruct excels in multi-turn tool-use benchmarks, general capabilities, and open-ended evaluations, outperforming all baselines in grounded, controlled, and agent-driven tool orchestration. It also shows strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. Kimi-K2-Base demonstrates state-of-the-art performance in general language understanding, coding capabilities, mathematical reasoning, and Chinese language understanding. The models were evaluated using diverse benchmarks and configurations, ensuring fair comparisons. Safety evaluations were conducted using red-teaming techniques, with Kimi K2 showing robust performance against various attack strategies. The document also acknowledges some limitations and areas for future improvement, such as handling hard reasoning tasks and unclear tool definitions. Overall, Kimi K2 is positioned as a leading open-weight model for agentic intelligence.\\\",\\n  \\\"authors\\\": [\\n    \\\"Kimi-K2 Team\\\"\\n  ]\\n}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from mistralai.extra import response_format_from_pydantic_model\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Initialize Mistral client with API key\n",
    "from mistralai import Mistral\n",
    "load_dotenv()\n",
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "# OCR Call with Annotations\n",
    "annotations_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    pages=list(range(16, 24)), # Document Annotations has a limit of 8 pages, we recommend spliting your documents when using it; bbox annotations does not have the same limit\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "    },\n",
    "    bbox_annotation_format=response_format_from_pydantic_model(BBox),\n",
    "    document_annotation_format=response_format_from_pydantic_model(Document),\n",
    "    include_image_base64=True, # Let's also include the images in the response\n",
    "    table_format=\"html\"\n",
    "  )\n",
    "\n",
    "# Convert response to JSON format\n",
    "response_dict = json.loads(annotations_response.model_dump_json())\n",
    "\n",
    "print(json.dumps(response_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a27a3",
   "metadata": {},
   "source": [
    "Let's split the pdf into 8 pages batches first as they advice to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8df55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9499b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 saved to RAG/documents/chunks/chunk_0.pdf\n",
      "Chunk 1 saved to RAG/documents/chunks/chunk_1.pdf\n",
      "Chunk 2 saved to RAG/documents/chunks/chunk_2.pdf\n",
      "Chunk 3 saved to RAG/documents/chunks/chunk_3.pdf\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "\n",
    "def split_pdf(input_path, chunk_size=8, output_dir=\"RAG/documents/chunks\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    reader = PdfReader(input_path)\n",
    "    for i in range(0, len(reader.pages), chunk_size):\n",
    "        writer = PdfWriter()\n",
    "        for page in reader.pages[i : i + chunk_size]:\n",
    "            writer.add_page(page)\n",
    "        \n",
    "        chunk_filename = f\"chunk_{i//chunk_size}.pdf\"\n",
    "        chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "        with open(chunk_path, \"wb\") as f:\n",
    "            writer.write(f)\n",
    "        print(f\"Chunk {i//chunk_size} saved to {chunk_path}\")\n",
    "\n",
    "split_pdf(\"RAG/documents/KimiK2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6311d",
   "metadata": {},
   "source": [
    "Let's actually parse and annotate tables as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f9052e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: chunk_0.pdf\n",
      "Successfully processed chunk_0.pdf\n",
      "Processing: chunk_1.pdf\n",
      "Successfully processed chunk_1.pdf\n",
      "Processing: chunk_2.pdf\n",
      "Successfully processed chunk_2.pdf\n",
      "Processing: chunk_3.pdf\n",
      "Successfully processed chunk_3.pdf\n"
     ]
    }
   ],
   "source": [
    "chunk_dir = \"RAG/documents/chunks\"\n",
    "responses = []\n",
    "\n",
    "# Sort the list to ensure pages stay in order\n",
    "for chunk_filename in sorted(os.listdir(chunk_dir)):\n",
    "    # Construct the full path\n",
    "    chunk_path = os.path.join(chunk_dir, chunk_filename)\n",
    "    \n",
    "    # Skip directories or non-pdf files if any exist\n",
    "    if not chunk_filename.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    with open(chunk_path, \"rb\") as f:\n",
    "        # Correctly encode the specific chunk\n",
    "        base64_chunk = base64.b64encode(f.read()).decode('utf-8')\n",
    "        print(f\"Processing: {chunk_filename}\")\n",
    "\n",
    "    try:\n",
    "        # OCR Call\n",
    "        annotations_response = client.ocr.process(\n",
    "            model=\"mistral-ocr-latest\",\n",
    "            # Remove the 'pages' limit because the file IS the limit now\n",
    "            document={\n",
    "                \"type\": \"document_url\",\n",
    "                \"document_url\": f\"data:application/pdf;base64,{base64_chunk}\"\n",
    "            },\n",
    "            bbox_annotation_format=response_format_from_pydantic_model(Image),\n",
    "            document_annotation_format=response_format_from_pydantic_model(Document),\n",
    "            include_image_base64=True,\n",
    "            table_format=\"html\"  # take out tables as well\n",
    "        )\n",
    "        \n",
    "        response_dict = annotations_response.model_dump()\n",
    "        # Parse nested JSON strings in document_annotation\n",
    "        if isinstance(response_dict.get(\"document_annotation\"), str):\n",
    "            response_dict[\"document_annotation\"] = json.loads(response_dict[\"document_annotation\"])\n",
    "        # Parse nested JSON strings in image annotations\n",
    "        for page in response_dict.get(\"pages\", []):\n",
    "            for img in page.get(\"images\", []):\n",
    "                if isinstance(img.get(\"image_annotation\"), str):\n",
    "                    img[\"image_annotation\"] = json.loads(img[\"image_annotation\"])\n",
    "\n",
    "        responses.append(response_dict)\n",
    "        print(f\"Successfully processed {chunk_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {chunk_filename}: {e}\")\n",
    "\n",
    "# Save the responses\n",
    "output_path = \"RAG/OCR/responses.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True) # Ensure directory exists\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0251595",
   "metadata": {},
   "source": [
    "Now since we split the document into several parts, our images' and tables' indexes will start over at each chunk, and that will give us repeated indices - we do not want that. So we re-index with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6696d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def reindex_ocr_responses(responses_list: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Re-indexes images and tables across all OCR responses to have globally unique IDs.\n",
    "    Updates both the ID fields in objects and all markdown references.\n",
    "    \"\"\"\n",
    "    global_image_counter = 0\n",
    "    global_table_counter = 0\n",
    "    \n",
    "    for response in responses_list:\n",
    "        for page in response.get(\"pages\", []):\n",
    "            # Create mapping of old IDs to new IDs for this page\n",
    "            image_id_map = {}\n",
    "            table_id_map = {}\n",
    "            \n",
    "            # Re-index images\n",
    "            for img in page.get(\"images\", []):\n",
    "                old_id = img[\"id\"]\n",
    "                # Get file extension\n",
    "                ext = old_id.split('.')[-1] if '.' in old_id else 'jpeg'\n",
    "                new_id = f\"img-{global_image_counter}.{ext}\"\n",
    "                \n",
    "                # Update the image ID in the object\n",
    "                img[\"id\"] = new_id\n",
    "                image_id_map[old_id] = new_id\n",
    "                global_image_counter += 1\n",
    "            \n",
    "            # Re-index tables\n",
    "            for table in page.get(\"tables\", []):\n",
    "                old_id = table[\"id\"]\n",
    "                # Get file extension\n",
    "                ext = old_id.split('.')[-1] if '.' in old_id else 'html'\n",
    "                new_id = f\"tbl-{global_table_counter}.{ext}\"\n",
    "                \n",
    "                # Update the table ID in the object\n",
    "                table[\"id\"] = new_id\n",
    "                table_id_map[old_id] = new_id\n",
    "                global_table_counter += 1\n",
    "            \n",
    "            # Update markdown to reflect new IDs\n",
    "            markdown = page.get(\"markdown\", \"\")\n",
    "            \n",
    "            # Replace image references: ![img-0.jpeg](img-0.jpeg) format\n",
    "            for old_id, new_id in image_id_map.items():\n",
    "                markdown = markdown.replace(f\"![{old_id}]({old_id})\", f\"![{new_id}]({new_id})\")\n",
    "            \n",
    "            # Replace table references: [tbl-0.html](tbl-0.html) format\n",
    "            for old_id, new_id in table_id_map.items():\n",
    "                markdown = markdown.replace(f\"[{old_id}]({old_id})\", f\"[{new_id}]({new_id})\")\n",
    "            \n",
    "            page[\"markdown\"] = markdown\n",
    "    \n",
    "    print(f\"Re-indexed {global_image_counter} images and {global_table_counter} tables\")\n",
    "    return responses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b418611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-indexed 18 images and 7 tables\n",
      "Saved re-indexed responses to RAG/OCR/responses_reindexed.json\n"
     ]
    }
   ],
   "source": [
    "# Load the original JSON\n",
    "input_file = \"RAG/OCR/responses.json\"\n",
    "output_file = \"RAG/OCR/responses_reindexed.json\"  # or use the same file to overwrite\n",
    "\n",
    "with open(input_file, \"r\") as f:\n",
    "    responses_list = json.load(f)\n",
    "\n",
    "# Re-index\n",
    "responses_list = reindex_ocr_responses(responses_list)\n",
    "\n",
    "# Save the re-indexed version\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(responses_list, f, indent=4)\n",
    "\n",
    "print(f\"Saved re-indexed responses to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d4c7a",
   "metadata": {},
   "source": [
    "Let's check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4900c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def replace_images_in_markdown_annotated(\n",
    "    markdown_str: str,\n",
    "    images_dict: dict,\n",
    "    tables_dict: dict = None,\n",
    "    include_images: bool = True,\n",
    "    include_tables: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Replaces images and tables in the markdown string with their content/descriptions.\n",
    "\n",
    "    Args:\n",
    "        markdown_str: The markdown string to replace images in.\n",
    "        images_dict: A dictionary of images to replace, with their names as keys and data as values.\n",
    "        tables_dict: A dictionary of tables to replace, with their names as keys and data as values.\n",
    "        include_images: Whether to include images base64 data in the output.\n",
    "        include_tables: Whether to include table HTML content in the output.\n",
    "\n",
    "    Returns:\n",
    "        The markdown string with images and tables replaced.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace images: ![img-0.jpeg](img-0.jpeg) format\n",
    "    for img_name, data in images_dict.items():\n",
    "        placeholder = f\"![{img_name}]({img_name})\"\n",
    "        \n",
    "        # Get annotation description\n",
    "        annotation = data.get('annotation', {})      \n",
    "        description = annotation.get('description', '')\n",
    "        \n",
    "        if include_images:\n",
    "            replacement = f\"![{img_name}]({data['image']})\\n\\n**{description}**\"\n",
    "        else:\n",
    "            replacement = f\"**Figure: {img_name}**\\n\\n**{description}**\"\n",
    "        \n",
    "        markdown_str = markdown_str.replace(placeholder, replacement)\n",
    "    \n",
    "    # Replace tables: [tbl-0.html](tbl-0.html) format (no exclamation mark!)\n",
    "    if tables_dict:\n",
    "        for tbl_name, data in tables_dict.items():\n",
    "            placeholder = f\"[{tbl_name}]({tbl_name})\"\n",
    "            \n",
    "            if include_tables:\n",
    "                # Insert the actual HTML table content\n",
    "                replacement = f\"\\n\\n{data['content']}\\n\\n\"\n",
    "            else:\n",
    "                replacement = f\"**Table: {tbl_name}**\"\n",
    "            \n",
    "            markdown_str = markdown_str.replace(placeholder, replacement)\n",
    "    \n",
    "    return markdown_str\n",
    "\n",
    "def process_saved_ocr_json(json_path: str, range: list[int] = None, include_images: bool = True, include_tables: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Reads the saved JSON list of responses and merges them into one Markdown string.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing OCR responses\n",
    "        range: Optional range on the number of responses to process\n",
    "        include_images: Whether to include images in the output\n",
    "        include_tables: Whether to include tables in the output\n",
    "\n",
    "    Returns:\n",
    "        Combined Markdown string with all OCR responses\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        responses_list = json.load(f)\n",
    "\n",
    "    full_markdown_parts = []\n",
    "\n",
    "    # Handle None range - process all responses\n",
    "    responses_to_process = responses_list[range[0]:range[1]] if range is not None else responses_list\n",
    "\n",
    "    for resp in responses_to_process:\n",
    "        # 1. Add the Document-level Annotation/Summary for this chunk\n",
    "        doc_anno = resp.get(\"document_annotation\", \"\")\n",
    "        if doc_anno:\n",
    "            full_markdown_parts.append(f\"## Document Summary\\n**{doc_anno.get('summary', '')}**\\n ## Authors \\n **{doc_anno.get('authors', '')}**\")\n",
    "\n",
    "        # 2. Iterate through pages in this chunk\n",
    "        for page in resp.get(\"pages\", []):\n",
    "            image_data = {}\n",
    "            # Extract image data for replacement\n",
    "            for img in page.get(\"images\", []):\n",
    "                image_data[img[\"id\"]] = {\n",
    "                    \"image\": img.get(\"image_base64\", \"\"), \n",
    "                    \"annotation\": img.get(\"image_annotation\", {})\n",
    "                }\n",
    "            \n",
    "            table_data = {}\n",
    "            for tbl in page.get(\"tables\", []):\n",
    "                table_data[tbl[\"id\"]] = {\n",
    "                    \"content\": tbl.get(\"content\", \"\"),\n",
    "                }\n",
    "            \n",
    "            # 3. Process the markdown for this specific page\n",
    "            page_md = page.get(\"markdown\", \"\")\n",
    "            processed_page = replace_images_in_markdown_annotated(\n",
    "                page_md, image_data, table_data, include_images, include_tables\n",
    "            )\n",
    "            full_markdown_parts.append(processed_page)\n",
    "\n",
    "    return \"\\n\\n---\\n\\n\".join(full_markdown_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d18528ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Document Summary\n",
       "**The document presents a technical report on Kimi K2, detailing its contributions, token template for tool calling, evaluation details, and other technical aspects. It includes a list of authors, an explanation of the token structure for tool calling, and performance metrics across various benchmarks. The report highlights Kimi K2's superior performance in coding tasks, tool use tasks, math and STEM tasks, general tasks, and long context and factuality tasks. It also discusses the minimal impact of QK-Clip on model quality and the engine switching pipeline for RL training.**\n",
       " ## Authors \n",
       " **['Yifan Bai', 'Yiping Bao', 'Guanduo Chen', 'Jiahao Chen', 'Ningxin Chen', 'Ruijue Chen', 'Yanru Chen', 'Yuankun Chen', 'Yutian Chen', 'Zhuofu Chen*', 'Jialei Cui', 'Hao Ding', 'Mengnan Dong', 'Ang`ang Du', 'Chenzhuang Du', 'Dikang Du', 'Yulun Du', 'Yu Fan', 'Yichen Feng', 'Kelin Fu', 'Bofei Gao', 'Hongcheng Gao', 'Peizhong Gao', 'Tong Gao', 'Xinran Gu', 'Longyu Guan', 'Haiqing Guo*', 'Jianhang Guo', 'Hao Hu', 'Xiaoru Hao', 'Tianhong He', 'Weiran He', 'Wenyang He', 'Chao Hong', 'Yangyang Hu', 'Zhenxing Hu', 'Weixiao Huang', 'Zhiqi Huang', 'Zihao Huang', 'Tao Jiang', 'Zhejun Jiang', 'Xinyi Jin', 'Yongsheng Kang*', 'Guokun Lai', 'Cheng Li', 'Fang Li', 'Haoyang Li', 'Ming Li', 'Wentao Li', 'Yanhao Li', 'Yiwei Li', 'Zhaowei Li', 'Zheming Li', 'Xiaohan Lin', 'Zongyu Lin', 'Chengyin Liu', 'Chenyu Liu', 'Dikang Liu', 'Jingyuan Liu*', 'Junqi Liu', 'Liang Liu', 'Shaowei Liu', 'T.Y. Liu', 'Tianwei Liu', 'Weizhou Liu', 'Yangyang Liu', 'Yibo Liu', 'Yiping Liu', 'Yue Liu', 'Zhengying Liu', 'Enzhe Lu', 'Lijun Lu', 'Shengling Ma', 'Xinyu Ma', 'Yingwei Ma', 'Shaoguang Mao', 'Jie Mei', 'Xin Men', 'Yibo Miao', 'Jinjing Xu', 'L.H. Xu', 'Lin Xu', 'Ruoyu Qin', 'Bowen Qu', 'Zeyu Shang', 'Lidong Shi', 'Shengyuan Shi', 'Feifan Song', 'Flood Sung', 'Heyu Tang', 'Jiawen Tao', 'Qifeng Teng', 'Chensi Wang', 'Dinglu Wang', 'Feng Wang', 'Haiming Wang', 'Jianzhou Wang*', 'Jiaxing Wang', 'Jinhong Wang', 'Shengjie Wang', 'Shuyi Wang', 'Yao Wang', 'Yejie Wang', 'Yiqin Wang', 'Yuxin Wang', 'Yuzhi Wang', 'Zhaoji Wang', 'Zhengtao Wang', 'Zhexu Wang', 'Chu Wei', 'Qianqian Wei', 'Wenhao Wu', 'Xingzhe Wu', 'Yuxin Wu', 'Yutong Zhang', 'Haotian Zhao', 'Yikai Zhao', 'Yuang Zhang', 'Yizhi Zhang', 'Yongting Zhang', 'Yu Zhang', 'Yutao Zhang', 'Zhen Zhang', 'Huabin Zheng', 'Shaojie Zheng', 'Jianren Zhou', 'Xinyu Zhou', 'Zaida Zhou', 'Zhen Zhu', 'Weiyu Zhuang', 'Xinxing Zu', 'Kimi K2']**\n",
       "\n",
       "---\n",
       "\n",
       "Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "# Appendix\n",
       "\n",
       "# A Contributions\n",
       "\n",
       "The listing of authors is in alphabetical order based on their last names. Names marked with an asterisk (*) indicate people who are no longer part of our team.\n",
       "\n",
       "\n",
       "\n",
       "<table><tr><td>Yifan Bai</td><td>Guokun Lai</td><td>Shengyuan Shi</td><td>Ziyao Xu</td></tr><tr><td>Yiping Bao</td><td>Cheng Li</td><td>Feifan Song</td><td>Junjie Yan</td></tr><tr><td>Guanduo Chen</td><td>Fang Li</td><td>Jianlin Su</td><td>Yuzi Yan</td></tr><tr><td>Jiahao Chen</td><td>Haoyang Li</td><td>Zhengyuan Su</td><td>Xiaofei Yang</td></tr><tr><td>Ningxin Chen</td><td>Ming Li</td><td>Xinjie Sun*</td><td>Ying Yang</td></tr><tr><td>Ruijue Chen</td><td>Wentao Li</td><td>Flood Sung</td><td>Zhen Yang</td></tr><tr><td>Yanru Chen</td><td>Yanhao Li</td><td>Heyi Tang</td><td>Zhilin Yang</td></tr><tr><td>Yuankun Chen</td><td>Yiwei Li</td><td>Jiawen Tao</td><td>Zonghan Yang</td></tr><tr><td>Yutian Chen</td><td>Zhaowei Li</td><td>Qifeng Teng</td><td>Haotian Yao</td></tr><tr><td>Zhuofu Chen*</td><td>Zheming Li</td><td>Chensi Wang</td><td>Xingcheng Yao</td></tr><tr><td>Jialei Cui</td><td>Hongzhan Lin*</td><td>Dinglu Wang</td><td>Wenjie Ye</td></tr><tr><td>Hao Ding</td><td>Xiaohan Lin</td><td>Feng Wang</td><td>Zhuorui Ye</td></tr><tr><td>Mengnan Dong</td><td>Zongyu Lin</td><td>Haiming Wang</td><td>Bohong Yin</td></tr><tr><td>Ang'ang Du</td><td>Chengyin Liu</td><td>Jianzhou Wang*</td><td>Longhui Yu</td></tr><tr><td>Chenzhuang Du</td><td>Chenyu Liu</td><td>Jiaxing Wang</td><td>Enming Yuan</td></tr><tr><td>Dikang Du</td><td>Hongzhang Liu</td><td>Jinhong Wang</td><td>Hongbang Yuan*</td></tr><tr><td>Yulun Du</td><td>Jingyuan Liu*</td><td>Shengjie Wang</td><td>Mengjie Yuan</td></tr><tr><td>Yu Fan</td><td>Junqi Liu</td><td>Shuyi Wang</td><td>Haobing Zhan</td></tr><tr><td>Yichen Feng</td><td>Liang Liu</td><td>Yao Wang</td><td>Dehao Zhang</td></tr><tr><td>Kelin Fu</td><td>Shaowei Liu</td><td>Yejie Wang</td><td>Hao Zhang</td></tr><tr><td>Bofei Gao</td><td>T.Y. Liu</td><td>Yiqin Wang</td><td>Wanlu Zhang</td></tr><tr><td>Hongcheng Gao</td><td>Tianwei Liu</td><td>Yuxin Wang</td><td>Xiaobin Zhang</td></tr><tr><td>Peizhong Gao</td><td>Weizhou Liu</td><td>Yuzhi Wang</td><td>Yangkun Zhang</td></tr><tr><td>Tong Gao</td><td>Yangyang Liu</td><td>Zhaoji Wang</td><td>Yizhi Zhang</td></tr><tr><td>Xinran Gu</td><td>Yibo Liu</td><td>Zhengtao Wang</td><td>Yongting Zhang</td></tr><tr><td>Longyu Guan</td><td>Yiping Liu</td><td>Zhexu Wang</td><td>Yu Zhang</td></tr><tr><td>Haiqing Guo*</td><td>Yue Liu</td><td>Chu Wei</td><td>Yutao Zhang</td></tr><tr><td>Jianhang Guo</td><td>Zhengying Liu</td><td>Qianqian Wei</td><td>Yutong Zhang</td></tr><tr><td>Hao Hu</td><td>Enzhe Lu</td><td>Wenhao Wu</td><td>Zheng Zhang</td></tr><tr><td>Xiaoru Hao</td><td>Lijun Lu</td><td>Xingzhe Wu</td><td>Haotian Zhao</td></tr><tr><td>Tianhong He</td><td>Shengling Ma</td><td>Yuxin Wu</td><td>Yikai Zhao</td></tr><tr><td>Weiran He</td><td>Xinyu Ma</td><td>Chenjun Xiao</td><td>Huabin Zheng</td></tr><tr><td>Wenyang He</td><td>Yingwei Ma</td><td>Xiaotong Xie</td><td>Shaojie Zheng</td></tr><tr><td>Chao Hong</td><td>Shaoguang Mao</td><td>Weimin Xiong*</td><td>Jianren Zhou</td></tr><tr><td>Yangyang Hu</td><td>Jie Mei</td><td>Boyu Xu</td><td>Xinyu Zhou</td></tr><tr><td>Zhenxing Hu</td><td>Xin Men</td><td>Jing Xu*</td><td>Zaida Zhou</td></tr><tr><td>Weixiao Huang</td><td>Yibo Miao</td><td>Jinjing Xu</td><td>Zhen Zhu</td></tr><tr><td>Zhiqi Huang</td><td>Siyuan Pan</td><td>L.H. Xu</td><td>Weiyu Zhuang</td></tr><tr><td>Zihao Huang</td><td>Yebo Peng</td><td>Lin Xu</td><td>Xinxing Zu</td></tr><tr><td>Tao Jiang</td><td>Ruoyu Qin</td><td>Suting Xu</td><td>Kimi K2</td></tr><tr><td>Zhejun Jiang</td><td>Bowen Qu</td><td>Weixin Xu</td><td></td></tr><tr><td>Xinyi Jin</td><td>Zeyu Shang</td><td>Xinran Xu</td><td></td></tr><tr><td>Yongsheng Kang*</td><td>Lidong Shi</td><td>Yangchuan Xu</td><td></td></tr></table>\n",
       "\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "B Token Template of Tool Calling\n",
       "\n",
       "There are three components in the token structure for tool-calling:\n",
       "\n",
       "- Tool declaration message: defines the list of available tools and the schema of the arguments;\n",
       "- Tool invoking section in assistant message: encodes the models request to invoke tools;\n",
       "- Tool result message: encapsulates the invoked tools execution result.\n",
       "\n",
       "The raw tokens of the tool declaration message are formatted as follows:\n",
       "\n",
       "\n",
       "<|im_begin|>\n",
       "tool_declare\n",
       "<|im_middle|>\n",
       "# Tools\n",
       "\n",
       "{{ tool declaration content }}\n",
       "<|im_end|>\n",
       "\n",
       "The blue highlighted marks represent special tokens, and the green part, quoted by brackets, is the tool declaration content. We use TypeScript to express the tool declaration content, since TypeScript is a concise language with a comprehensive type system, able to express the types and constraints of tool parameters with brief text. The code 1 shows an example for two simple tools in JSON format compatible with OpenAIs chat completion API, as a comparison, the same tools defined in TypeScript (listed in Code 2) is much shorter. To improve compatibility, part of our training data also uses JSON as the tool declaration language, so that 3rd-party frameworks need not additional development to support our tool calling scheme.\n",
       "\n",
       "Listing 1: Tool definition with JSON in OpenAI compatible API\n",
       "\n",
       "[{\n",
       "\"type\": \"function\",\n",
       "\"function\": {\n",
       "\"name\": \"get_weather\",\n",
       "\"description\": \"Get weather for a location and date\",\n",
       "\"parameters\": {\n",
       "\"type\": \"object\",\n",
       "\"properties\": {\n",
       "\"location\": {\n",
       "\"type\": \"string\",\n",
       "\"description\": \"City and country e.g. Beijing, China\"\n",
       "},\n",
       "\"date\": {\n",
       "\"type\": \"string\",\n",
       "\"description\": \"Date to query, format in %Y-%m-%d\"\n",
       "}\n",
       "},\n",
       "\"required\": [\n",
       "\"location\"\n",
       "]\n",
       "}\n",
       "}\n",
       "},\n",
       "{\n",
       "\"type\": \"function\",\n",
       "\"function\": {\n",
       "\"name\": \"Calculator\",\n",
       "\"description\": \"Simple calculator\",\n",
       "\"parameters\": {\n",
       "\"properties\": {\n",
       "\"expr\": {\n",
       "\"type\": \"string\",\n",
       "\"description\": \"Arithmetic expression in javascript\"\n",
       "}\n",
       "},\n",
       "\n",
       "---\n",
       "\n",
       "Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "```txt\n",
       "\"type\": \"object\"\n",
       "}\n",
       "}\n",
       "```\n",
       "\n",
       "Listing 2: Tool definition in TypeScript\n",
       "```txt\n",
       "namespace functions {\n",
       "// Get weather for a location and date\n",
       "type get_weather = (_: {\n",
       "// City and country e.g. Beijing, China\n",
       "location: string,\n",
       "// Date to query, format in '%Y-%m-%d'\n",
       "date?: string\n",
       "}) =&gt; any;\n",
       "// Simple calculator\n",
       "type Calculator = (_: {\n",
       "// Arithmetic expression in javascript\n",
       "expr?: string\n",
       "}) =&gt; any;\n",
       "}\n",
       "```\n",
       "\n",
       "The token template of the tool invoking section in the model's response messages is listed as follows:\n",
       "\n",
       "```txt\n",
       "<tool_call_section_begin|>\n",
       "<|tool_call_begin|>\n",
       "// call_id part\n",
       "functions.{{tool name}}:{{counter}}\n",
       "<|toolarguments_begin|>\n",
       "{json serialized call arguments}\n",
       "<|tool_call_end|>\n",
       "<|tool_call_begin|>\n",
       "// more tool calls\n",
       "<|tool_call_end|>\n",
       "<|tool_call_section_end|>\n",
       "```\n",
       "\n",
       "As shown in the template, we support parallel tool calling by placing multiple tool calls in a single response turn. Each tool call has a unique call id, formatted as functions.{tool-name}:{counter}, where tool-name is the name of the tool, and counter is an auto-increasing counter of all tool calls starting from 0 in the dialog.\n",
       "\n",
       "During inference, the model may occasionally generate unexpected tokens, leading to format errors when parsing a tool call. To solve this issue, we developed a constrained decoding module named enforcer, inspired by lm-format-enforcer $^6$ . When a <tool_call_section_begin|> token is generated, it ensures that the upcoming tool-related tokens follow the predefined template, and the JSON argument string follows the declared schema.\n",
       "\n",
       "The tool result message is simply a text message encoded with the tool's call id and the corresponding results.\n",
       "\n",
       "```txt\n",
       "&lt;|im_begin|&gt;\n",
       "tool\n",
       "&lt;|im_middle|&gt;\n",
       "## Results of {{call_id}}\n",
       "{execution result content}\n",
       "&lt;|im_end|&gt;\n",
       "```\n",
       "\n",
       "# C Evaluation Details\n",
       "\n",
       "Coding Tasks. We evaluate Kimi-K2-Instruct's capabilities on competitive coding benchmarks, LiveCodeBench and OJBench, where Kimi-K2-Instruct attains superior performance with scores of  $53.7\\%$  and  $27.1\\%$ , respectively. This excellence spans both medium-level coding challenges, such as LeetCode and AtCoder, and hard-level contests like NOI and ICPC, outperforming leading open-source and proprietary models. For multilingual programming proficiency, we employ MultiPL-E, covering languages including C++, C#, Java, JavaScript, PHP, Go, Kimi-K2-Instruct surpasses top</tool_call_section_begin|></tool_call_section_begin|></tool_call_section_begin|>\n",
       "\n",
       "---\n",
       "\n",
       "open-source models with an accuracy of 85.7%, compared with 83.1% for DeepSeek-V3-0324 and 78.2% for Qwen3-235B-A22B. In software engineering tasks, Kimi-K2-Instruct demonstrates robust performance on SWE-bench Verified (Python), SWE-lancer (Python), SWE-bench Multilingual, and Multi-SWE-bench datasets. It significantly outperforms open-source counterparts in resolving real-world code repository issues and notably narrows the performance gap with proprietary models. For example:\n",
       "\n",
       "- SWE-bench Verified (multiple attempts): 71.6% (Kimi-K2-Instruct) vs. 80.2% (Claude 4 Sonnet)\n",
       "- SWE-bench Multilingual: 47.3% (Kimi-K2-Instruct) vs. 51.0% (Claude 4 Sonnet)\n",
       "- SWE-lancer: 39.1% (Kimi-K2-Instruct) vs. 40.8% (Claude 4 Sonnet)\n",
       "\n",
       "On PaperBench, Kimi-K2-Instruct achieves an accuracy of 27.8%, closely matching GPT-4.1 and outperforming DeepSeek-V3-0324 (12.2%) and Qwen3-235B-A22B (8.2%) by a substantial margin. In terminal interaction tasks measured by TerminalBench, Kimi-K2-Instruct attains 25.0% using the default Terminus framework and rises to 30% within Moonshots in-house agentic framework, underscoring its capabilities in real-world agentic programming scenarios. Moreover, on the Aider-Polyglot benchmark, Kimi-K2-Instruct attains a 60.0% accuracy while employing rigorous decontamination procedures, further illustrating its strength and reliability across diverse coding environments.\n",
       "\n",
       "##### Tool Use Tasks.\n",
       "\n",
       "We evaluate multi-turn tool use with two complementary suites: $\\tau^{2}$-Bench and ACEBench. $\\tau^{2}$-Bench extends the original $\\tau$-bench single-control setup to a dual-control environment in which both the agent and an LLM-simulated user have constrained tool affordances over a shared state, adding a realistic Telecom troubleshooting domain alongside the prior Airline/Retail TAU tasks and enabling analysis of coordination vs. pure reasoning. ACEBench is a large bilingual (En/Zh) API-grounded benchmark (4.5K APIs across 8 domains; 2K annotated eval items) partitioned into Normal (basic/personalized/atomic), Special (imperfect or out-of-scope inputs), and Agent (scenario-driven multi-turn, multi-step sandbox) tracks with automated grading of calls and outcomes. All models run in non-thinking mode; we set the temperature to 0.0, use deterministic tool adapters, score $\\tau^{2}$ Airline/Retail/Telecom under Avg@4 seeds with Pass@1/4, and report overall on ACEBench English. Kimi-K2-Instruct averages 66.1 micro Pass@1 across $\\tau^{2}$ vs DeepSeek-V3-0324 48.8 / Qwen3-235B-A22B 37.3. On ACEBench Overall Kimi-K2-Instruct scores 76.5 vs DeepSeek 72.7 / Qwen 70.5 and remains competitive with GPT-4.1 (80.1).\n",
       "\n",
       "##### Math & STEM & Logical Tasks.\n",
       "\n",
       "For Math tasks, Kimi-K2-Instruct achieves consistently strong performance, averaging over Geimini-2.5-Flash by 5.3 percentage points, over DeepSeek-V3-0324 by 5.5 points and over GPT4.1 by 15.8 points. For example, on AIME 2024, Kimi-K2-Instruct scores 69.6%, outperforming another two top open-source models by a large margin, DeepSeek-V3-0324 by 10.2 points and Qwen3-235B-A22B by 29.5 points. In STEM evaluations, Kimi-K2-Instruct achieves 75.1% on GPQA-Diamond, outperforming DeepSeek-V3-0324 (68.4%) and all non-thinking baselines by at least 5 percentage points. On SuperGPQA, it also exceeds the previous best open-source model, DeepSeek-V3-0324, by 3.5 points. Kimi-K2-Instruct also surpasses the other two leading models in logical reasoning. It achieves 89.0% on ZebraLogic and 89.5% on AutoLogi, exceeding DeepSeek-V3-0324 (84.0%, 88.9%) and substantially outperforming Qwen3-235B-A22B (37.7%, 83.3%).\n",
       "\n",
       "##### General Tasks.\n",
       "\n",
       "Kimi-K2-Instruct ties DeepSeek-V3-0324 on MMLU and MMLU-Pro, and takes the lead on MMLU-Redux with a 92.7 EM scoreslightly ahead of GPT-4.1 (92.4) and just 1.5 points behind Claude-Opus-4. Beyond multiple-choice tasks, the model achieves 31.0% accuracy on the short-answer SimpleQA3.3 points above DeepSeek-V3-0324 and more than twice that of Qwen3-235B-A22Bthough still below GPT-4.1 (42.3%). On the adversarial free-response LiveBench (2024-11-25 snapshot), it reaches 76.4%, surpassing Claude-Sonnet 4 (74.8%) and leading Gemini 2.5 Flash Preview by 8.6 points. Across this challenging triad measuring breadth, depth, and robustness of world knowledge, Kimi-K2-Instruct secures a top-tier position among open-source models. We evaluate instruction-following with IFEval and Multi-Challenge. On IFEval, Kimi-K2-Instruct scores 89.8%, higher than DeepSeek-V3-0324 (81.1%) and GPT-4.1 (88.0%). On Multi-Challenge, which involves multi-turn dialogues with conflicting instructions, it achieves 54.1%, outperforming DeepSeek-V3-0324 (31.4%), GPT-4.1 (36.4%), and Claude-Opus-4 (49.0%). These results demonstrate that Kimi-K2-Instruct integrates strong factual knowledge with consistent instruction adherence across both single- and multi-turn settings, supporting robust and reliable real-world deployment.\n",
       "\n",
       "##### Long Context and Factuality Tasks.\n",
       "\n",
       "To evaluate the factuality of Kimi-K2-Instruct, we employ three benchmarks: FACTS Grounding, which measures adherence to provided documents using the proprietary models GPT-4o, Gemini 1.5 Pro and Claude 3.5 Sonnet; HHEM, which assesses summarization quality via the open-source HHEM-2.1-Open judge; and FaithJudge, which analyzes faithfulness in RAG tasks with o3-mini as the judge. Kimi-K2-Instruct scores 88.5 on FACTS Grounding, substantially outperforming all open-source rivals and even surpassing the closed-source Gemini 2.5 Flash. With HHEM-2.1-Open it achieves a hallucination rate of 1.1 %, reported in the tables as 1 minus the\n",
       "\n",
       "---\n",
       "\n",
       "Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "**Figure: img-13.jpeg**\n",
       "\n",
       "**The image is a bar chart titled 'Kimi-K2-Instruct Open-Ended Evaluation (aggregated)'. It compares the performance of Kimi-K2-Instruct against three other models: DeepSeek-V3-0324, Claude-Sonnet-4, and ChatGPT-4o-latest. The chart is divided into three horizontal bars, each representing a different comparison. Each bar is segmented into three sections: Win (blue), Tie (gray), and Loss (red). The percentages for each segment are as follows: Kimi-K2-Instruct vs DeepSeek-V3-0324 has 59.6% Win, 23.5% Tie, and 16.9% Loss; Kimi-K2-Instruct vs Claude-Sonnet-4 has 64.6% Win, 18.8% Tie, and 16.6% Loss; Kimi-K2-Instruct vs ChatGPT-4o-latest has 65.4% Win, 17.6% Tie, and 17.0% Loss. The x-axis represents the percentage win rate, ranging from 0% to 100%.**\n",
       "Figure 11: Chinese in-house benchmark evaluation.\n",
       "\n",
       "rate, i.e. 98.9. On FaithJudge's RAG tasks the hallucination rate is  $7.4\\%$ , likewise present as 92.6 for table consistency. For long-context capabilities, Kimi-K2-Instruct outperforms all open source and proprietary models on DROP  $(93.5\\%)$ , and exceeds DeepSeek-V3-0324 on retrieval task MRCR  $(55.0\\%$  vs  $50.8\\%)$ . For long-context reasoning tasks FRAMES and LongBench v2, Kimi-K2-Instruct  $(77.1\\%, 49.1\\%)$  lags slightly behind DeepSeek-V3-0324 by around  $2\\%$ .\n",
       "\n",
       "Open-Ended Evaluation Beyond static, closed-ended benchmarks, we evaluate the model's performance on open-ended, nuanced tasks that more closely resemble real-world usage.\n",
       "\n",
       "For English scenarios, we leverage the Arena-Hard-Auto v2.0 benchmark, which use LLM-as-a-judge protocols to assess generation quality across diverse, open-ended prompts [42]. These evaluations cover a wide range of high-difficulty prompts and are widely recognized in the research community. On Arena-Hard-Auto v2.0, Kimi-K2-Instruct achieves state-of-the-art win-rate on both hard prompts (54.5%) and creative writing tasks (85.0%), outperforming all open-source models and rivaling top proprietary systems such as GPT-4.1 and Claude Sonnet. These results underscore the model's strength in handling complex reasoning and nuanced generation under diverse, unconstrained settings.\n",
       "\n",
       "However, Arena-Hard-Auto provides limited coverage of Chinese-specific tasks. To address this gap, we developed an in-house held-out benchmark grounded in authentic user queries. To safeguard the integrity of the evaluation, the benchmark data is access-restricted, thereby eliminating the risk of overfitting.\n",
       "\n",
       "As shown in Figure 11, Kimi-K2-Instruct shows strong performance across all comparisons on Chinese in-house benchmarks. It outperforms ChatGPT-4o-latest with a  $65.4\\%$  win rate, Claude Sonnet 4 with  $64.6\\%$ , and DeepSeek-V3-0324 with  $59.6\\%$ . In all cases, the loss rate stays low (around  $17\\%$ ), indicating that Kimi-K2-Instruct rarely falls behind. The high win rates and consistent margins demonstrate its strong ability on open-ended Chinese tasks.\n",
       "\n",
       "In addition to controlled evaluations, we also consider real-world user preference through public human assessments. As of July 17, 2025, Kimi-K2-Instruct ranked as the top open-source model and fifth overall on the LMSYS Arena leaderboard $^{7}$ , based on over 3,000 blind votes from real users. Unlike LLM-as-a-judge protocols, this leaderboard reflects direct human preference on diverse, user-submitted prompts, providing a complementary perspective on practical model performance.\n",
       "\n",
       "The results on Arena-Hard-Auto, our in-house benchmark and votes from LMSYS Arena collectively offer a comprehensive view of Kimi-K2-Instruct's open-ended capabilities, showing that it is a highly preferred model in real-world user experience across English and Chinese.\n",
       "\n",
       "# D QK-Clip Does Not Impair Model Quality\n",
       "\n",
       "The QK-Clip design follows a minimal intervention principle: it activates only when necessary, and deactivates after training stabilizes. Empirical evidence and analysis converge on its negligible impact on model quality.\n",
       "\n",
       "---\n",
       "\n",
       "K Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "**Figure: img-14.jpeg**\n",
       "\n",
       "**This graph shows the validation loss over training steps for two different scenarios: with QK-Clip (w/ QK-Clip) and without QK-Clip (w/o QK-Clip). The x-axis represents the number of training steps, ranging from 0 to 20,000, while the y-axis represents the validation loss, ranging from 1.6 to 2.8. The graph indicates that the validation loss decreases as the number of training steps increases. The scenario with QK-Clip consistently shows a lower validation loss compared to the scenario without QK-Clip, suggesting that QK-Clip helps in achieving better model performance.**\n",
       "Figure 12: Applying QK-Clip to Muon in a small-scale setting with an aggressive threshold  $(\\tau = 30)$  has negligible impact on loss, indicating that it is a safe and effective method for constraining attention logits.\n",
       "\n",
       "Small-Scale Ablations We train two small-scale 0.5B activated and 3B total parameters MoE models, one with vanilla Muon and the other with MuonClip using a low clipping threshold  $(\\tau = 30)$ . As shown in Figure 12, applying MuonClip has negligible effects on the loss curve, indicating that even aggressive clipping does not impair convergence or training dynamics with MuonClip. This demonstrates that MuonClip is a safe and effective method for bounding attention logits without degrading model performance. Furthermore, evaluation on downstream tasks reveals no statistically significant degradation in performance. These results collectively demonstrate that MuonClip is a safe and effective method for bounding attention logits without compromising model quality.\n",
       "\n",
       "Self-deactivation In Kimi K2, QK-Clip was only transiently active:\n",
       "\n",
       "- Initial 70000 steps:  $12.7\\%$  of attention heads triggered QK-Clip for at least once, clamping  $S_{\\mathrm{max}}$  to 100.\n",
       "- Post-70000 steps: All heads at some point reduced their  $S_{\\mathrm{max}}$  below 100, rendering QK-Clip inactive.\n",
       "\n",
       "When QK-Clip is active, it is applied per-head (rather than per-layer) to minimize potential over-regularization on other heads. After training stabilizes, QK-clip is deactivated and has no effect at all.\n",
       "\n",
       "# E Why Muon is More Prone to Logit Explosion\n",
       "\n",
       "Logit explosion occurs when the largest pre-softmax attention score\n",
       "\n",
       "$$\n",
       "S _ {\\max } = \\max  _ {i, j} \\left(q _ {i} \\cdot k _ {j}\\right) \\tag {1}\n",
       "$$\n",
       "\n",
       "grows unboundedly during training. Since\n",
       "\n",
       "$$\n",
       "\\left| q _ {i} \\cdot k _ {j} \\right| \\leq \\| q _ {i} \\| \\| k _ {j} \\| \\leq \\| x _ {i} \\| \\| x _ {j} \\| \\| \\mathbf {W} _ {q} \\| \\| \\mathbf {W} _ {k} \\|, \\tag {2}\n",
       "$$\n",
       "\n",
       "and RMS-Norm keeps  $\\| x_{i}\\| \\| x_{j}\\|$  bounded, the phenomenon is primarily driven by the growing spectral-norm of  $\\mathbf{W}_q$  or  $\\mathbf{W}_k$ . Empirically, we found that Muon is more susceptible to logit explosion. We give our hypothesis below.\n",
       "\n",
       "Structural difference in updates Muon produces a weight update coming from the msign operation; as a result, all singular values of the update matrix are equal  its effective rank is full. In contrast, a typical update matrix produced by Adam exhibits a skewed spectrum: a few large singular values dominate, and the effective rank is low. This low-rank assumption for Adam is not new; higher-order muP makes the same assumption.\n",
       "\n",
       "Such phenomenon is verified on the 16 B Moonlight model, which shows weights trained with Muon exhibit higher singular-value entropy (i.e. higher effective rank) than those trained with Adam, corroborating the theoretical intuition.\n",
       "\n",
       "SVD formulation Let the parameter matrix at step  $t - 1$  have the singular value decomposition\n",
       "\n",
       "$$\n",
       "\\mathbf {W} _ {t - 1} = \\sum_ {i} \\sigma_ {i} u _ {i} v _ {i} ^ {\\top} \\tag {3}\n",
       "$$\n",
       "\n",
       "---\n",
       "\n",
       "We write the update matrices as\n",
       "\n",
       "$\\Delta\\mathbf{W}_{t}=\\sum_{j}\\bar{\\sigma}\\>\\bar{u}_{j}\\bar{v}_{j}^{\\top}$ (4)\n",
       "\n",
       "The next parameter update is therefore\n",
       "\n",
       "$\\mathbf{W}_{t}\\leftarrow\\sum_{i}\\sigma_{i}u_{i}v_{i}^{\\top}+\\sum_{j}\\bar{\\sigma}\\>\\bar{u}_{j}\\bar{v}_{j}^{\\top}$ (5)\n",
       "\n",
       "In Muon, as both the weights and the updates have a higher effective rank than Adam, we hypothesize there is a higher probability for singular-vector pair $u_{i}v_{i}^{\\top}$ to align with $\\bar{u}_{j}\\bar{v}_{j}^{\\top}$. This could cause the corresponding singular value of $\\mathbf{W}_{t}$ to increase additively.\n",
       "\n",
       "##### Attention-specific amplification\n",
       "\n",
       "Attention logits are computed via the bilinear form\n",
       "\n",
       "$q_{i}\\cdot k_{j}=(x_{i}\\mathbf{W}_{q})\\cdot(x_{j}\\mathbf{W}_{k}).$ (6)\n",
       "\n",
       "The product $\\mathbf{W}_{q}\\mathbf{W}_{k}^{\\top}$ squares the spectral norm, so any singular-value increase in either matrix is compounded. Muons tendency to enlarge singular values therefore translates into a higher risk of logit explosion.\n",
       "\n",
       "## Appendix F K2 Critic Rubrics for General RL\n",
       "\n",
       "### F.1 Core Rubrics\n",
       "\n",
       "- Clarity and Relevance: Assesses the extent to which the response is succinct while fully addressing the users intent. The focus is on eliminating unnecessary detail, staying aligned with the central query, and using efficient formats such as brief paragraphs or compact lists. Unless specifically required, long itemizations should be avoided. When a choice is expected, the response should clearly offer a single, well-defined answer.\n",
       "- Conversational Fluency and Engagement: Evaluates the responses contribution to a natural, flowing dialogue that extends beyond simple question-answering. This includes maintaining coherence, showing appropriate engagement with the topic, offering relevant observations or insights, potentially guiding the conversation constructively when appropriate, using follow-up questions judiciously, handling hypothetical or personal-analogy queries gracefully, and adapting tone effectively to suit the conversational context (e.g., empathetic, formal, casual).\n",
       "- Objective and Grounded Interaction: Assesses the responses ability to maintain an objective and grounded tone, focusing squarely on the substance of the users request. It evaluates the avoidance of both metacommentary (analyzing the querys structure, topic combination, perceived oddity, or the nature of the interaction itself) and unwarranted flattery or excessive praise directed at the user or their input. Excellent responses interact respectfully but neutrally, prioritizing direct, task-focused assistance over commentary on the conversational dynamics or attempts to curry favor through compliments.\n",
       "\n",
       "### F.2 Prescriptive Rubrics\n",
       "\n",
       "- Initial Praise: Responses must not begin with compliments directed at the user or the question (e.g., Thats a beautiful question, Good question!).\n",
       "- Explicit Justification: Any sentence or clause that explains why the response is good or how it successfully fulfilled the users request. This is different from simply describing the content.\n",
       "\n",
       "### F.3 Limitations\n",
       "\n",
       "One potential side effect of this evaluation framework is that it may favor responses that appear confident and assertive, even in contexts involving ambiguity or subjectivity. This stems from two key constraints in the current rubric:\n",
       "\n",
       "- Avoidance of Self-Qualification: The prescriptive rules prohibit self-assessments, explicit disclaimers, or hedging language (e.g., this may not be accurate, I might be wrong). While these phrases can reflect epistemic humility, they are often penalized as non-informative or performative.\n",
       "- Preference for Clarity and Singularity: The rubric reward direct, decisive answers when users ask for a recommendation or explanation. In complex or open-ended scenarios, this may disincentivize appropriately cautious or multi-perspective responses.\n",
       "\n",
       "---\n",
       "\n",
       "K Kimi K2\n",
       "\n",
       "TECHNICAL REPORT\n",
       "\n",
       "As a result, the model may occasionally overstate certainty in areas where ambiguity, nuance, or epistemic modesty would be more appropriate. Future iterations of the framework may incorporate more fine-grained handling of calibrated uncertainty.\n",
       "\n",
       "# G Engine Switching Pipeline for RL Training\n",
       "\n",
       "**Figure: img-15.jpeg**\n",
       "\n",
       "**This image is a Gantt chart that illustrates the scheduling of various operations across four devices. The chart uses different colors to represent different types of operations: H2D Buffer (blue), IPC Buffer (light orange), Reload weights (dark orange), Broadcast (src) (yellow), and Broadcast (dst) (light beige). Each row corresponds to a different device (Device 0 to Device 3), and the horizontal axis represents time. The chart shows the sequence and overlap of these operations, indicating how tasks are managed and synchronized across multiple devices.**\n",
       "(a) Theoretical perfect three-stage pipeline weight update\n",
       "\n",
       "**Figure: img-16.jpeg**\n",
       "\n",
       "**The image depicts a Gantt chart, which is a type of bar chart that illustrates a project schedule. Each horizontal bar represents a task, with the length of the bar corresponding to the duration of the task. The chart includes overlapping and sequential tasks, indicated by the alignment and spacing of the bars. Different colors are used to distinguish between different tasks or phases of the project. The chart helps in visualizing the start and end dates of tasks, their dependencies, and the overall project timeline.**\n",
       "(b) A PCIE bounded three-stage pipeline\n",
       "\n",
       "**Figure: img-17.jpeg**\n",
       "\n",
       "**The image shows a sequence of Gantt charts, each representing a different project timeline. The charts are composed of horizontal bars divided into segments, with different colors indicating various stages or activities within each project. The blue segments likely represent planned or scheduled activities, while the orange and beige segments could signify different phases or statuses of the tasks, such as in progress or completed. Each row in the image represents a different project, showing how tasks overlap and progress over time.**\n",
       "(c) Fixed two-stage pipeline\n",
       "Figure 13: pipeline for RL weight update\n",
       "\n",
       "The checkpoint engine manages three equal-size device buffers on each GPU: an H2D buffer for loading the offloaded model parameters, and two IPC buffers for GPU-to-GPU broadcast. The IPC buffers are shared to inference engines, allowing it to directly access the same physical memory. These three buffers allow us to arrange the three steps in a pipeline.\n",
       "\n",
       "Theoretical three-stage pipeline. As illustrated in Figure 13a, a three-stage pipeline is introduced. (1)  $H2D$ : a shard of the latest weights is copied into the H2D buffer asynchronously. (2) Broadcast: Once the copy completes, the shard will be copied to one IPC buffers and broadcast to all devices. (3) Reload: Inference engines simultaneously load parameters from the other IPC buffer.\n",
       "\n",
       "Two-stage pipeline due to PCIe saturation. On NVIDIA H800 clusters, concurrent H2D and broadcast saturate the shared PCIe fabric, collapsing the three stages into a sequential procedure (Figure 13b). We therefore adopt a simpler, two-stage scheme (Figure 13c): (1) All devices perform a single, synchronous H2D transfer. (2) The broadcast and reload proceed in parallel.\n",
       "\n",
       "The two-stage pipeline will be bound by multiple synchronous H2D copy operations. But in large scale devices, model will be split into small shards, the entire parameter set fits into the H2D buffer in one transfer, the overhead will disappear.\n",
       "\n",
       "By overlapping H2D, Broadcast, and Reload weights, we can obtain a high bandwidth to reshard the weights from train engines to all inference engines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display (just first page)\n",
    "final_markdown = process_saved_ocr_json(\"RAG/OCR/responses_reindexed.json\", range=[3, 4], include_images=False, include_tables=True)\n",
    "display(Markdown(final_markdown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2c9ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Document Summary\n",
      "**{'summary': \"The document presents a technical report on Kimi K2, detailing its contributions, token template for tool calling, evaluation details, and other technical aspects. It includes a list of authors, an explanation of the token structure for tool calling, and performance metrics across various benchmarks. The report highlights Kimi K2's superior performance in coding tasks, tool use tasks, math and STEM tasks, general tasks, and long context and factuality tasks. It also discusses the minimal impact of QK-Clip on model quality and the engine switching pipeline for RL training.\", 'authors': ['Yifan Bai', 'Yiping Bao', 'Guanduo Chen', 'Jiahao Chen', 'Ningxin Chen', 'Ruijue Chen', 'Yanru Chen', 'Yuankun Chen', 'Yutian Chen', 'Zhuofu Chen*', 'Jialei Cui', 'Hao Ding', 'Mengnan Dong', 'Ang`ang Du', 'Chenzhuang Du', 'Dikang Du', 'Yulun Du', 'Yu Fan', 'Yichen Feng', 'Kelin Fu', 'Bofei Gao', 'Hongcheng Gao', 'Peizhong Gao', 'Tong Gao', 'Xinran Gu', 'Longyu Guan', '\n"
     ]
    }
   ],
   "source": [
    "# authors are on the last page:\n",
    "final_page = process_saved_ocr_json(\"RAG/OCR/responses.json\", range=[3, 4])\n",
    "print(final_page[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257c98b",
   "metadata": {},
   "source": [
    "### 2.2 Parsing Metadata\n",
    "\n",
    "An important aspect of a RAG system is the metadata: we want to present data to the user with the given source, probably the page, images and tables if we parsed them... and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8ee8223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ocr_json(json_path: str, range: list[int] = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Loads the saved JSON list of OCR responses.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing OCR responses\n",
    "        range: Optional range [start, end] to slice the responses list\n",
    "\n",
    "    Returns:\n",
    "        List of OCR response dictionaries\n",
    "    \"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        responses_list = json.load(f)\n",
    "    \n",
    "    # Return sliced or full list\n",
    "    return responses_list[range[0]:range[1]] if range is not None else responses_list\n",
    "\n",
    "def load_ocr_text(json_path: str, range: list[int] = None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Loads the saved JSON list of OCR responses and returns the text only.\n",
    "\n",
    "    Args:\n",
    "        json_path: Path to the JSON file containing OCR responses\n",
    "    Returns: \n",
    "        The text of the pages, replacing images and tables with their indices.\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "300076dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'The document introduces Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. It highlights the MuonClip optimizer, which improves upon the Muon optimizer with a novel QK-clip technique to enhance training stability and token efficiency. Kimi K2 was pre-trained on 15.5 trillion tokens and underwent a multi-stage post-training process, including a large-scale agentic data synthesis pipeline and a joint reinforcement learning stage. The model demonstrates state-of-the-art performance on various benchmarks, particularly excelling in agentic capabilities and tasks related to software engineering. The document also discusses the technical aspects of the model, including the MuonClip optimizer, the architecture of Kimi K2, and the training infrastructure used. It concludes by mentioning the release of the base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.', 'authors': ['Kimi Team']}\n",
      "{'summary': 'The document discusses the technical aspects of a model named Kimi K2, focusing on its training, post-training, and reinforcement learning processes. It details the activation CPU offload mechanism, training recipe, supervised fine-tuning, and reinforcement learning strategies. The document also covers the data synthesis pipeline for tool use, domain evolution, agent diversification, and the infrastructure supporting these processes. It concludes with evaluations of Kimi-K2-Instruct against various benchmarks, highlighting its performance in coding, tool use, math & STEM tasks, and general tasks.', 'authors': ['Kimi K2 Technical Report']}\n",
      "{'summary': 'The document presents a comprehensive evaluation of the Kimi-K2-Instruct and Kimi-K2-Base models, highlighting their superior performance across various benchmarks and tasks. Kimi-K2-Instruct excels in multi-turn tool-use benchmarks, general capabilities, and open-ended evaluations, outperforming all baselines in grounded, controlled, and agent-driven tool orchestration. It also shows strong, balanced performance across general knowledge, math, instruction following, and long-context tasks. Kimi-K2-Base demonstrates state-of-the-art performance in general language understanding, coding capabilities, mathematical reasoning, and Chinese language understanding. The document also details the evaluation settings and results for Kimi-K2-Base, showcasing its leading performance across diverse evaluation benchmarks. Additionally, it discusses safety evaluations, limitations, and conclusions, acknowledging the support of the OpenHands and Multi-SWE-bench teams.', 'authors': ['Kimi-K2 Team']}\n",
      "{'summary': \"The document presents a technical report on Kimi K2, detailing its contributions, token template for tool calling, evaluation details, and other technical aspects. It includes a list of authors, an explanation of the token structure for tool calling, and performance metrics across various benchmarks. The report highlights Kimi K2's superior performance in coding tasks, tool use tasks, math and STEM tasks, general tasks, and long context and factuality tasks. It also discusses the minimal impact of QK-Clip on model quality and the engine switching pipeline for RL training.\", 'authors': ['Yifan Bai', 'Yiping Bao', 'Guanduo Chen', 'Jiahao Chen', 'Ningxin Chen', 'Ruijue Chen', 'Yanru Chen', 'Yuankun Chen', 'Yutian Chen', 'Zhuofu Chen*', 'Jialei Cui', 'Hao Ding', 'Mengnan Dong', 'Ang`ang Du', 'Chenzhuang Du', 'Dikang Du', 'Yulun Du', 'Yu Fan', 'Yichen Feng', 'Kelin Fu', 'Bofei Gao', 'Hongcheng Gao', 'Peizhong Gao', 'Tong Gao', 'Xinran Gu', 'Longyu Guan', 'Haiqing Guo*', 'Jianhang Guo', 'Hao Hu', 'Xiaoru Hao', 'Tianhong He', 'Weiran He', 'Wenyang He', 'Chao Hong', 'Yangyang Hu', 'Zhenxing Hu', 'Weixiao Huang', 'Zhiqi Huang', 'Zihao Huang', 'Tao Jiang', 'Zhejun Jiang', 'Xinyi Jin', 'Yongsheng Kang*', 'Guokun Lai', 'Cheng Li', 'Fang Li', 'Haoyang Li', 'Ming Li', 'Wentao Li', 'Yanhao Li', 'Yiwei Li', 'Zhaowei Li', 'Zheming Li', 'Xiaohan Lin', 'Zongyu Lin', 'Chengyin Liu', 'Chenyu Liu', 'Dikang Liu', 'Jingyuan Liu*', 'Junqi Liu', 'Liang Liu', 'Shaowei Liu', 'T.Y. Liu', 'Tianwei Liu', 'Weizhou Liu', 'Yangyang Liu', 'Yibo Liu', 'Yiping Liu', 'Yue Liu', 'Zhengying Liu', 'Enzhe Lu', 'Lijun Lu', 'Shengling Ma', 'Xinyu Ma', 'Yingwei Ma', 'Shaoguang Mao', 'Jie Mei', 'Xin Men', 'Yibo Miao', 'Jinjing Xu', 'L.H. Xu', 'Lin Xu', 'Ruoyu Qin', 'Bowen Qu', 'Zeyu Shang', 'Lidong Shi', 'Shengyuan Shi', 'Feifan Song', 'Flood Sung', 'Heyu Tang', 'Jiawen Tao', 'Qifeng Teng', 'Chensi Wang', 'Dinglu Wang', 'Feng Wang', 'Haiming Wang', 'Jianzhou Wang*', 'Jiaxing Wang', 'Jinhong Wang', 'Shengjie Wang', 'Shuyi Wang', 'Yao Wang', 'Yejie Wang', 'Yiqin Wang', 'Yuxin Wang', 'Yuzhi Wang', 'Zhaoji Wang', 'Zhengtao Wang', 'Zhexu Wang', 'Chu Wei', 'Qianqian Wei', 'Wenhao Wu', 'Xingzhe Wu', 'Yuxin Wu', 'Yutong Zhang', 'Haotian Zhao', 'Yikai Zhao', 'Yuang Zhang', 'Yizhi Zhang', 'Yongting Zhang', 'Yu Zhang', 'Yutao Zhang', 'Zhen Zhang', 'Huabin Zheng', 'Shaojie Zheng', 'Jianren Zhou', 'Xinyu Zhou', 'Zaida Zhou', 'Zhen Zhu', 'Weiyu Zhuang', 'Xinxing Zu', 'Kimi K2']}\n"
     ]
    }
   ],
   "source": [
    "# Load all responses\n",
    "all_responses = load_ocr_json(\"RAG/OCR/responses.json\")\n",
    "# Access the data\n",
    "for resp in all_responses:\n",
    "    print(resp[\"document_annotation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4619e",
   "metadata": {},
   "source": [
    "This is a part of our metadata. Since we split the document into several parts, we may want to fuse together all the author fields, but maybe get the summary only from the first part (where the abstract is) or maybe get an llm to summarize the split summaries into one. Another part is the page, of the document, but that is easy to retrieve.\n",
    " \n",
    "But another important part of our metadata is the images and the tables. \n",
    "\n",
    "We want to replace images and tables with a textual description for our RAG. Then we'll map to the actual base64/html encoding with metadata.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5fb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img-0.jpeg\n",
      "{\n",
      "  \"image_type\": \"graph\",\n",
      "  \"description\": \"This image contains a series of bar charts comparing the performance of various AI models across different benchmarks. The benchmarks are categorized into 'Agentic and Competitive Coding' and 'Math & STEM'. Each bar chart represents a specific benchmark, such as SWE-bench Verified, SWE-bench Multilingual, LiveCodeBench v6, OJBench, Tau2-bench micro-average, AceBench (en), AIME 2025, and GPQA-Diamond. The AI models compared include Kimi-K2-Instruct, DeepSeekV3-0324, Owen3-2535B-A22B, OpenAI GPT-4.1, Claude 4 Opus, Claude 4 Sonnet, and Gemini 2.5 Flash non-thinking. The performance scores for each model are displayed on the y-axis, with higher scores indicating better performance. The charts show that different models perform variably across different benchmarks, with Kimi-K2-Instruct generally performing well across most benchmarks.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "first_img = all_responses[0][\"pages\"][0][\"images\"][0]\n",
    "print(first_img['id'])\n",
    "print(first_img['image_annotation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3d246",
   "metadata": {},
   "source": [
    "We already parsed our OCR'd document in a nice way with the above functions: we are able to get a full markdown with indexed placeholders for images and tables, together with images' descriptions.\n",
    "\n",
    "But how about tables descriptions??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32be93",
   "metadata": {},
   "source": [
    "Well, we may have to do this manually, since mistral does not implement that specific function. \n",
    "\n",
    "*My idea is to give the pdf of the paper, together with the extracted tables, to a multimodal model that will provide a summary of the given tables and an index. Then we swap the result in with the tables extracted from Mistral.*\n",
    "\n",
    "Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27aaeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field, SecretStr\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gemini-2.5-flash\", \n",
    "    \n",
    "    # redirect LangChain to OpenRouter\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "\n",
    "    # pass the OpenRouter key\n",
    "    api_key=SecretStr(os.environ[\"OPENROUTER_API_KEY\"])\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a document indexing assistant. \n",
    "I will provide a document and its corresponding OCR Markdown. \n",
    "Your task is to find every table placeholder (e.g., [tbl-x.html]) in the text, identify what that table represents, and return a JSON mapping.\n",
    "\n",
    "Specifically, you must return an answer composed of 2 parts:\n",
    "- title: the title of the table, extracted from the document.\n",
    "- description: the description of the table. \n",
    "This description must be thorough and include all the information in the table, highlighting the main points and the context.\n",
    "\"\"\"\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    title : str = Field(\"The title of the table, extracted from the document.\")\n",
    "    description : str = Field(\"The description of the table.\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    system_prompt=prompt,\n",
    "    response_format=ResponseFormat,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e8580c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# get the full text with no base64 image data but with table contents (as html)\n",
    "# we want this llm to check the tables content and have full context \n",
    "full_text = process_saved_ocr_json(\"RAG/OCR/responses_reindexed.json\", include_images=False, include_tables=True)\n",
    "# we also pass the pdf in order to have full context (maybe not needed)\n",
    "base64_pdf = encode_pdf(\"RAG/documents/KimiK2.pdf\")\n",
    "\n",
    "message = HumanMessage(\n",
    "    content=[\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Here is the structured OCR text and tables for context:\\n\\n{full_text}\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"image_url\", # OpenAI uses this block for PDF vision support\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:application/pdf;base64,{base64_pdf}\"\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_state = {\"messages\" : [message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ec448e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'gemini-2.5-flash is not a valid model ID', 'code': 400}, 'user_id': 'user_36hWo69Fljn0qd35HPzkWJkn7jD'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langgraph/pregel/main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain/agents/factory.py:1130\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1117\u001b[39m request = ModelRequest(\n\u001b[32m   1118\u001b[39m     model=model,\n\u001b[32m   1119\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m     runtime=runtime,\n\u001b[32m   1126\u001b[39m )\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1132\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1133\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain/agents/factory.py:1101\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1099\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1101\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1103\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain_core/runnables/base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1225\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1223\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1229\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1386\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1385\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1389\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1390\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1391\u001b[39m ):\n\u001b[32m   1392\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1381\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1375\u001b[39m             response,\n\u001b[32m   1376\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1377\u001b[39m             metadata=generation_info,\n\u001b[32m   1378\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1379\u001b[39m         )\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1381\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m         response = raw_response.parse()\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1189\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1187\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1188\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/course/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'gemini-2.5-flash is not a valid model ID', 'code': 400}, 'user_id': 'user_36hWo69Fljn0qd35HPzkWJkn7jD'}",
      "During task with name 'model' and id '59773c94-3c8f-0707-cc74-3225ce5a56da'"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['structured_response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
