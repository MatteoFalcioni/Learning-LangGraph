{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c5fd30f",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MatteoFalcioni/Learning-LangGraph/blob/main/notebooks/10_deep_agents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67191aa",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64add61f",
   "metadata": {},
   "source": [
    "#### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d8ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/langgraph/\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U -r https://raw.githubusercontent.com/MatteoFalcioni/Learning-LangGraph/main/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd5141",
   "metadata": {},
   "source": [
    "#### local (notebooks or files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce458820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # load api keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dde4fe",
   "metadata": {},
   "source": [
    "#### Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "REQUIRED_KEYS = [\n",
    "    'OPENAI_API_KEY',\n",
    "    'LANGSMITH_TRACING',\n",
    "    'LANGSMITH_ENDPOINT',\n",
    "    'LANGSMITH_API_KEY',\n",
    "    'LANGSMITH_PROJECT'\n",
    "]\n",
    "\n",
    "def _set_colab_keys(key : str):\n",
    "    # Retrieve the secret value using its key/name\n",
    "    secret_value = userdata.get(key)\n",
    "    # set it as a standard OS environment variable\n",
    "    os.environ[key] = secret_value\n",
    "\n",
    "for key in REQUIRED_KEYS:\n",
    "    _set_colab_keys(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7a5b7",
   "metadata": {},
   "source": [
    "# Deep Agents & Middleware\n",
    "\n",
    "In this notebook we will og over two concepts that are very new in LangGraph (at least in december 2025 while I'm writing this): Deep Agents and Middleware. \n",
    "\n",
    "Deep agents, also referred to sometimes as \"agent harnesses\" are a particular kind of agents that can go \"in depth\" into their workflow: we'll see what this actually means in a second. \n",
    "\n",
    "Middleware, on the other hand, is not a LangGraph concept: it's a term used to refer to all software layers that \"glue together\" different applications, databases or services. \n",
    "\n",
    "We'll see how LangChain developers translated this idea into some really useful LangGraph implementations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005bc4c",
   "metadata": {},
   "source": [
    "## 1. Deep Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248b97b",
   "metadata": {},
   "source": [
    "Citing LangChain's [Deep Agents main page](https://docs.langchain.com/oss/python/deepagents/overview): \n",
    "\n",
    "*deepagents is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303bc27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "*Use deep agents when you need agents that can:*\n",
    "\n",
    "- *Handle complex, multi-step tasks that require planning and decomposition*\n",
    "- *Manage large amounts of context through file system tools*\n",
    "- *Delegate work to specialized subagents for context isolation*\n",
    "- *Persist memory across conversations and threads*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea290be9",
   "metadata": {},
   "source": [
    "**For simpler use cases, consider using LangChain’s create_agent or building a custom LangGraph workflow.**\n",
    "\n",
    "This above ^ is highlighted because I think it's a concept that we cannot let go over our heads: deep agents are really powerful, but we will see that they are \"heavyweight\": for more simple tasks, there is no need to use this heavy machinery. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d945ab",
   "metadata": {},
   "source": [
    "### 1.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7b964",
   "metadata": {},
   "source": [
    "[This very interesting blog post](https://blog.langchain.com/deep-agents/) from LangChain explains perfectly the architecture of deep agents. I encouracge you to read it, or if you are more of a yt enjoyer, here's the [Deep Agents video by LangChain](https://youtu.be/IVts6ztrkFg?si=o51HELlDVN3YR6Ei).\n",
    "\n",
    "The core algorithm is actually the same - it’s an LLM running in a loop calling tools. The difference compared to the naive agent that is easy to build is:\n",
    "\n",
    "- A detailed system prompt\n",
    "- Planning tool\n",
    "- Sub agents\n",
    "- File system\n",
    "\n",
    "Now, in order to go over some deep agents application, we will not go into the specific details of the architecture. However, you can find a detailed youtube video from LangChain devs explaining the specific details here: [Rewriting Deep Agents on top of LangChain 1.0](https://youtu.be/AZ6257Ya_70?si=QSw20KdbgUixCb2p).\n",
    "\n",
    "Here you can find the previous implementation, which is slightly updated but goes in depth into the actual structure of deep agents: [Implementing deepagents: a technical walkthrough](https://youtu.be/TTMYJAw5tiA?si=sCRI95ihge_1n_qu). I find it really interesting.\n",
    "\n",
    "There is also a LangChain academy course specifically on deep agents: [LangChain Academy - Deep Agents](https://academy.langchain.com/courses/deep-agents-with-langgraph/?utm_medium=social&utm_source=youtube&utm_campaign=q3-2025_deep-agents-course_co).\n",
    "\n",
    "Now, let's quickly go over the cited components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4d570d",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/deep_agents.png\" width=\"1000\">\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2fe5b6",
   "metadata": {},
   "source": [
    "#### 1.1.1 Planning Tool \n",
    "\n",
    "Claude Code uses a Todo list tool, and this is the inspiration for the todo tool that comes by default with a deep agent. You will be amazed of how useful this can be for your agents. \n",
    "\n",
    "#### 1.1.2 Spawning Subagents \n",
    "\n",
    "Deep agents go deeper on topics. This is largely accomplished by spinning up sub agents that specifically focused on individual tasks, and allowing those sub agents to go deep there.\n",
    "\n",
    "Basically your agent will be creating subagents and assigning them tasks. After the subagent complete their tasks, they report back to the main agent.\n",
    "\n",
    "#### 1.1.3 Virtual File System\n",
    "\n",
    "Deep agents run for long periods of time and accumulate a lot of context that they need to manage. LangChain equipped Deep Agents with a file system tool, which creates a virtual file system in the agent's state, that the agent can use to write out notes, in order to manage context. \n",
    "\n",
    "These files are not created in your machine, they are just dictionaries in the agent's state.\n",
    "\n",
    "#### 1.1.4 System Prompt\n",
    "\n",
    "We all know by now how important a prompt is. Deep agents have longer context (like Claude Code) and therefore their prompts can be huge. If we want to write our own instructions, these will be added to the pre-built deep agents system prompt that LangChain devs built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477b1dd",
   "metadata": {},
   "source": [
    "## 1.2 Example\n",
    "\n",
    "Since our course has a practical setup, we will dive right in with an example.\n",
    "\n",
    "We are going to build a Deep Research Agent that searches the web and writes report on a specific topic, following this nice LangChain yt video: [Build a Research Agent with Deep Agents](https://youtu.be/5tn6O0uXYEg?si=pHKBOz6zYstytPqS) (GitHub repo here: [Deep Agents Quickstart](https://github.com/langchain-ai/deepagents-quickstarts/tree/main)).\n",
    "\n",
    "Here comes that time of the course where notebooks are starting to feel limited and messy for our applications. That is why in this lecture we will start deferring to the `projects/` directory: specifically, the example in question can be found in [projects/deep_agents](../projects/deep_agents/) directory. \n",
    "\n",
    "You can easily follow in the notebook [research_agent.ipynb](../projects/deep_agents/deep_research/research_agent.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b2b49",
   "metadata": {},
   "source": [
    "## 2. Middleware\n",
    "\n",
    "Now let's quickly go over middleware. It's super simple to use.\n",
    "\n",
    "Middleware runs hooks before and/or after model execution, in this way: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47de7453",
   "metadata": {},
   "source": [
    "<img src=\"./images/middleware.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d90e47f",
   "metadata": {},
   "source": [
    "LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable:\n",
    "https://docs.langchain.com/oss/python/langchain/middleware/built-in#provider-agnostic-middleware\n",
    "\n",
    "Some middleware add tools to your agent, some add prompts, some add additional functionalities like prompt caching or tool retries, and there is also a human in the loop middleware. You have a wide choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534235cd",
   "metadata": {},
   "source": [
    "## 2.1 Human In The Loop Middleware\n",
    "\n",
    "Since we recently saw HITL, let's see how we can implement human in the loop before tool calls with middleware with two lines of code.\n",
    "\n",
    "(Recall that HITL requires a checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8ac6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "def read_email_tool(email_id: str) -> str:\n",
    "    \"\"\"read an email by its ID.\"\"\"\n",
    "    return f\"Email content for ID: {email_id}\"\n",
    "\n",
    "def send_email_tool(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"send an email.\"\"\"\n",
    "    return f\"Email sent to {recipient} with subject '{subject}'\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[read_email_tool, send_email_tool],\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email_tool\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                \"read_email_tool\": False,  # do not interrupt on this \n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba267b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\" : {\"thread_id\" : \"test\"}}\n",
    "\n",
    "result = agent.invoke(\n",
    "        {\n",
    "            \"messages\" : [HumanMessage(content=\"Send Matteo an email asking him if he comes to work tomorrow. No need to make it formal.\")]\n",
    "        },\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58cbfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_requests : [{'name': 'send_email_tool', 'args': {'recipient': 'Matteo', 'subject': 'Work Tomorrow?', 'body': 'Hey Matteo, are you coming to work tomorrow?'}, 'description': \"Tool execution requires approval\\n\\nTool: send_email_tool\\nArgs: {'recipient': 'Matteo', 'subject': 'Work Tomorrow?', 'body': 'Hey Matteo, are you coming to work tomorrow?'}\"}]\n",
      "review_configs : [{'action_name': 'send_email_tool', 'allowed_decisions': ['approve', 'edit', 'reject']}]\n"
     ]
    }
   ],
   "source": [
    "interrupt = result['__interrupt__'][0]\n",
    "for key, value in interrupt.value.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b0944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Send Matteo an email asking him if he comes to work tomorrow. No need to make it formal.', additional_kwargs={}, response_metadata={}, id='c7dad376-334e-4caf-b373-3db72b5982a9'),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 93, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-Cxy5Sm36yv2BybdkkMh2fJ0AqUwNK', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019bbd5a-2871-7d52-9d2f-ced4b9bff7c1-0', tool_calls=[{'name': 'send_email_tool', 'args': {'recipient': 'Matteo', 'subject': 'Work Tomorrow?', 'body': 'Hey Matteo, are you coming to work tomorrow?'}, 'id': 'call_OhyNvDLjL1SOxsQXPDlLrJSK', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 93, 'output_tokens': 36, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"Email sent to Matteo with subject 'Work Tomorrow?'\", name='send_email_tool', id='8bcaa9fd-7159-4d54-830b-a082bde5b7d5', tool_call_id='call_OhyNvDLjL1SOxsQXPDlLrJSK'),\n",
       "  AIMessage(content='I have sent an email to Matteo asking if he is coming to work tomorrow.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 148, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-Cxy5oU7g8Y6w2Ky2uxQpcLbZfrzhJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019bbd5a-8153-72e2-b935-c63d25962459-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 148, 'output_tokens': 17, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "agent.invoke(\n",
    "    Command( \n",
    "        resume={\"decisions\": [{\"type\": \"approve\"}]}  # or \"reject\"\n",
    "    ), \n",
    "    config=config # Same thread ID to resume the paused conversation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f098230",
   "metadata": {},
   "source": [
    "## 2.2 To Do List \n",
    "\n",
    "Next, one of the most simple and at the same time useful tools implemented in middleware: we can allow our agent to write and update a todo list to keep track of its progresses. You'll be suprised of how better your agent performs when using this tool.\n",
    "\n",
    "This middleware automatically provides agents with a write_todos tool and system prompts to guide effective task planning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb46e97",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import TodoListMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[read_file, write_file, run_tests],  # example tools\n",
    "    middleware=[TodoListMiddleware()],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0142f5",
   "metadata": {},
   "source": [
    "## 2.3 File Search (actual files in your folder)\n",
    "\n",
    "This is a cool one: look for files in your file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1677407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import FilesystemFileSearchMiddleware\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        FilesystemFileSearchMiddleware(\n",
    "            root_path=\"./\",\n",
    "            use_ripgrep=True,\n",
    "            max_file_size_mb=10,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Agent can now use glob_search and grep_search tools\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Find all jupyter notebooks containing @tool decorators\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4643717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The following Jupyter notebooks contain `@tool` decorators:\\n\\n1. `/10_deep_agents.ipynb`\\n2. `/11_supervisor_&_MCP.ipynb`\\n3. `/12_hierarchical_agent_teams.ipynb`\\n4. `/13_voice_agents.ipynb`\\n5. `/15_RAG_pt2.ipynb`\\n6. `/3.5_recap.ipynb`\\n7. `/3_llms_&_agents.ipynb`\\n8. `/5_command.ipynb`\\n9. `/6_agentic_graph.ipynb`\\n10. `/7.5_recap.ipynb`\\n11. `/7_memory.ipynb`\\n12. `/8_human_in_the_loop.ipynb`\\n13. `/9_agent_collaboration.ipynb`' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 164, 'prompt_tokens': 556, 'total_tokens': 720, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-Cxy6XtA93sBrxExdA3W4pUiawBPcI', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bbd5b-2f28-79a3-b204-79287cf81802-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 556, 'output_tokens': 164, 'total_tokens': 720, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result['messages'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff35f9c",
   "metadata": {},
   "source": [
    "## 2.4 Shell Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c0166",
   "metadata": {},
   "source": [
    "Expose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following:\n",
    "\n",
    "- Agents that need to execute system commands\n",
    "- Development and deployment automation tasks\n",
    "- Testing and validation workflows\n",
    "- File system operations and script execution\n",
    "\n",
    "Use appropriate execution policies (HostExecutionPolicy, DockerExecutionPolicy, or CodexSandboxExecutionPolicy) to match your deployment’s security requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9aa0df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import (\n",
    "    ShellToolMiddleware,\n",
    "    HostExecutionPolicy,\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[],\n",
    "    middleware=[\n",
    "        ShellToolMiddleware(\n",
    "            workspace_root=\"./\",\n",
    "            execution_policy=HostExecutionPolicy(),  # agent can do rm -rf * be careful\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c94e2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"Create a new folder named tests/\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f360baef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a new folder named tests/\n",
      "\n",
      "<no output>\n",
      "The folder named `tests` has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "for message in result['messages']:\n",
    "    print(message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
